[{"authors":["Christopher"],"categories":null,"content":"I\u0026rsquo;m an organizational scientist, which means that I ask questions about workplace behavior: What makes people productive? How do you ask for help effectively? What promotes cooperation among a collective? The research groups that I\u0026rsquo;m lucky enough to work with try to discover simple mechanisms that reveal how people work together to create the wonderful complexity that we see in the world. I also work in the quantitative areas of dynamics and computational modeling.\n","date":1513728000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1513728000,"objectID":"e23cb71100d51c11123b1b2597de799d","permalink":"https://christopherdishop.netlify.app/authors/alison/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alison/","section":"authors","summary":"I\u0026rsquo;m an organizational scientist, which means that I ask questions about workplace behavior: What makes people productive?","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"  Quick note on adding a “time” column when participants differ in the number of responses they offer. Let’s say my data are as follows:\nlibrary(tidyverse) library(kableExtra) df \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1, 1, 2, 2, 2, 3, 4, 4), \u0026#39;score\u0026#39; = c(6, 5, 3, 4, 2, 8, 7, 7) ) head(df, 8) %\u0026gt;% kable() %\u0026gt;% kable_styling()   id  score      1  6    1  5    2  3    2  4    2  2    3  8    4  7    4  7     where person 1 responded twice, person 2 three times, person 3 once, and person 4 twice. I want to add another column indicating that idea.\nIdentify the number of times each id appears in the dataframe.\ntable(df$id) ## ## 1 2 3 4 ## 2 3 1 2 Save the values.\nid_appear_times \u0026lt;- unname(table(df$id)) Create a sequence from 1 to i for each i in the vector.\ntimer \u0026lt;- c() for(i in id_appear_times){ new_time \u0026lt;- c(1:i) timer \u0026lt;- c(timer, new_time) } Add it to my data.\nhead(df, 8) %\u0026gt;% mutate(time = timer) %\u0026gt;% select(time, id, everything()) %\u0026gt;% kable() %\u0026gt;% kable_styling()   time  id  score      1  1  6    2  1  5    1  2  3    2  2  4    3  2  2    1  3  8    1  4  7    2  4  7     Miscellaneous Afterthought While playing with the code above, I considered how to generate the id column with rep or seq. Here’s how:\nrep_each \u0026lt;- function(x, times) { times \u0026lt;- rep(times, length.out = length(x)) rep(x, times = times) } time_vec \u0026lt;- rep_each(c(1,2,3,4), times = id_appear_times) time_vec ## [1] 1 1 2 2 2 3 4 4 Bo\\(^2\\)m =)\n ","date":1599868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599868800,"objectID":"3606198e819bf5fa0825731c331fd6de","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-09-12/","publishdate":"2020-09-12T00:00:00Z","relpermalink":"/computational_notes/2020-09-12/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Unequal Time Stamps","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"  The Schelling model that I created in my last script used matrix operations. It required me to think in terms of patches housed in a matrix. Consider the following 3x3 grid.\n## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 1 0 1 ## [3,] 1 1 0 Each cell can be thought of as a patch. When a given patch is 0, it is unoccupied. When a given patch is 1 or 2, it is occupied by a hockey or soccer player, respectively. When I implement a Schelling model using a matrix, it puts me in a certain frame of mind. I have to consider patches as locations specified by rows and columns. At row 1 column 2, for example, sits patch XXX that is either occupied or unoccupied.\nAnother way to implement the Schelling model is to use long data. The same information conveyed in the 3x3 matrix is shown in long form below.\n## xcoord ycoord type ## 1 1 1 1 ## 2 2 1 1 ## 3 3 1 1 ## 4 1 2 2 ## 5 2 2 0 ## 6 3 2 1 ## 7 1 3 1 ## 8 2 3 1 ## 9 3 3 0 Coordinates are now viewed as information that can be stored in respective columns. The type column represents whether the patch is unoccupied (0), houses a hockey player (1), or houses a soccer player (2). The goal of this post is to re-create the Schelling model using long data.\nI’m going to present the code in two sections. The first demonstrates the behavior of 1 agent within a single time point. The second reveals the full model: it iterates over 50 time points using all agents.\nBasic Idea The model uses two data frames to store (the most important) information. One holds the coordinates of the living location of each agent. Susan, for example, lives at xcoord = 3 \u0026amp; ycoord = 10, whereas Johnny lives at xcoord = 2 \u0026amp; ycoord = 15. The other specifies the object located at each patch on the grid (0 = unoccupied, 1 = hockey player, 2 = soccer player), and this second data frame will be used for plotting.\nTo reiterate, one data frame stores agent coordinates:\nlibrary(kableExtra) head(agent_df) %\u0026gt;% kable() %\u0026gt;% kable_styling()   xcoord  ycoord  type  agent      1  1  2  1    3  1  2  2    5  1  2  3    8  1  1  4    9  1  1  5    10  1  2  6     and the other, which will be used for plotting, stores patch information.\nhead(patch_df) %\u0026gt;% kable() %\u0026gt;% kable_styling()   xcoord  ycoord  type      1  1  2    2  1  0    3  1  2    4  1  0    5  1  2    6  1  0     The pseudocode for the model is as follows.\n\u0026quot; for each period for each agent i identify i\u0026#39;s 8 surrounding neighbors count the number of similar and dissimilar neighbors -- e.g., if i is a hockey player and is surrounded by soccer players... -- then he has mostly dissimilar neighbors if agent i has more dissimilar neighbors than he desires, then label him as unhappy if agent i has more similar neighbors than he desires, then label him as happy repeat for all agents for each unhappy agent j randomly select a new patch to possibly move to if the patch is unoccupied, move there repeat for all unhappy agents plot the grid of patches for this period save the plot end \u0026quot; Of course, the model is much more complex in syntax. But the basic idea is straightforward: people move if they have many dissimilar neighbors, and they stay if they have similar neighbors. Moreover, new patches, which are selected when agents want to move, are pulled randomly.\nLet’s pretend we are at the first period and are beginning to iterate across agents. The code works as follows.\nStarting with agent 1, identify her coordinates and type (type meaning hockey or soccer player).\n agent_coords \u0026lt;- agent_df %\u0026gt;% filter(agent == 1) %\u0026gt;% select(xcoord, ycoord) agent_type \u0026lt;- agent_df %\u0026gt;% filter(agent == 1) %\u0026gt;% select(type) %\u0026gt;% pull() glimpse(agent_coords) ## Rows: 1 ## Columns: 2 ## $ xcoord \u0026lt;int\u0026gt; 1 ## $ ycoord \u0026lt;int\u0026gt; 1 glimpse(agent_type) ## num 2 Using agent i’s coordinates, identify her 8 surrounding neighbors.\nneigh \u0026lt;- get_neigh(agent_coords) The function get_neigh is prespecified (I will show you the syntax below). It’s too complicated to pick apart now. Just know that it returns the coordinates of her 8 surrounding neighbors.\nneigh %\u0026gt;% kable() %\u0026gt;% kable_styling   xcoord  ycoord      2  1    2  51    1  51    51  51    51  1    51  2    1  2    2  2     I’m about to start counting similar neighbors, so I need to initialize a few counters.\ntotal_neighs \u0026lt;- 0 similar_neighs \u0026lt;- 0 Count similar neighbors. Go through each row in the neigh matrix to find a given neighbor’s coordinates. Use those coordinates to pull all information about neighbor n from the agent data frame. Then, increment similar_neighs by 1 if neighbor n is the same type as agent i. Increment total_neighs if the patch isn’t empty.\nStated simply, if agent i is a hockey player then count the number of other hockey players. Also count the number of non-empty patches.\n for(n in 1:nrow(neigh)){ # save neighbor neigh_agent \u0026lt;- patch_df %\u0026gt;% filter(xcoord == neigh$xcoord[n], ycoord == neigh$ycoord[n]) # increment similar neighbors by 1 if agent is same as neighbor # increment total neighbors by 1 if patch isn\u0026#39;t empty if(agent_type == neigh_agent$type){similar_neighs \u0026lt;- similar_neighs + 1} if(neigh_agent$type != 0){total_neighs \u0026lt;- total_neighs + 1} } Now my counters have values.\ncat( paste(\u0026quot;Total Neighbors =\u0026quot;, total_neighs, \u0026quot;\\nSimilar Neighbors =\u0026quot;, similar_neighs) ) ## Total Neighbors = 5 ## Similar Neighbors = 5 Calculate a similarity ratio. Take the number of similar neighbors and divide it by the total number of neighbors (i.e., non-empty patches).\nsim_ratio \u0026lt;- similar_neighs / total_neighs I won’t show the code here, but all of the information calculated so far then gets stored in a master “results” data frame.\nSo far, we have identified agent i’s neighbors and calculated her similarity ratio. We now need to determine whether she wants to move. If she does, we then need to find a new place for her to move to.\nSchelling’s original model used inequalities to generate agent happiness (or satisfaction). When one’s similarity ratio is greater than some innate preference for similar others (say, 0.6), then the agent is happy and stays put. When one’s similarity ratio is lower than 0.6, the agent is unhappy and moves. Here is that idea embodied in code.\nempty \u0026lt;- is.nan(sim_ratio) happy \u0026lt;- NULL if(empty == TRUE){happy \u0026lt;- TRUE} # if the agent has no neighbors, he is happy if(empty == FALSE \u0026amp;\u0026amp; sim_ratio \u0026gt; 0.6){happy \u0026lt;- TRUE} if(empty == FALSE \u0026amp;\u0026amp; sim_ratio \u0026lt; 0.6){happy \u0026lt;- FALSE} if(empty == FALSE \u0026amp;\u0026amp; sim_ratio == 0.6){happy \u0026lt;- FALSE} The inequalities are located in the if statements. What makes the syntax a bit tricky is that I also included an empty object. I did that because not all agents have neighbors. It is possible for an agent to be surrounded by all empty patches. When this (unlikely) case happens, then total_neighs is equal to 0, and we all know that dividing by 0 doesn’t work. So, the code above asks whether sim_ratio has an actual value, and it only moves forward if so. Said differently, empty would equal TRUE when agent i has no neighbors. If the agent is happy, the code moves forward. If the agent is unhappy, she gets stored (not shown).\nThe steps above then repeat for every agent. Once it iterates over all agents, storing the unhappy agents when they arise, it finds new patches for the unhappy agents. First, randomly select new coordinates.\n new_x \u0026lt;- sample(51, 1) new_y \u0026lt;- sample(51, 1) Is the patch located at those coordinates occupied?\n agent_type_at_new \u0026lt;- patch_df %\u0026gt;% filter(xcoord == new_x, ycoord == new_y) %\u0026gt;% select(type) %\u0026gt;% pull() # 0 = unoccupied # 1 = hockey player # 2 = soccer player occupied \u0026lt;- FALSE if(agent_type_at_new != 0){occupied \u0026lt;- TRUE} If the patch is unoccupied, then we can work with our unhappy agent. If the patch is occupied, we need to continue to sample patches until we find one that is unoccupied.\n while(occupied == TRUE){ new_x \u0026lt;- sample(51, 1) new_y \u0026lt;- sample(51, 1) agent_type_at_new \u0026lt;- patch_df %\u0026gt;% filter(xcoord == new_x, ycoord == new_y) %\u0026gt;% select(type) %\u0026gt;% pull() if(agent_type_at_new == 0){occupied \u0026lt;- FALSE} } Once selected, we change the new patch to occupied within the patch_df, change the old patch to unoccupied within the patch_df, and update the agent data frame with unhappy agent i’s new coordinates. The code to do so is something like the following.\n # change new patch to unhappy agent i\u0026#39;s type patch_df[patch_df$xcoord == new_x \u0026amp; patch_df$ycoord == new_y, \u0026quot;type\u0026quot;] \u0026lt;- current_unhappy$type[1] # go to the old patch where unhappy agent i used to live and change it to 0 patch_df[patch_df$xcoord == current_unhappy$xcoord[1] \u0026amp; patch_df$ycoord == current_unhappy$ycoord[1], \u0026quot;type\u0026quot;] \u0026lt;- 0 # update the agent_df to reflect unhappy agent i\u0026#39;s new coordinates agent_df[agent_df$agent == current_unhappy$agent[1], \u0026quot;xcoord\u0026quot;] \u0026lt;- new_x agent_df[agent_df$agent == current_unhappy$agent[1], \u0026quot;ycoord\u0026quot;] \u0026lt;- new_y  Full Model Here is the full model.\nlibrary(tidyverse) library(reshape2) library(ggplot2) # initial grid # # # # # # dims \u0026lt;- 51*51 empty_patches \u0026lt;- 781 peeps \u0026lt;- c(rep(1, (dims-empty_patches) / 2), rep(2, (dims-empty_patches) / 2), rep(0, empty_patches)) num_agents \u0026lt;- dims - empty_patches mat \u0026lt;- matrix(sample(peeps, dims, replace = F), 51, 51, byrow = T) patch_df \u0026lt;- melt(mat) %\u0026gt;% mutate(xcoord = Var1, ycoord = Var2, type = value) %\u0026gt;% select(xcoord, ycoord, type) agent_df \u0026lt;- patch_df %\u0026gt;% filter(type %in% c(1,2)) %\u0026gt;% mutate(agent = 1:num_agents) plotfirst \u0026lt;- patch_df alike_preference \u0026lt;- 0.6 # get neighbors function # # # # # # # # get_neigh \u0026lt;- function(xy){ # starting from right and going clockwise, I want neighbors a,b,c,d,e,f,g,h ax \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] + 1 ay \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] # xcoord, ycoord a \u0026lt;- c(ax, ay) bx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] + 1 by \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] - 1 # xcoord, ycoord b \u0026lt;- c(bx, by) cx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] cy \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] - 1 # xcoord, ycoord c \u0026lt;- c(cx, cy) dx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] - 1 dy \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] - 1 # xcoord, ycoord d \u0026lt;- c(dx, dy) ex \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] - 1 ey \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] # xcoord, ycoord e \u0026lt;- c(ex, ey) fx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] - 1 fy \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] + 1 # xcoord, ycoord f \u0026lt;- c(fx, fy) gx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] gy \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] + 1 # xcoord, ycoord g \u0026lt;- c(gx, gy) hx \u0026lt;- xy[1 , \u0026quot;xcoord\u0026quot;] + 1 hy \u0026lt;- xy[1, \u0026quot;ycoord\u0026quot;] + 1 # xcoord, ycoord h \u0026lt;- c(hx, hy) dff \u0026lt;- data.frame( \u0026#39;xcoord\u0026#39; = c(a[1], b[1], c[1], d[1], e[1], f[1], g[1], h[1]), \u0026#39;ycoord\u0026#39; = c(a[2], b[2], c[2], d[2], e[2], f[2], g[2], h[2]) ) dff \u0026lt;- dff %\u0026gt;% mutate(xcoord = ifelse(xcoord == 0, 51, xcoord), xcoord = ifelse(xcoord == 52, 1, xcoord), ycoord = ifelse(ycoord == 0, 51, ycoord), ycoord = ifelse(ycoord == 52, 1, ycoord)) return(dff) } # initialize stores # # # # # # # # # time \u0026lt;- 40 result_df \u0026lt;- data.frame( \u0026quot;time\u0026quot; = numeric(time*num_agents), \u0026quot;agent\u0026quot; = numeric(time*num_agents), \u0026quot;simratio\u0026quot; = numeric(time*num_agents) ) count \u0026lt;- 0 save_plots \u0026lt;- list() # begin iterations over periods # # # # # # # for(i in 1:time){ unhappy_store \u0026lt;- list() unhappy_counter \u0026lt;- 0 # for each agent for(ag in 1:num_agents){ count \u0026lt;- count + 1 # save agent\u0026#39;s coords # save agent\u0026#39;s type agent_coords \u0026lt;- agent_df %\u0026gt;% filter(agent == ag) %\u0026gt;% select(xcoord, ycoord) agent_type \u0026lt;- agent_df %\u0026gt;% filter(agent == ag) %\u0026gt;% select(type) %\u0026gt;% pull() # identify neighbors - save their coordinates neigh \u0026lt;- get_neigh(agent_coords) total_neighs \u0026lt;- 0 similar_neighs \u0026lt;- 0 # for each neighbor for(n in 1:nrow(neigh)){ # save neighbor neigh_agent \u0026lt;- patch_df %\u0026gt;% filter(xcoord == neigh$xcoord[n], ycoord == neigh$ycoord[n]) # increment similar neighbors by 1 if agent is same as neighbor # increment total neighbors by 1 if patch isn\u0026#39;t empty if(agent_type == neigh_agent$type){similar_neighs \u0026lt;- similar_neighs + 1} if(neigh_agent$type != 0){total_neighs \u0026lt;- total_neighs + 1} } # save his sim/total (time, agent, simratio) sim_ratio \u0026lt;- similar_neighs / total_neighs result_df[count, \u0026quot;time\u0026quot;] \u0026lt;- i result_df[count, \u0026quot;agent\u0026quot;] \u0026lt;- ag result_df[count, \u0026quot;simratio\u0026quot;] \u0026lt;- sim_ratio # if the agent has empty patches around him (is.nan(sim_ratio) == T) # or # if sim/total \u0026gt; then alike_preferences, # then the agent is happy # otherwise, he is unhappy empty \u0026lt;- is.nan(sim_ratio) happy \u0026lt;- NULL if(empty == TRUE){happy \u0026lt;- TRUE} if(empty == FALSE \u0026amp;\u0026amp; sim_ratio \u0026gt; alike_preference){happy \u0026lt;- TRUE} if(empty == FALSE \u0026amp;\u0026amp; sim_ratio \u0026lt; alike_preference){happy \u0026lt;- FALSE} if(empty == FALSE \u0026amp;\u0026amp; sim_ratio == alike_preference){happy \u0026lt;- FALSE} # if the agent is unhappy, store him if(happy == FALSE){ unhappy_counter \u0026lt;- unhappy_counter + 1 unhappy_store[[unhappy_counter]] \u0026lt;- ag } } # after going through all agents, have the unhappy agents move unhappy_agents \u0026lt;- unlist(unhappy_store) for(q in 1:length(unhappy_agents)){ if(is.null(unhappy_agents) == TRUE){break} # randomly select a new patch new_x \u0026lt;- sample(51, 1) new_y \u0026lt;- sample(51, 1) # is the new patch occupied? agent_type_at_new \u0026lt;- patch_df %\u0026gt;% filter(xcoord == new_x, ycoord == new_y) %\u0026gt;% select(type) %\u0026gt;% pull() occupied \u0026lt;- FALSE if(agent_type_at_new != 0){occupied \u0026lt;- TRUE} while(occupied == TRUE){ new_x \u0026lt;- sample(51, 1) new_y \u0026lt;- sample(51, 1) agent_type_at_new \u0026lt;- patch_df %\u0026gt;% filter(xcoord == new_x, ycoord == new_y) %\u0026gt;% select(type) %\u0026gt;% pull() if(agent_type_at_new == 0){occupied \u0026lt;- FALSE} } # unhappy agent current_unhappy \u0026lt;- agent_df %\u0026gt;% filter(agent == unhappy_agents[q]) # go to the new x and y position in the patch and place the agent type there patch_df[patch_df$xcoord == new_x \u0026amp; patch_df$ycoord == new_y, \u0026quot;type\u0026quot;] \u0026lt;- current_unhappy$type[1] # go to the old x and y position in the patch and change it to 0 patch_df[patch_df$xcoord == current_unhappy$xcoord[1] \u0026amp; patch_df$ycoord == current_unhappy$ycoord[1], \u0026quot;type\u0026quot;] \u0026lt;- 0 # change the agent_df to reflect the agent\u0026#39;s new position agent_df[agent_df$agent == current_unhappy$agent[1], \u0026quot;xcoord\u0026quot;] \u0026lt;- new_x agent_df[agent_df$agent == current_unhappy$agent[1], \u0026quot;ycoord\u0026quot;] \u0026lt;- new_y } # create plot # save and store plot gp \u0026lt;- ggplot(patch_df, aes(x = xcoord, y = ycoord, fill = factor(type))) + geom_tile() + ggtitle(paste(\u0026quot;Period =\u0026quot;, i)) + scale_fill_brewer(palette = \u0026quot;Greens\u0026quot;, name = \u0026quot;Type of Patch\u0026quot;) save_plots[[i]] \u0026lt;- gp } ggplot(plotfirst, aes(x = xcoord, y = ycoord, fill = factor(type))) + geom_tile() + ggtitle(\u0026quot;Period = 0\u0026quot;) + scale_fill_brewer(palette = \u0026quot;Greens\u0026quot;, name = \u0026quot;Type of Patch\u0026quot;) for(l in 1:time){ print(save_plots[[l]]) } Bo\\(^2\\)m =)\n ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"60067963cf3c50c8007925249cbd0c8c","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-09-10/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/computational_notes/2020-09-10/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Thomas Schelling's Model: Long Form","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A replication of Thomas Schelling’s model, which was originally published in The Journal of Mathematical Sociology. Yuk Tung Liu offers a great summary.\n The following map shows the distribution of people with different ethnicity living in the city of Chicago (source: radicalcartography.net):\n  Segregation may arise from social and economic reasons. However, Thomas Schelling, winner of the 2005 Nobel Memorial Prize in Economic Sciences, pointed out another possible reason. He constructed a simple model and used pennies and nickels on a graph paper to demonstrate that segregation can develop naturally even though each individual is tolerant towards another group. For example, if everyone requires at least half of his neighbors to be of the same color, the final outcome is a high degree of segregation. What Schelling demonstrated was that the “macrobehavior” in a society may not reflect the “micromotives” of its individual members.\n  Schelling’s model is an example of an agent-based model for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) on the overall system. Agent-based models are useful in simulating complex systems. An interesting phenomenon that can occur in a complex system is emergence, in which a structure or pattern arises in the system from the bottom up. As you will see, segregation is a result of emergence in the system described by the Schelling model. Members of each group do not consciously choose to live in a certain area, but the collective behavior of the individuals gives rise to segregation.\n Schelling Model Let’s start by situating people on a grid. The cells of the grid will contain a value, and that value will indicate one of three states: uninhabited (0), inhabited by a hockey player (1), or inhabited by a soccer player (2). Let’s use a 51x51 grid with 2000 occupied cells. A 51x51 grid contains 2601 cells in total.\nCreate a vector with 1000s 1s, 1000 2s, and the remaining 601 slots 0s.\ngroup 0 1 2 601 1000 1000  So far, all I have is a vector with a bunch of 1s, 2s, and 0s.\nNow, collate those numbers into a matrix through random sampling.\n[1] 0 2 2 0 [1] 1 1 0 1 0 [1] 1 0 2 2 2 2 1 Plot with base R\nPlot with ggplot2 - requires long data\n Var1 Var2 value 1 1 1 0 2 2 1 1 3 3 1 0 4 4 1 1 5 5 1 1 6 6 1 0 The grid is now filled with randomly dispersed hockey players, soccer players, and empty lots. The next step is to introduce a parameter that Schelling used in his original model. The similarity threshold, \\(z\\), takes a value between 0 and 1, and it measures how intolerant an agent is towards other athletes. An agent is satisfied if at least a fraction \\(z\\) of his neighbors belong to the same group – i.e., a hockey player likes to be around other hockey players. Mathematically, an agent is satisfied if the number of people around him is greater than \\(z\\). He is dissatisfied if he has fewer people of similar type around him. The smaller the value of \\(z\\), the more tolerant agents are of other groups.\nWith a similarity threshold of 0.30, a hockey player will move if fewer than 30% of his neighbors are other hockey players. A hockey player will stay if at least 30% of his neighbors are hockey players.\nHaving set the threshold, we now need a function to calculate how many neighbors are hockey players and how many are soccer players. This function spits back the similarity ratio, \\(r_{sim}\\). \\(r_{sim}\\) is a proportion: the number of neighbors of the same group divided by the total number of neighbors.\n\\[\\begin{equation} r_{sim} = \\dfrac{n_{same}}{n_{neighbors}} \\end{equation}\\]\nFor a hockey player, the ratio would become\n\\[\\begin{equation} r_{sim_{hockey}} = \\dfrac{n_{hockey}}{n_{neighbors}} \\end{equation}\\]\nHere is an example:\nIf I were a super programmer, I could create a function to do so. I’m not. Instead, I’ll create a function called get_neighbor_coords that returns the locations of every neighbor for agent \\(i\\). The function takes a vector parameter that houses agent \\(i\\)s location (e.g., [2, 13]). Then, it pulls the coordinates of each neighbor under the Moore paradigm (8 surrounding patches - clockwise).\nThe function returns a matrix with the coordinates of the surrounding 8 patches. If I was focused, for example, on agent [2, 3], then the function would return\n [,1] [,2] [1,] 3 3 [2,] 3 4 [3,] 2 4 [4,] 1 4 [5,] 1 3 [6,] 1 2 [7,] 2 2 [8,] 3 2 which shows that coordinate [3,3] is just below agent \\(i\\), coordinate [3,4] is just below and to the right, and coordinate [2,4] is directly to the right.\nNow we are ready to iterate across every agent (i.e., every cell in grid).\nBo\\(^2\\)m =)\n ","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599523200,"objectID":"1db167902caa5abba2a896e185c8f355","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-09-08/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/computational_notes/2020-09-08/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Thomas Schelling's Model","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" GitHub allows you to host one static website per repository. Here’s how to do it:\nSteps:\n Create a repository\n Create a ‘gh-pages’ branch\n   Make the ‘gh-pages’ branch your default   Delete the old ‘master’ branch\n Clone the repo to your local computer\n Create an index using whatever source-code you prefer\n   Commit and push the files\n GitHub will automatically render the site after a few moments\n  Bo\\(^2\\)m =)\n","date":1596326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596326400,"objectID":"db9c577c4aba33d328b7f86c72efcb40","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-08-02/","publishdate":"2020-08-02T00:00:00Z","relpermalink":"/computational_notes/2020-08-02/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Host Multiple Pages on GitHub","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Quick note on cloning, committing, and pushing anonymously on GitHub.\nSteps:\n Create an anonymous account and repo on GitHub\n Clone repo to local computer\n Navigate to it\n Configure anonymous username and email\n  git add . git commit -m \u0026quot;initial commit\u0026quot; git pull  Push using full specification  # replace username, password, and repository: git push \u0026#39;https://username:password@github.com/username/repository.git\u0026#39; Bo\\(^2\\)m =)\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"0d5b3333930f42cb0df10e4cccad85a3","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-08-01/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/computational_notes/2020-08-01/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Anonymous Push on GitHub","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" This post contains a bunch of examples where I practice counting dfs. In each example, I generate the data, estimate the parameters using SEM, count the dfs, and then compare my count to what the model spits back. To count dfs, I need to know the number of knowns and unknowns in my system:\n\\[\\begin{equation} \\textrm{DFs} = \\textrm{knowns - unknowns} \\end{equation}\\]\nTo count the number of knowns, I need to know the number of observed variables, p:\n\\[\\begin{equation} \\textrm{knowns} = p*(p+1) / 2 \\end{equation}\\]\nTo count the number of unknowns, I count the number of parameters that my model estimates. Now for the examples.\nExample 1 - Trust and availability cause helping DGP people \u0026lt;- 400 trust \u0026lt;- rnorm(people, 40, 2) availability \u0026lt;- rnorm(people, 20, 5) error \u0026lt;- rnorm(people, 0, 2) helping \u0026lt;- 3 + 0.2*trust + 0.7*availability + error  SEM library(tidyverse) library(lavaan) df \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:people), \u0026#39;trust\u0026#39; = c(trust), \u0026#39;availability\u0026#39; = c(availability), \u0026#39;helping\u0026#39; = c(helping) ) ex1_string \u0026lt;- \u0026#39; helping ~ b1*trust + b2*availability \u0026#39; ex1_model \u0026lt;- sem(ex1_string, data = df)   Count dfs Knowns (count the observed variables) # p*(p + 1) / 2 3*(3+1) / 2 ## [1] 6  Unknowns (count the estimated parameters)  1 for b1 1 for b2 1 for the variance of trust 1 for the variance of availability 1 for the covariance of trust and availability 1 for the prediction error on helping total = 6\n 6 - 6 = 0\n  show(ex1_model) ## lavaan 0.6-6 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 3 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 Now if I restrict the covariance of trust and availability to be zero I should have 1 df\nex1_string_restrict \u0026lt;- \u0026#39; helping ~ b1*trust + b2*availability trust ~~ 0*availability \u0026#39; ex1_model_restrict \u0026lt;- sem(ex1_string_restrict, data = df) show(ex1_model_restrict) # yup ## lavaan 0.6-6 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 0.027 ## Degrees of freedom 1 ## P-value (Chi-square) 0.868   Example 2 - Common factor underlying 6 observed items DGP common_factor \u0026lt;- rnorm(people, 30, 2) error_cf \u0026lt;- rnorm(people, 0, 2) item1 \u0026lt;- 0.35*common_factor + error_cf item2 \u0026lt;- 0.22*common_factor + error_cf item3 \u0026lt;- 0.18*common_factor + error_cf item4 \u0026lt;- 0.24*common_factor + error_cf item5 \u0026lt;- 0.31*common_factor + error_cf item6 \u0026lt;- 0.44*common_factor + error_cf # nope, that approach is wrong. If I do above then my errors are not independent # prediction errors (in this case measurement) should be independent item1 \u0026lt;- 0.35*common_factor + rnorm(people, 0, 2) item2 \u0026lt;- 0.22*common_factor + rnorm(people, 0, 2) item3 \u0026lt;- 0.18*common_factor + rnorm(people, 0, 2) item4 \u0026lt;- 0.24*common_factor + rnorm(people, 0, 2) item5 \u0026lt;- 0.31*common_factor + rnorm(people, 0, 2) item6 \u0026lt;- 0.44*common_factor + rnorm(people, 0, 2) df_cf \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:people), \u0026#39;item1\u0026#39; = c(item1), \u0026#39;item2\u0026#39; = c(item2), \u0026#39;item3\u0026#39; = c(item3), \u0026#39;item4\u0026#39; = c(item4), \u0026#39;item5\u0026#39; = c(item5), \u0026#39;item6\u0026#39; = c(item6) )  SEM ex2_string \u0026lt;- \u0026#39; com_factor =~ 1*item1 + fl2*item2 + fl3*item3 + fl4*item4 + fl5*item5 + fl6*item6 \u0026#39; ex2_model \u0026lt;- sem(ex2_string, data = df_cf)  Count dfs knowns (count the observed variables) # p*(p + 1) / 2 6*(6 + 1) / 2 ## [1] 21  unknowns (count the estimated parameters)  6 factor loadings, but I constrained the first one to be 1 (I have to to estimate the latent variable), so 5 parameters 5 measurement errors for the 5 factor loadings 1 variance for the latent exogenous variable 1 mean for the latent exogenous variable total = 12\n 21 - 12 = 9\n  show(ex2_model) ## lavaan 0.6-6 ended normally after 54 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 12 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 10.821 ## Degrees of freedom 9 ## P-value (Chi-square) 0.288    Example 3 - Two latent variables predict one observed outcome Cognitive ability (latent variable 1) and assertiveness (latent variable 2) predict productivity. Cognitive ability and assertiveness are both captured with 2 manifest items/variables.\nDGP # cog ability (latent exogenous variable 1) cog_ability \u0026lt;- rnorm(people, 100, 15) ca_item1 \u0026lt;- 0.78*cog_ability + rnorm(people, 0, 1) ca_item2 \u0026lt;- 0.11*cog_ability + rnorm(people, 0, 1) # assertiveness (latent exogenous variable 2) assertive \u0026lt;- rnorm(people, 30, 8) ass_item1 \u0026lt;- 0.81*assertive + rnorm(people, 0, 1) ass_item2 \u0026lt;- 0.34*assertive + rnorm(people, 0, 1) # productivity (observed outcome) productivity \u0026lt;- 0.55*cog_ability + 0.82*assertive + rnorm(people, 0, 5) # data df_3 \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:people), \u0026#39;ca_item1\u0026#39; = c(ca_item1), \u0026#39;ca_item2\u0026#39; = c(ca_item2), \u0026#39;ass_item1\u0026#39; = c(ass_item1), \u0026#39;ass_item2\u0026#39; = c(ass_item2), \u0026#39;productivity\u0026#39; = c(productivity) )  SEM ex3_string \u0026lt;- \u0026#39; cog_ability =~ 1*ca_item1 + fl2*ca_item2 assertiveness =~ 1*ass_item1 + fla*ass_item2 cog_ability ~~ cog_ability assertiveness ~~ assertiveness cog_ability ~~ assertiveness productivity ~ b1*cog_ability + b2*assertiveness \u0026#39; ex3_model \u0026lt;- sem(ex3_string, data = df_3)  Count dfs knowns (count the observed variables) # p*(p+1) / 2 5*(5+1) / 2 ## [1] 15  unknowns (count the estimated parameters)  4 factor loadings but I constrained 2 of them, so 2 factor loadings 2 measurement errors (4 items, but constrained 2 of them) 1 variance on cog ability 1 mean on cog ability 1 variance on assertiveness 1 mean on assertiveness 1 covariance among cog ability and assertiveness b1 b2 2 prediction errors total = 13\n 15 - 13 = 2\n  show(ex3_model) ## lavaan 0.6-6 ended normally after 197 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 12 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 6.623 ## Degrees of freedom 3 ## P-value (Chi-square) 0.085 Nope. I’m one off, where did I go wrong?\nAh, there is only 1 prediction error because productivity is being predicted. I counted 2 prediction errors because I gave one to both b1 and b2. So, the unknowns should be…\n 4 factor loadings but I constrained 2 of them, so 2 factor loadings 2 measurement errors (4 items, but constrained 2 of them) 1 variance on cog ability 1 mean on cog ability 1 variance on assertiveness 1 mean on assertiveness 1 covariance among cog ability and assertiveness b1 b2 1 prediction error total = 12\n 15 - 12 = 3\n  show(ex3_model) ## lavaan 0.6-6 ended normally after 197 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 12 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 6.623 ## Degrees of freedom 3 ## P-value (Chi-square) 0.085    Example 4 - a causes b, which causes c, which causes d DGP a \u0026lt;- rnorm(people, 300, 3) b \u0026lt;- 0.67*a + rnorm(people, 0, 1) c \u0026lt;- 0.99*b + rnorm(people, 0, 10) d \u0026lt;- 4 + 4*c + rnorm(people, 0, 4) df_chain \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:people), \u0026#39;a\u0026#39; = c(a), \u0026#39;b\u0026#39; = c(b), \u0026#39;c\u0026#39; = c(c), \u0026#39;d\u0026#39; = c(d) )  SEM ex4_string \u0026lt;- \u0026#39; b ~ b1*a c ~ b2*b d ~ b3*c a ~~ a \u0026#39; ex4_model \u0026lt;- sem(ex4_string, data = df_chain)  Count dfs knowns (count the observed variables) # p*(p+1) / 2 4*(4+1) / 2 ## [1] 10  unknowns (count the estimated parameters)  b1 b2 b3 3 prediction errors 1 variance for the lone exogenous variable (a) total = 7\n 10 - 7 = 3\n  show(ex4_model) ## lavaan 0.6-6 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 7 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 1.099 ## Degrees of freedom 3 ## P-value (Chi-square) 0.777    Example 5 - Observed affect over 7 time points DGP time \u0026lt;- 7 affect_store \u0026lt;- matrix(, ncol = 3, nrow = time*people) count \u0026lt;- 0 for(i in 1:people){ unob_het \u0026lt;- rnorm(1, 0, 3) for(j in 1:time){ count \u0026lt;- count + 1 if(j == 1){ affect_store[count, 1] \u0026lt;- i affect_store[count, 2] \u0026lt;- j affect_store[count, 3] \u0026lt;- unob_het + 50 + rnorm(1, 0, 1) }else{ affect_store[count, 1] \u0026lt;- i affect_store[count, 2] \u0026lt;- j affect_store[count, 3] \u0026lt;- 0.8*affect_store[count - 1, 3] + unob_het + rnorm(1, 0, 1) } } } df5 \u0026lt;- data.frame(affect_store) names(df5) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) library(reshape2) df5_wide \u0026lt;- reshape(df5, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;)  SEM ex5_string \u0026lt;- \u0026#39; unob_het =~ 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 affect.2 ~ ar*affect.1 affect.3 ~ ar*affect.2 affect.4 ~ ar*affect.3 affect.5 ~ ar*affect.4 affect.6 ~ ar*affect.5 affect.7 ~ ar*affect.6 affect.1 ~~ affect.1 unob_het ~~ unob_het affect.1 ~~ unob_het \u0026#39; ex5_model \u0026lt;- sem(ex5_string, data = df5_wide)  Count dfs knowns (count the observed variables) # p*(p+1) / 2 7*(7+1) / 2 ## [1] 28  unknowns (count the estimated parameters)  ar is 1 estimated parameter 1 variance of unobserved heterogeneity 1 variance of affect.1 1 covariance among affect.1 and unobserved heterogeneity 6 prediction errors total = 10\n 28 - 10 = 18\n  show(ex5_model) ## lavaan 0.6-6 ended normally after 121 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 15 ## Number of equality constraints 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 21.373 ## Degrees of freedom 18 ## P-value (Chi-square) 0.261 Why didn’t I estimate a mean for unobserved heterogeneity here? In all of the other examples I estimated the variance (1 parameter) and the mean (1 parameter) of the latent exogenous variable. In this case, unobserved heterogeneity is the latent exogenous variable but I only estimated its variance. That’s because in this model we don’t really care about the mean of unobserved heterogeneity, it’s just a latent variable that we incorporate to account for stable individual differences. In other words, when I estimate latent cog ability and assertiveness as IVs to predict an outcome, I care about their means. Here, unobserved heterogeneity is just an additional factor to account for, not a variable whose mean I really care to know. That said, if I wanted to estimate the mean of unobserved heterogeneity (which would result in one additional estimated parameter and one fewer df) then I would incorporate the following into the model string.\n\u0026#39; unob_het ~ 1 # lavaan code for estimating the mean of a latent variable \u0026#39; Bo\\(^2\\)m =)\n   ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"bd6a2af9056c7b3fd93c9f9d7411f771","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-07-13-dfs/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/computational_notes/2017-07-13-dfs/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Counting Degrees of Freedom","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" My last post demonstrated a dual change model for one variable, now I want to demonstrate a bivariate dual change model. A SEM path diagram for a bivariate dual change model is below, taken from Wang, Zhou, and Zhang (2016)\n(If you do not have access to that link you can view a similar path diagram in Jones, King, Gilrane, McCausland, Cortina, \u0026amp; Grimm, 2016)\nEssentially, we have two dual change processes and a coupling parameter from the latent true score on one variable to the latent change score on the other.\nThe DGP \\[\\begin{equation} y_t = constant_y + (1 + proportion_y)*y_{t-1} + coupling_{xy}*x_{t-1} + e \\end{equation}\\]\nwhere \\(constant_y\\) is the change factor (or latent slope) on \\(y\\), \\(proportion_y\\) is the proportion change factor, and \\(coupling_xy\\) is the coupling parameter relating \\(x\\) to \\(y\\). The DGP for \\(x\\) is\n\\[\\begin{equation} x_t = constant_x + (1 + proportion_x)*x_{t-1} + coupling_{yx}*y_{t-1} + e \\end{equation}\\]\nwhere the terms are similar but now applied to values of \\(x\\). The true values used in the DGP are:\n\\[\\begin{align} y_t \u0026amp;= 0.5 + (1 + -0.32)y_{t-1} + 0.4x_{t-1} + e \\\\ x_t \u0026amp;= 0.5 + (1 + 0.22)x_{t-1} - 0.4y_{t-1} + e \\end{align}\\]\nwith initial values for both \\(x\\) and \\(y\\) sampled from \\(N\\) ~ (10, 1).\npeople \u0026lt;- 700 time \u0026lt;- 6 x_cause_y \u0026lt;- 0.4 y_cause_x \u0026lt;- -0.4 const_x \u0026lt;- 0.5 const_y \u0026lt;- 0.5 prop_x \u0026lt;- 0.22 prop_y \u0026lt;- -0.32 df_mat \u0026lt;- matrix(, ncol = 4, nrow = people*time) count \u0026lt;- 0 for(i in 1:people){ unob_het_y \u0026lt;- rnorm(1, 0, 3) unob_het_x \u0026lt;- rnorm(1, 0, 3) for(j in 1:time){ count \u0026lt;- count + 1 if(j == 1){ df_mat[count, 1] \u0026lt;- i df_mat[count, 2] \u0026lt;- j df_mat[count, 3] \u0026lt;- rnorm(1, 10, 1) df_mat[count, 4] \u0026lt;- rnorm(1, 10, 1) }else{ df_mat[count, 1] \u0026lt;- i df_mat[count, 2] \u0026lt;- j df_mat[count, 3] \u0026lt;- const_x + (1+prop_x)*df_mat[count - 1, 3] + y_cause_x*df_mat[count - 1, 4] + unob_het_x + rnorm(1,0,1) df_mat[count, 4] \u0026lt;- const_y + (1+prop_y)*df_mat[count - 1, 4] + x_cause_y*df_mat[count - 1, 3] + unob_het_y + rnorm(1,0,1) } } } library(tidyverse) library(ggplot2) library(reshape2) df \u0026lt;- data.frame(df_mat) names(df) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;) Values of \\(y\\) over time.\nrandom_nums \u0026lt;- sample(c(1:700), 6) df_sample \u0026lt;- df %\u0026gt;% filter(id %in% random_nums) ggplot(df, aes(x = time, y = y, group = id)) + geom_point(color = \u0026#39;grey85\u0026#39;) + geom_line(color = \u0026#39;grey85\u0026#39;) + geom_point(data = df_sample, aes(x = time, y = y, group = id)) + geom_line(data = df_sample, aes(x = time, y = y, group = id)) Values of \\(x\\) over time.\nplot_single_response \u0026lt;- function(y_axis){ plot_it \u0026lt;- ggplot(df, aes(x = time, y = !!y_axis, group = id)) + geom_point(color = \u0026#39;grey85\u0026#39;) + geom_line(color = \u0026#39;grey85\u0026#39;) + geom_point(data = df_sample, aes(x = time, y = !!y_axis, group = id)) + geom_line(data = df_sample, aes(x = time, y = !!y_axis, group = id)) return(plot_it) } plot_single_response(quo(x)) Three randomly selected individuals with \\(x\\) and \\(y\\) plotted simultaneously.\nthree_cases \u0026lt;- df %\u0026gt;% filter(id == 4 | id == 500 | id == 322) %\u0026gt;% gather(x, y, key = \u0026#39;variable\u0026#39;, value = \u0026#39;response\u0026#39;) ggplot(three_cases, aes(x = time, y = response, color = variable)) + geom_point() + geom_line() + facet_wrap(~id)  Dual Change Model on Y df_wide_y \u0026lt;- df %\u0026gt;% select(id, time, y) %\u0026gt;% reshape(idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) library(lavaan) dual_change_y_string \u0026lt;- \u0026#39; # latent true scores over y ly1 =~ 1*y.1 ly2 =~ 1*y.2 ly3 =~ 1*y.3 ly4 =~ 1*y.4 ly5 =~ 1*y.5 ly6 =~ 1*y.6 # latent change scores over the true scores (but not the first time point) cy2 =~ 1*ly2 cy3 =~ 1*ly3 cy4 =~ 1*ly4 cy5 =~ 1*ly5 cy6 =~ 1*ly6 # autoregressions of latent true scores over y constrained to 1 ly2 ~ 1*ly1 ly3 ~ 1*ly2 ly4 ~ 1*ly3 ly5 ~ 1*ly4 ly6 ~ 1*ly5 # latent intercept over first latent true score on y l_intercept =~ 1*ly1 # change component 1 of the dual change model # latent slope (or change factor) over the change scores l_slope =~ 1*cy2 + 1*cy3 + 1*cy4 + 1*cy5 + 1*cy6 # estimate means and variances of those intercept and slope terms l_intercept ~~ l_intercept l_slope ~~ l_slope l_slope ~ 1 l_intercept ~ 1 # and a covariance between them l_intercept ~~ l_slope # change component 2 of the dual change model # proportion change from true scores over y to the change factors cy2 ~ prop*ly1 cy3 ~ prop*ly2 cy4 ~ prop*ly3 cy5 ~ prop*ly4 cy6 ~ prop*ly5 # means and variances of latent factors set to zero ly1 ~ 0 ly2 ~ 0 ly3 ~ 0 ly4 ~ 0 ly5 ~ 0 ly6 ~ 0 cy2 ~ 0 cy3 ~ 0 cy4 ~ 0 cy5 ~ 0 cy6 ~ 0 ly1 ~~ 0*ly1 ly2 ~~ 0*ly2 ly3 ~~ 0*ly3 ly4 ~~ 0*ly4 ly5 ~~ 0*ly5 ly6 ~~ 0*ly6 cy2 ~~ 0*cy2 cy3 ~~ 0*cy3 cy4 ~~ 0*cy4 cy5 ~~ 0*cy5 cy6 ~~ 0*cy6 # means of indicators to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to equality across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # do not allow change factors to correlate cy2 ~~ 0*cy3 + 0*cy4 + 0*cy5 + 0*cy6 cy3 ~~ 0*cy4 + 0*cy5 + 0*cy6 cy4 ~~ 0*cy5 + 0*cy6 cy5 ~~ 0*cy6 \u0026#39; dc_y_model \u0026lt;- sem(dual_change_y_string, data = df_wide_y) summary(dc_y_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 57 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 16 ## Number of equality constraints 9 ## ## Number of observations 700 ## ## Model Test User Model: ## ## Test statistic 6389.847 ## Degrees of freedom 20 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 8286.339 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.230 ## Tucker-Lewis Index (TLI) 0.422 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11708.642 ## Loglikelihood unrestricted model (H1) -8513.718 ## ## Akaike (AIC) 23431.283 ## Bayesian (BIC) 23463.141 ## Sample-size adjusted Bayesian (BIC) 23440.914 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.675 ## 90 Percent confidence interval - lower 0.661 ## 90 Percent confidence interval - upper 0.688 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 2.363 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ly1 =~ ## y.1 1.000 ## ly2 =~ ## y.2 1.000 ## ly3 =~ ## y.3 1.000 ## ly4 =~ ## y.4 1.000 ## ly5 =~ ## y.5 1.000 ## ly6 =~ ## y.6 1.000 ## cy2 =~ ## ly2 1.000 ## cy3 =~ ## ly3 1.000 ## cy4 =~ ## ly4 1.000 ## cy5 =~ ## ly5 1.000 ## cy6 =~ ## ly6 1.000 ## l_intercept =~ ## ly1 1.000 ## l_slope =~ ## cy2 1.000 ## cy3 1.000 ## cy4 1.000 ## cy5 1.000 ## cy6 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ly2 ~ ## ly1 1.000 ## ly3 ~ ## ly2 1.000 ## ly4 ~ ## ly3 1.000 ## ly5 ~ ## ly4 1.000 ## ly6 ~ ## ly5 1.000 ## cy2 ~ ## ly1 (prop) 0.250 0.019 13.327 0.000 ## cy3 ~ ## ly2 (prop) 0.250 0.019 13.327 0.000 ## cy4 ~ ## ly3 (prop) 0.250 0.019 13.327 0.000 ## cy5 ~ ## ly4 (prop) 0.250 0.019 13.327 0.000 ## cy6 ~ ## ly5 (prop) 0.250 0.019 13.327 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_intercept ~~ ## l_slope -1.394 0.223 -6.256 0.000 ## .cy2 ~~ ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .cy3 ~~ ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .cy4 ~~ ## .cy5 0.000 ## .cy6 0.000 ## .cy5 ~~ ## .cy6 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_slope -3.285 0.213 -15.445 0.000 ## l_intercept 11.707 0.114 102.723 0.000 ## .ly1 0.000 ## .ly2 0.000 ## .ly3 0.000 ## .ly4 0.000 ## .ly5 0.000 ## .ly6 0.000 ## .cy2 0.000 ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_ntrcp 5.916 0.485 12.193 0.000 ## l_slope 2.273 0.201 11.307 0.000 ## .ly1 0.000 ## .ly2 0.000 ## .ly3 0.000 ## .ly4 0.000 ## .ly5 0.000 ## .ly6 0.000 ## .cy2 0.000 ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .y.1 (rs_v) 7.123 0.190 37.417 0.000 ## .y.2 (rs_v) 7.123 0.190 37.417 0.000 ## .y.3 (rs_v) 7.123 0.190 37.417 0.000 ## .y.4 (rs_v) 7.123 0.190 37.417 0.000 ## .y.5 (rs_v) 7.123 0.190 37.417 0.000 ## .y.6 (rs_v) 7.123 0.190 37.417 0.000 Code to change the \\(y\\)’s in the string to \\(x\\)’s without manually deleting and inserting \\(x\\) into the string above. All you have to do is paste the string into a .txt document and save the file as “y_file.txt”\nlibrary(readr) mystring \u0026lt;- read_file(\u0026#39;y_file.txt\u0026#39;) new_data \u0026lt;- gsub(\u0026#39;y\u0026#39;, \u0026#39;x\u0026#39;, mystring) # write_file(new_data, path = \u0026#39;x_file.txt\u0026#39;) # not executed but will work  Dual Change Model on X df_wide_x \u0026lt;- df %\u0026gt;% select(id, time, x) %\u0026gt;% reshape(idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) library(lavaan) dual_change_x_string \u0026lt;- \u0026#39; # latent true scores over x lx1 =~ 1*x.1 lx2 =~ 1*x.2 lx3 =~ 1*x.3 lx4 =~ 1*x.4 lx5 =~ 1*x.5 lx6 =~ 1*x.6 # latent change scores over the true scores (but not the first time point) cx2 =~ 1*lx2 cx3 =~ 1*lx3 cx4 =~ 1*lx4 cx5 =~ 1*lx5 cx6 =~ 1*lx6 # autoregressions of latent true scores over x constrained to 1 lx2 ~ 1*lx1 lx3 ~ 1*lx2 lx4 ~ 1*lx3 lx5 ~ 1*lx4 lx6 ~ 1*lx5 # latent intercept over first latent true score on x l_intercept =~ 1*lx1 # change component 1 of the dual change model # latent slope (or change factor) over the change scores l_slope =~ 1*cx2 + 1*cx3 + 1*cx4 + 1*cx5 + 1*cx6 # estimate means and variances of those intercept and slope terms l_intercept ~~ l_intercept l_slope ~~ l_slope l_slope ~ 1 l_intercept ~ 1 # and a covariance between them l_intercept ~~ l_slope # change component 2 of the dual change model # proportion change from true scores over x to the change factors cx2 ~ prop*lx1 cx3 ~ prop*lx2 cx4 ~ prop*lx3 cx5 ~ prop*lx4 cx6 ~ prop*lx5 # means and variances of latent factors set to zero lx1 ~ 0 lx2 ~ 0 lx3 ~ 0 lx4 ~ 0 lx5 ~ 0 lx6 ~ 0 cx2 ~ 0 cx3 ~ 0 cx4 ~ 0 cx5 ~ 0 cx6 ~ 0 lx1 ~~ 0*lx1 lx2 ~~ 0*lx2 lx3 ~~ 0*lx3 lx4 ~~ 0*lx4 lx5 ~~ 0*lx5 lx6 ~~ 0*lx6 cx2 ~~ 0*cx2 cx3 ~~ 0*cx3 cx4 ~~ 0*cx4 cx5 ~~ 0*cx5 cx6 ~~ 0*cx6 # means of indicators to zero x.1 ~ 0 x.2 ~ 0 x.3 ~ 0 x.4 ~ 0 x.5 ~ 0 x.6 ~ 0 # residual variances constrained to equalitx across time x.1 ~~ res_var*x.1 x.2 ~~ res_var*x.2 x.3 ~~ res_var*x.3 x.4 ~~ res_var*x.4 x.5 ~~ res_var*x.5 x.6 ~~ res_var*x.6 # do not allow change factors to correlate cx2 ~~ 0*cx3 + 0*cx4 + 0*cx5 + 0*cx6 cx3 ~~ 0*cx4 + 0*cx5 + 0*cx6 cx4 ~~ 0*cx5 + 0*cx6 cx5 ~~ 0*cx6 \u0026#39; dc_x_model \u0026lt;- sem(dual_change_x_string, data = df_wide_x) summary(dc_x_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 64 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 16 ## Number of equality constraints 9 ## ## Number of observations 700 ## ## Model Test User Model: ## ## Test statistic 4424.279 ## Degrees of freedom 20 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 11743.090 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.624 ## Tucker-Lewis Index (TLI) 0.718 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10266.470 ## Loglikelihood unrestricted model (H1) -8054.331 ## ## Akaike (AIC) 20546.941 ## Bayesian (BIC) 20578.798 ## Sample-size adjusted Bayesian (BIC) 20556.572 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.561 ## 90 Percent confidence interval - lower 0.547 ## 90 Percent confidence interval - upper 0.575 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.901 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lx1 =~ ## x.1 1.000 ## lx2 =~ ## x.2 1.000 ## lx3 =~ ## x.3 1.000 ## lx4 =~ ## x.4 1.000 ## lx5 =~ ## x.5 1.000 ## lx6 =~ ## x.6 1.000 ## cx2 =~ ## lx2 1.000 ## cx3 =~ ## lx3 1.000 ## cx4 =~ ## lx4 1.000 ## cx5 =~ ## lx5 1.000 ## cx6 =~ ## lx6 1.000 ## l_intercept =~ ## lx1 1.000 ## l_slope =~ ## cx2 1.000 ## cx3 1.000 ## cx4 1.000 ## cx5 1.000 ## cx6 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lx2 ~ ## lx1 1.000 ## lx3 ~ ## lx2 1.000 ## lx4 ~ ## lx3 1.000 ## lx5 ~ ## lx4 1.000 ## lx6 ~ ## lx5 1.000 ## cx2 ~ ## lx1 (prop) 0.157 0.004 35.120 0.000 ## cx3 ~ ## lx2 (prop) 0.157 0.004 35.120 0.000 ## cx4 ~ ## lx3 (prop) 0.157 0.004 35.120 0.000 ## cx5 ~ ## lx4 (prop) 0.157 0.004 35.120 0.000 ## cx6 ~ ## lx5 (prop) 0.157 0.004 35.120 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_intercept ~~ ## l_slope -0.838 0.257 -3.259 0.001 ## .cx2 ~~ ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .cx3 ~~ ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .cx4 ~~ ## .cx5 0.000 ## .cx6 0.000 ## .cx5 ~~ ## .cx6 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_slope -3.550 0.122 -29.197 0.000 ## l_intercept 10.396 0.079 131.733 0.000 ## .lx1 0.000 ## .lx2 0.000 ## .lx3 0.000 ## .lx4 0.000 ## .lx5 0.000 ## .lx6 0.000 ## .cx2 0.000 ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .x.1 0.000 ## .x.2 0.000 ## .x.3 0.000 ## .x.4 0.000 ## .x.5 0.000 ## .x.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_ntrcp 3.105 0.224 13.844 0.000 ## l_slope 9.678 0.559 17.306 0.000 ## .lx1 0.000 ## .lx2 0.000 ## .lx3 0.000 ## .lx4 0.000 ## .lx5 0.000 ## .lx6 0.000 ## .cx2 0.000 ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .x.1 (rs_v) 2.309 0.062 37.417 0.000 ## .x.2 (rs_v) 2.309 0.062 37.417 0.000 ## .x.3 (rs_v) 2.309 0.062 37.417 0.000 ## .x.4 (rs_v) 2.309 0.062 37.417 0.000 ## .x.5 (rs_v) 2.309 0.062 37.417 0.000 ## .x.6 (rs_v) 2.309 0.062 37.417 0.000  Bivariate Dual Change Model bi_dc_string \u0026lt;- \u0026#39; # DUAL CHANGE IN Y # # # # latent true scores over y ly1 =~ 1*y.1 ly2 =~ 1*y.2 ly3 =~ 1*y.3 ly4 =~ 1*y.4 ly5 =~ 1*y.5 ly6 =~ 1*y.6 # latent change scores over the true scores (but not the first time point) cy2 =~ 1*ly2 cy3 =~ 1*ly3 cy4 =~ 1*ly4 cy5 =~ 1*ly5 cy6 =~ 1*ly6 # autoregressions of latent true scores over y constrained to 1 ly2 ~ 1*ly1 ly3 ~ 1*ly2 ly4 ~ 1*ly3 ly5 ~ 1*ly4 ly6 ~ 1*ly5 # latent intercept over first latent true score on y l_intercept =~ 1*ly1 # change component 1 of the dual change model # latent slope (or change factor) over the change scores l_slope =~ 1*cy2 + 1*cy3 + 1*cy4 + 1*cy5 + 1*cy6 # estimate means and variances of those intercept and slope terms l_intercept ~~ l_intercept l_slope ~~ l_slope l_slope ~ 1 l_intercept ~ 1 # and a covariance between them l_intercept ~~ l_slope # change component 2 of the dual change model # proportion change from true scores over y to the change factors cy2 ~ prop*ly1 cy3 ~ prop*ly2 cy4 ~ prop*ly3 cy5 ~ prop*ly4 cy6 ~ prop*ly5 # means and variances of latent factors set to zero ly1 ~ 0 ly2 ~ 0 ly3 ~ 0 ly4 ~ 0 ly5 ~ 0 ly6 ~ 0 cy2 ~ 0 cy3 ~ 0 cy4 ~ 0 cy5 ~ 0 cy6 ~ 0 ly1 ~~ 0*ly1 ly2 ~~ 0*ly2 ly3 ~~ 0*ly3 ly4 ~~ 0*ly4 ly5 ~~ 0*ly5 ly6 ~~ 0*ly6 cy2 ~~ 0*cy2 cy3 ~~ 0*cy3 cy4 ~~ 0*cy4 cy5 ~~ 0*cy5 cy6 ~~ 0*cy6 # means of indicators to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to equality across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # do not allow change factors to correlate cy2 ~~ 0*cy3 + 0*cy4 + 0*cy5 + 0*cy6 cy3 ~~ 0*cy4 + 0*cy5 + 0*cy6 cy4 ~~ 0*cy5 + 0*cy6 cy5 ~~ 0*cy6 # DUAL CHANGE IN X # # # # latent true scores over x lx1 =~ 1*x.1 lx2 =~ 1*x.2 lx3 =~ 1*x.3 lx4 =~ 1*x.4 lx5 =~ 1*x.5 lx6 =~ 1*x.6 # latent change scores over the true scores (but not the first time point) cx2 =~ 1*lx2 cx3 =~ 1*lx3 cx4 =~ 1*lx4 cx5 =~ 1*lx5 cx6 =~ 1*lx6 # autoregressions of latent true scores over x constrained to 1 lx2 ~ 1*lx1 lx3 ~ 1*lx2 lx4 ~ 1*lx3 lx5 ~ 1*lx4 lx6 ~ 1*lx5 # latent intercept over first latent true score on x lx_intercept =~ 1*lx1 # change component 1 of the dual change model # latent slope (or change factor) over the change scores lx_slope =~ 1*cx2 + 1*cx3 + 1*cx4 + 1*cx5 + 1*cx6 # estimate means and variances of those intercept and slope terms lx_intercept ~~ lx_intercept lx_slope ~~ lx_slope lx_slope ~ 1 lx_intercept ~ 1 # and a covariance between them lx_intercept ~~ lx_slope # change component 2 of the dual change model # proportion change from true scores over x to the change factors cx2 ~ propx*lx1 cx3 ~ propx*lx2 cx4 ~ propx*lx3 cx5 ~ propx*lx4 cx6 ~ propx*lx5 # means and variances of latent factors set to zero lx1 ~ 0 lx2 ~ 0 lx3 ~ 0 lx4 ~ 0 lx5 ~ 0 lx6 ~ 0 cx2 ~ 0 cx3 ~ 0 cx4 ~ 0 cx5 ~ 0 cx6 ~ 0 lx1 ~~ 0*lx1 lx2 ~~ 0*lx2 lx3 ~~ 0*lx3 lx4 ~~ 0*lx4 lx5 ~~ 0*lx5 lx6 ~~ 0*lx6 cx2 ~~ 0*cx2 cx3 ~~ 0*cx3 cx4 ~~ 0*cx4 cx5 ~~ 0*cx5 cx6 ~~ 0*cx6 # means of indicators to zero x.1 ~ 0 x.2 ~ 0 x.3 ~ 0 x.4 ~ 0 x.5 ~ 0 x.6 ~ 0 # residual variances constrained to equalitx across time x.1 ~~ res_varx*x.1 x.2 ~~ res_varx*x.2 x.3 ~~ res_varx*x.3 x.4 ~~ res_varx*x.4 x.5 ~~ res_varx*x.5 x.6 ~~ res_varx*x.6 # do not allow change factors to correlate cx2 ~~ 0*cx3 + 0*cx4 + 0*cx5 + 0*cx6 cx3 ~~ 0*cx4 + 0*cx5 + 0*cx6 cx4 ~~ 0*cx5 + 0*cx6 cx5 ~~ 0*cx6 # COUPLING # # cy2 ~ xy*lx1 cy3 ~ xy*lx2 cy4 ~ xy*lx3 cy5 ~ xy*lx4 cy6 ~ xy*lx5 cx2 ~ yx*ly1 cx3 ~ yx*ly2 cx4 ~ yx*ly3 cx5 ~ yx*ly4 cx6 ~ yx*ly5 \u0026#39; df_both \u0026lt;- df %\u0026gt;% reshape(idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) bi_dc_model \u0026lt;- sem(bi_dc_string, data = df_both) summary(bi_dc_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 123 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 46 ## Number of equality constraints 26 ## ## Number of observations 700 ## ## Model Test User Model: ## ## Test statistic 1546.104 ## Degrees of freedom 70 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 24109.208 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.939 ## Tucker-Lewis Index (TLI) 0.942 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -15301.211 ## Loglikelihood unrestricted model (H1) -14528.159 ## ## Akaike (AIC) 30642.422 ## Bayesian (BIC) 30733.444 ## Sample-size adjusted Bayesian (BIC) 30669.940 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.174 ## 90 Percent confidence interval - lower 0.166 ## 90 Percent confidence interval - upper 0.181 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.102 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ly1 =~ ## y.1 1.000 ## ly2 =~ ## y.2 1.000 ## ly3 =~ ## y.3 1.000 ## ly4 =~ ## y.4 1.000 ## ly5 =~ ## y.5 1.000 ## ly6 =~ ## y.6 1.000 ## cy2 =~ ## ly2 1.000 ## cy3 =~ ## ly3 1.000 ## cy4 =~ ## ly4 1.000 ## cy5 =~ ## ly5 1.000 ## cy6 =~ ## ly6 1.000 ## l_intercept =~ ## ly1 1.000 ## l_slope =~ ## cy2 1.000 ## cy3 1.000 ## cy4 1.000 ## cy5 1.000 ## cy6 1.000 ## lx1 =~ ## x.1 1.000 ## lx2 =~ ## x.2 1.000 ## lx3 =~ ## x.3 1.000 ## lx4 =~ ## x.4 1.000 ## lx5 =~ ## x.5 1.000 ## lx6 =~ ## x.6 1.000 ## cx2 =~ ## lx2 1.000 ## cx3 =~ ## lx3 1.000 ## cx4 =~ ## lx4 1.000 ## cx5 =~ ## lx5 1.000 ## cx6 =~ ## lx6 1.000 ## lx_intercept =~ ## lx1 1.000 ## lx_slope =~ ## cx2 1.000 ## cx3 1.000 ## cx4 1.000 ## cx5 1.000 ## cx6 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ly2 ~ ## ly1 1.000 ## ly3 ~ ## ly2 1.000 ## ly4 ~ ## ly3 1.000 ## ly5 ~ ## ly4 1.000 ## ly6 ~ ## ly5 1.000 ## cy2 ~ ## ly1 (prop) -0.315 0.004 -77.421 0.000 ## cy3 ~ ## ly2 (prop) -0.315 0.004 -77.421 0.000 ## cy4 ~ ## ly3 (prop) -0.315 0.004 -77.421 0.000 ## cy5 ~ ## ly4 (prop) -0.315 0.004 -77.421 0.000 ## cy6 ~ ## ly5 (prop) -0.315 0.004 -77.421 0.000 ## lx2 ~ ## lx1 1.000 ## lx3 ~ ## lx2 1.000 ## lx4 ~ ## lx3 1.000 ## lx5 ~ ## lx4 1.000 ## lx6 ~ ## lx5 1.000 ## cx2 ~ ## lx1 (prpx) 0.221 0.002 100.363 0.000 ## cx3 ~ ## lx2 (prpx) 0.221 0.002 100.363 0.000 ## cx4 ~ ## lx3 (prpx) 0.221 0.002 100.363 0.000 ## cx5 ~ ## lx4 (prpx) 0.221 0.002 100.363 0.000 ## cx6 ~ ## lx5 (prpx) 0.221 0.002 100.363 0.000 ## cy2 ~ ## lx1 (xy) 0.402 0.002 188.751 0.000 ## cy3 ~ ## lx2 (xy) 0.402 0.002 188.751 0.000 ## cy4 ~ ## lx3 (xy) 0.402 0.002 188.751 0.000 ## cy5 ~ ## lx4 (xy) 0.402 0.002 188.751 0.000 ## cy6 ~ ## lx5 (xy) 0.402 0.002 188.751 0.000 ## cx2 ~ ## ly1 (yx) -0.428 0.004 -101.317 0.000 ## cx3 ~ ## ly2 (yx) -0.428 0.004 -101.317 0.000 ## cx4 ~ ## ly3 (yx) -0.428 0.004 -101.317 0.000 ## cx5 ~ ## ly4 (yx) -0.428 0.004 -101.317 0.000 ## cx6 ~ ## ly5 (yx) -0.428 0.004 -101.317 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_intercept ~~ ## l_slope 0.324 0.149 2.167 0.030 ## .cy2 ~~ ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .cy3 ~~ ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .cy4 ~~ ## .cy5 0.000 ## .cy6 0.000 ## .cy5 ~~ ## .cy6 0.000 ## lx_intercept ~~ ## lx_slope 0.113 0.129 0.877 0.381 ## .cx2 ~~ ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .cx3 ~~ ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .cx4 ~~ ## .cx5 0.000 ## .cx6 0.000 ## .cx5 ~~ ## .cx6 0.000 ## l_intercept ~~ ## lx_intercept -0.227 0.051 -4.436 0.000 ## lx_slope 0.088 0.144 0.609 0.543 ## l_slope ~~ ## lx_intercept -0.071 0.135 -0.524 0.601 ## lx_slope 0.483 0.358 1.348 0.178 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_slope 0.605 0.124 4.892 0.000 ## l_intercept 10.055 0.047 214.190 0.000 ## .ly1 0.000 ## .ly2 0.000 ## .ly3 0.000 ## .ly4 0.000 ## .ly5 0.000 ## .ly6 0.000 ## .cy2 0.000 ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## lx_slope 0.857 0.122 7.050 0.000 ## lx_intercept 9.975 0.042 235.419 0.000 ## .lx1 0.000 ## .lx2 0.000 ## .lx3 0.000 ## .lx4 0.000 ## .lx5 0.000 ## .lx6 0.000 ## .cx2 0.000 ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .x.1 0.000 ## .x.2 0.000 ## .x.3 0.000 ## .x.4 0.000 ## .x.5 0.000 ## .x.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_ntr 1.069 0.080 13.399 0.000 ## l_slp 9.410 0.520 18.097 0.000 ## .ly1 0.000 ## .ly2 0.000 ## .ly3 0.000 ## .ly4 0.000 ## .ly5 0.000 ## .ly6 0.000 ## .cy2 0.000 ## .cy3 0.000 ## .cy4 0.000 ## .cy5 0.000 ## .cy6 0.000 ## .y.1 (res_vr) 0.729 0.020 37.350 0.000 ## .y.2 (res_vr) 0.729 0.020 37.350 0.000 ## .y.3 (res_vr) 0.729 0.020 37.350 0.000 ## .y.4 (res_vr) 0.729 0.020 37.350 0.000 ## .y.5 (res_vr) 0.729 0.020 37.350 0.000 ## .y.6 (res_vr) 0.729 0.020 37.350 0.000 ## lx_nt 1.022 0.065 15.701 0.000 ## lx_sl 8.955 0.489 18.297 0.000 ## .lx1 0.000 ## .lx2 0.000 ## .lx3 0.000 ## .lx4 0.000 ## .lx5 0.000 ## .lx6 0.000 ## .cx2 0.000 ## .cx3 0.000 ## .cx4 0.000 ## .cx5 0.000 ## .cx6 0.000 ## .x.1 (rs_vrx) 0.432 0.012 36.464 0.000 ## .x.2 (rs_vrx) 0.432 0.012 36.464 0.000 ## .x.3 (rs_vrx) 0.432 0.012 36.464 0.000 ## .x.4 (rs_vrx) 0.432 0.012 36.464 0.000 ## .x.5 (rs_vrx) 0.432 0.012 36.464 0.000 ## .x.6 (rs_vrx) 0.432 0.012 36.464 0.000 Bo\\(^2\\)m =)\n ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"759fe4a24d59593c72e1b77163b64a8d","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-05-26/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/computational_notes/2020-05-26/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Bivariate Latent Dual Change Model","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I begin with an intercept-only model in a latent change framework and then build to a full dual change model. SEM images in this post come from a lecture by Amy Nuttall. Two notes about the models and code below. First, the initial models will not fit well because they are too simple. The DGP uses both constant and proportion change (hence, “dual-change”) whereas the first few models only estimate an intercept. Second, I use the sem rather than growth command in lavaan because it forces me to specify the entire model. I do not like using commands that make automatic constraints for me – if you do, you are much more likely to make a mistake or not know what your model is doing.\nDGP The underlying DGP will be the same throughout this exercise. Consistent with Ghisletta and McArdle, 2012, we have:\n\\[\\begin{equation} y_t = \\alpha*b_1 + (1 + b_2)*y_{t-1} \\end{equation}\\]\nwhere \\(b_1\\) is the constant change (similar to the “slope” term in a basic growth model, in latent change frameworks it is called the “change factor”) and \\(b_2\\) is the proportion change, or the change from point to point. The values specified in the DGP are\n\\[\\begin{equation} y_t = 1*0.3 + (1 + -0.4)*y_{t-1} \\end{equation}\\]\nwhere \\(b_1\\) is equal to 0.3 and \\(b_2\\) is equal to -0.4. Let’s generate data for 500 people across six time points.\nconstant \u0026lt;- 0.3 proportion \u0026lt;- -0.4 people \u0026lt;- 500 time \u0026lt;- 6 df \u0026lt;- matrix(, nrow = people*time, ncol = 3) count \u0026lt;- 0 for(i in 1:people){ y_het \u0026lt;- rnorm(1, 0, 2) for(j in 1:time){ count \u0026lt;- count + 1 if(j == 1){ df[count, 1] \u0026lt;- i df[count, 2] \u0026lt;- j df[count, 3] \u0026lt;- y_het + rnorm(1,0,1) }else{ df[count, 1] \u0026lt;- i df[count, 2] \u0026lt;- j df[count, 3] \u0026lt;- 1*constant + (1+proportion)*df[count - 1, 3] + y_het + rnorm(1,0,1) } } } df \u0026lt;- data.frame(df) names(df) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;y\u0026#39;) random_ids \u0026lt;- sample(1:people, 5) sample_df \u0026lt;- df %\u0026gt;% filter(id %in% random_ids) ggplot(df, aes(x = time, y = y, group = id)) + geom_point(color = \u0026#39;grey85\u0026#39;) + geom_line(color = \u0026#39;grey85\u0026#39;) + geom_point(data = sample_df, aes(x = time, y = y, group = id)) + geom_line(data = sample_df, aes(x = time, y = y, group = id)) Change the data to wide and load lavaan before we start modeling.\ndf_wide \u0026lt;- reshape(df, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) library(lavaan)  Intercept Only Model Similar to the intercept-only model in a “non-latent change” framework (i.e., a simple growth model), the intercept-only model here contains a latent variable over the first observation.\nThere are six observations of \\(y\\) and each is predicted by its latent “true score.” The first true score term is regressed on a latent intercept. The other true scores are regressed on additional latent variables that represent latent change. We don’t have anything relating to those latent change score terms yet so they don’t do much in this model. The autoregressive paths from true score to true score are constrained to 1. Here is how we estimate it.\nint_only_string \u0026lt;- \u0026#39; # latent true scores over the observed y points l_y1 =~ 1*y.1 l_y2 =~ 1*y.2 l_y3 =~ 1*y.3 l_y4 =~ 1*y.4 l_y5 =~ 1*y.5 l_y6 =~ 1*y.6 # latent change scores over the latent true scores # y1 does not get one because it is the first time point lc_y2 =~ 1*l_y2 lc_y3 =~ 1*l_y3 lc_y4 =~ 1*l_y4 lc_y5 =~ 1*l_y5 lc_y6 =~ 1*l_y6 # autoregression of the latent true scores l_y2 ~ 1*l_y1 l_y3 ~ 1*l_y2 l_y4 ~ 1*l_y3 l_y5 ~ 1*l_y4 l_y6 ~ 1*l_y5 # latent intercept over the first true score of y latent_intercept =~ 1*l_y1 # estimate mean and variance of latent intercept latent_intercept ~~ latent_intercept latent_intercept ~ 1 # means and variances of latent factors set to zero l_y1 ~ 0 l_y2 ~ 0 l_y3 ~ 0 l_y4 ~ 0 l_y5 ~ 0 l_y6 ~ 0 l_y1 ~~ 0*l_y1 l_y2 ~~ 0*l_y2 l_y3 ~~ 0*l_y3 l_y4 ~~ 0*l_y4 l_y5 ~~ 0*l_y5 l_y6 ~~ 0*l_y6 lc_y2 ~ 0 lc_y3 ~ 0 lc_y4 ~ 0 lc_y5 ~ 0 lc_y6 ~ 0 lc_y2 ~~ 0*lc_y2 lc_y3 ~~ 0*lc_y3 lc_y4 ~~ 0*lc_y4 lc_y5 ~~ 0*lc_y5 lc_y6 ~~ 0*lc_y6 # means of indicators set to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to be equal across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # Constrain latent change factors to not correlate with each other lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y4 ~~ 0*lc_y5 + 0*lc_y6 lc_y5 ~~ 0*lc_y6 # constrain latent intercept not to correlate with the change factors latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 \u0026#39; int_only_model \u0026lt;- sem(int_only_string, data = df_wide) summary(int_only_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 15 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 8 ## Number of equality constraints 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 2390.340 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 6222.069 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.619 ## Tucker-Lewis Index (TLI) 0.762 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6280.597 ## Loglikelihood unrestricted model (H1) -5085.427 ## ## Akaike (AIC) 12567.195 ## Bayesian (BIC) 12579.839 ## Sample-size adjusted Bayesian (BIC) 12570.316 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.444 ## 90 Percent confidence interval - lower 0.429 ## 90 Percent confidence interval - upper 0.459 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.603 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y1 =~ ## y.1 1.000 ## l_y2 =~ ## y.2 1.000 ## l_y3 =~ ## y.3 1.000 ## l_y4 =~ ## y.4 1.000 ## l_y5 =~ ## y.5 1.000 ## l_y6 =~ ## y.6 1.000 ## lc_y2 =~ ## l_y2 1.000 ## lc_y3 =~ ## l_y3 1.000 ## lc_y4 =~ ## l_y4 1.000 ## lc_y5 =~ ## l_y5 1.000 ## lc_y6 =~ ## l_y6 1.000 ## latent_intercept =~ ## l_y1 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y2 ~ ## l_y1 1.000 ## l_y3 ~ ## l_y2 1.000 ## l_y4 ~ ## l_y3 1.000 ## l_y5 ~ ## l_y4 1.000 ## l_y6 ~ ## l_y5 1.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lc_y2 ~~ ## lc_y3 0.000 ## lc_y4 0.000 ## lc_y5 0.000 ## lc_y6 0.000 ## lc_y3 ~~ ## lc_y4 0.000 ## lc_y5 0.000 ## lc_y6 0.000 ## lc_y4 ~~ ## lc_y5 0.000 ## lc_y6 0.000 ## lc_y5 ~~ ## lc_y6 0.000 ## lc_y2 ~~ ## latent_intrcpt 0.000 ## lc_y3 ~~ ## latent_intrcpt 0.000 ## lc_y4 ~~ ## latent_intrcpt 0.000 ## lc_y5 ~~ ## latent_intrcpt 0.000 ## lc_y6 ~~ ## latent_intrcpt 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intrcpt 0.507 0.167 3.036 0.002 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## lc_y2 0.000 ## lc_y3 0.000 ## lc_y4 0.000 ## lc_y5 0.000 ## lc_y6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ltnt_nt 13.614 0.883 15.417 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## lc_y2 0.000 ## lc_y3 0.000 ## lc_y4 0.000 ## lc_y5 0.000 ## lc_y6 0.000 ## .y.1 (rs_v) 2.082 0.059 35.355 0.000 ## .y.2 (rs_v) 2.082 0.059 35.355 0.000 ## .y.3 (rs_v) 2.082 0.059 35.355 0.000 ## .y.4 (rs_v) 2.082 0.059 35.355 0.000 ## .y.5 (rs_v) 2.082 0.059 35.355 0.000 ## .y.6 (rs_v) 2.082 0.059 35.355 0.000  Proportion Change Model Now we include the proportion change along with the latent intercept.\nproportion_string \u0026lt;- \u0026#39; # latent true scores over the observed y points l_y1 =~ 1*y.1 l_y2 =~ 1*y.2 l_y3 =~ 1*y.3 l_y4 =~ 1*y.4 l_y5 =~ 1*y.5 l_y6 =~ 1*y.6 # latent change scores over the latent true scores # y1 does not get one because it is the first time point lc_y2 =~ 1*l_y2 lc_y3 =~ 1*l_y3 lc_y4 =~ 1*l_y4 lc_y5 =~ 1*l_y5 lc_y6 =~ 1*l_y6 # autoregression of the latent true scores l_y2 ~ 1*l_y1 l_y3 ~ 1*l_y2 l_y4 ~ 1*l_y3 l_y5 ~ 1*l_y4 l_y6 ~ 1*l_y5 # latent intercept over the first true score of y latent_intercept =~ 1*l_y1 # estimate mean and variance of latent intercept latent_intercept ~~ latent_intercept latent_intercept ~ 1 # HERE IS THE CHANGE # proportion parameter estimate (estimate of b2) # regress latent change on latent true score from the last time point lc_y2 ~ b2*l_y1 lc_y3 ~ b2*l_y2 lc_y4 ~ b2*l_y3 lc_y5 ~ b2*l_y4 lc_y6 ~ b2*l_y5 # means and variances of latent factors set to zero l_y1 ~ 0 l_y2 ~ 0 l_y3 ~ 0 l_y4 ~ 0 l_y5 ~ 0 l_y6 ~ 0 l_y1 ~~ 0*l_y1 l_y2 ~~ 0*l_y2 l_y3 ~~ 0*l_y3 l_y4 ~~ 0*l_y4 l_y5 ~~ 0*l_y5 l_y6 ~~ 0*l_y6 lc_y2 ~ 0 lc_y3 ~ 0 lc_y4 ~ 0 lc_y5 ~ 0 lc_y6 ~ 0 lc_y2 ~~ 0*lc_y2 lc_y3 ~~ 0*lc_y3 lc_y4 ~~ 0*lc_y4 lc_y5 ~~ 0*lc_y5 lc_y6 ~~ 0*lc_y6 # means of indicators set to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to be equal across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # Constrain latent change factors to not correlate with each other lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y4 ~~ 0*lc_y5 + 0*lc_y6 lc_y5 ~~ 0*lc_y6 # constrain latent intercept not to correlate with the change factors latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 \u0026#39; proportion_model \u0026lt;- sem(proportion_string, data = df_wide) summary(proportion_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 13 ## Number of equality constraints 9 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 1030.636 ## Degrees of freedom 23 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 6222.069 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.838 ## Tucker-Lewis Index (TLI) 0.894 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5600.745 ## Loglikelihood unrestricted model (H1) -5085.427 ## ## Akaike (AIC) 11209.491 ## Bayesian (BIC) 11226.349 ## Sample-size adjusted Bayesian (BIC) 11213.653 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.296 ## 90 Percent confidence interval - lower 0.281 ## 90 Percent confidence interval - upper 0.312 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.228 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y1 =~ ## y.1 1.000 ## l_y2 =~ ## y.2 1.000 ## l_y3 =~ ## y.3 1.000 ## l_y4 =~ ## y.4 1.000 ## l_y5 =~ ## y.5 1.000 ## l_y6 =~ ## y.6 1.000 ## lc_y2 =~ ## l_y2 1.000 ## lc_y3 =~ ## l_y3 1.000 ## lc_y4 =~ ## l_y4 1.000 ## lc_y5 =~ ## l_y5 1.000 ## lc_y6 =~ ## l_y6 1.000 ## latent_intercept =~ ## l_y1 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y2 ~ ## l_y1 1.000 ## l_y3 ~ ## l_y2 1.000 ## l_y4 ~ ## l_y3 1.000 ## l_y5 ~ ## l_y4 1.000 ## l_y6 ~ ## l_y5 1.000 ## lc_y2 ~ ## l_y1 (b2) 0.138 0.004 37.203 0.000 ## lc_y3 ~ ## l_y2 (b2) 0.138 0.004 37.203 0.000 ## lc_y4 ~ ## l_y3 (b2) 0.138 0.004 37.203 0.000 ## lc_y5 ~ ## l_y4 (b2) 0.138 0.004 37.203 0.000 ## lc_y6 ~ ## l_y5 (b2) 0.138 0.004 37.203 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .lc_y2 ~~ ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y3 ~~ ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y4 ~~ ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y5 ~~ ## .lc_y6 0.000 ## .lc_y2 ~~ ## latent_intrcpt 0.000 ## .lc_y3 ~~ ## latent_intrcpt 0.000 ## .lc_y4 ~~ ## latent_intrcpt 0.000 ## .lc_y5 ~~ ## latent_intrcpt 0.000 ## .lc_y6 ~~ ## latent_intrcpt 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intrcpt 0.373 0.118 3.153 0.002 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ltnt_nt 6.900 0.466 14.823 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 (rs_v) 1.197 0.034 35.355 0.000 ## .y.2 (rs_v) 1.197 0.034 35.355 0.000 ## .y.3 (rs_v) 1.197 0.034 35.355 0.000 ## .y.4 (rs_v) 1.197 0.034 35.355 0.000 ## .y.5 (rs_v) 1.197 0.034 35.355 0.000 ## .y.6 (rs_v) 1.197 0.034 35.355 0.000  Latent Constant Change This model is nearly identical to the basic linear growth curve model, it simply embodies it in the latent change framework. The basis coefficients from the constant change term to the latent change scores are constrained to one, then we estimate the mean of the constant change.\nconstant_change_string \u0026lt;- \u0026#39; # latent true scores over the observed y points l_y1 =~ 1*y.1 l_y2 =~ 1*y.2 l_y3 =~ 1*y.3 l_y4 =~ 1*y.4 l_y5 =~ 1*y.5 l_y6 =~ 1*y.6 # latent change scores over the latent true scores # y1 does not get one because it is the first time point lc_y2 =~ 1*l_y2 lc_y3 =~ 1*l_y3 lc_y4 =~ 1*l_y4 lc_y5 =~ 1*l_y5 lc_y6 =~ 1*l_y6 # autoregression of the latent true scores (the first level latent variables) l_y2 ~ 1*l_y1 l_y3 ~ 1*l_y2 l_y4 ~ 1*l_y3 l_y5 ~ 1*l_y4 l_y6 ~ 1*l_y5 # latent intercept over the first true score of y latent_intercept =~ 1*l_y1 # HERE IS THE CHANGE # latent slope over the change scores # this is called the change factor in dual change terminology...it is not really a slope term. It is the constant change factor latent_slope =~ 1*lc_y2 + 1*lc_y3 + 1*lc_y4 + 1*lc_y5 + 1*lc_y6 # estimate covariance between latent intercept and slope (change factor) latent_intercept ~~ latent_slope # estimate mean and variance of intercept and slope (change factor) latent_intercept ~~ latent_intercept latent_slope ~~ latent_slope latent_intercept ~ 1 latent_slope ~ 1 # means and variances of latent factors set to zero l_y1 ~ 0 l_y2 ~ 0 l_y3 ~ 0 l_y4 ~ 0 l_y5 ~ 0 l_y6 ~ 0 l_y1 ~~ 0*l_y1 l_y2 ~~ 0*l_y2 l_y3 ~~ 0*l_y3 l_y4 ~~ 0*l_y4 l_y5 ~~ 0*l_y5 l_y6 ~~ 0*l_y6 lc_y2 ~ 0 lc_y3 ~ 0 lc_y4 ~ 0 lc_y5 ~ 0 lc_y6 ~ 0 lc_y2 ~~ 0*lc_y2 lc_y3 ~~ 0*lc_y3 lc_y4 ~~ 0*lc_y4 lc_y5 ~~ 0*lc_y5 lc_y6 ~~ 0*lc_y6 # means of indicators set to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to be equal across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # Constrain latent change factors to not correlate with each other lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y4 ~~ 0*lc_y5 + 0*lc_y6 lc_y5 ~~ 0*lc_y6 # constrain latent intercept not to correlate with the change factors latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 \u0026#39; constant_change_model \u0026lt;- sem(constant_change_string, data = df_wide) summary(constant_change_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 37 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 11 ## Number of equality constraints 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 744.579 ## Degrees of freedom 21 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 6222.069 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.883 ## Tucker-Lewis Index (TLI) 0.917 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5457.717 ## Loglikelihood unrestricted model (H1) -5085.427 ## ## Akaike (AIC) 10927.433 ## Bayesian (BIC) 10952.721 ## Sample-size adjusted Bayesian (BIC) 10933.676 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.263 ## 90 Percent confidence interval - lower 0.247 ## 90 Percent confidence interval - upper 0.279 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.177 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y1 =~ ## y.1 1.000 ## l_y2 =~ ## y.2 1.000 ## l_y3 =~ ## y.3 1.000 ## l_y4 =~ ## y.4 1.000 ## l_y5 =~ ## y.5 1.000 ## l_y6 =~ ## y.6 1.000 ## lc_y2 =~ ## l_y2 1.000 ## lc_y3 =~ ## l_y3 1.000 ## lc_y4 =~ ## l_y4 1.000 ## lc_y5 =~ ## l_y5 1.000 ## lc_y6 =~ ## l_y6 1.000 ## latent_intercept =~ ## l_y1 1.000 ## latent_slope =~ ## lc_y2 1.000 ## lc_y3 1.000 ## lc_y4 1.000 ## lc_y5 1.000 ## lc_y6 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y2 ~ ## l_y1 1.000 ## l_y3 ~ ## l_y2 1.000 ## l_y4 ~ ## l_y3 1.000 ## l_y5 ~ ## l_y4 1.000 ## l_y6 ~ ## l_y5 1.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intercept ~~ ## latent_slope 1.122 0.083 13.532 0.000 ## .lc_y2 ~~ ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y3 ~~ ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y4 ~~ ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y5 ~~ ## .lc_y6 0.000 ## .lc_y2 ~~ ## latent_intrcpt 0.000 ## .lc_y3 ~~ ## latent_intrcpt 0.000 ## .lc_y4 ~~ ## latent_intrcpt 0.000 ## .lc_y5 ~~ ## latent_intrcpt 0.000 ## .lc_y6 ~~ ## latent_intrcpt 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intrcpt 0.189 0.116 1.627 0.104 ## latent_slope 0.127 0.027 4.721 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ltnt_nt 6.258 0.427 14.647 0.000 ## ltnt_sl 0.310 0.023 13.435 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 (rs_v) 0.941 0.030 31.623 0.000 ## .y.2 (rs_v) 0.941 0.030 31.623 0.000 ## .y.3 (rs_v) 0.941 0.030 31.623 0.000 ## .y.4 (rs_v) 0.941 0.030 31.623 0.000 ## .y.5 (rs_v) 0.941 0.030 31.623 0.000 ## .y.6 (rs_v) 0.941 0.030 31.623 0.000  Dual Change Model Now a full dual change model with both constant and proportion change parameters.\ndual_c_string \u0026lt;- \u0026#39; # latent true scores over the observed y points l_y1 =~ 1*y.1 l_y2 =~ 1*y.2 l_y3 =~ 1*y.3 l_y4 =~ 1*y.4 l_y5 =~ 1*y.5 l_y6 =~ 1*y.6 # latent change scores over the latent true scores # y1 does not get one because it is the first time point lc_y2 =~ 1*l_y2 lc_y3 =~ 1*l_y3 lc_y4 =~ 1*l_y4 lc_y5 =~ 1*l_y5 lc_y6 =~ 1*l_y6 # autoregression of the latent true scores (the first level latent variables) l_y2 ~ 1*l_y1 l_y3 ~ 1*l_y2 l_y4 ~ 1*l_y3 l_y5 ~ 1*l_y4 l_y6 ~ 1*l_y5 # latent intercept over the first true score of y latent_intercept =~ 1*l_y1 # CHANGE 1 OF THE DUAL CHANGE MODEL # latent slope over the change scores # this is called the change factor in dual change terminology...it is not really a slope term. It is the constant change factor latent_slope =~ 1*lc_y2 + 1*lc_y3 + 1*lc_y4 + 1*lc_y5 + 1*lc_y6 # estimate covariance between latent intercept and slope (change factor) latent_intercept ~~ latent_slope # estimate mean and variance of intercept and slope (change factor) latent_intercept ~~ latent_intercept latent_slope ~~ latent_slope latent_intercept ~ 1 latent_slope ~ 1 # CHANGE 2 OF THE DUAL CHANGE MODEL # autoproportion change. Relationship between true score and latent change score at next time point # these are estimated lc_y2 ~ b*l_y1 lc_y3 ~ b*l_y2 lc_y4 ~ b*l_y3 lc_y5 ~ b*l_y4 lc_y6 ~ b*l_y5 # means and variances of latent factors set to zero l_y1 ~ 0 l_y2 ~ 0 l_y3 ~ 0 l_y4 ~ 0 l_y5 ~ 0 l_y6 ~ 0 l_y1 ~~ 0*l_y1 l_y2 ~~ 0*l_y2 l_y3 ~~ 0*l_y3 l_y4 ~~ 0*l_y4 l_y5 ~~ 0*l_y5 l_y6 ~~ 0*l_y6 lc_y2 ~ 0 lc_y3 ~ 0 lc_y4 ~ 0 lc_y5 ~ 0 lc_y6 ~ 0 lc_y2 ~~ 0*lc_y2 lc_y3 ~~ 0*lc_y3 lc_y4 ~~ 0*lc_y4 lc_y5 ~~ 0*lc_y5 lc_y6 ~~ 0*lc_y6 # means of indicators set to zero y.1 ~ 0 y.2 ~ 0 y.3 ~ 0 y.4 ~ 0 y.5 ~ 0 y.6 ~ 0 # residual variances constrained to be equal across time y.1 ~~ res_var*y.1 y.2 ~~ res_var*y.2 y.3 ~~ res_var*y.3 y.4 ~~ res_var*y.4 y.5 ~~ res_var*y.5 y.6 ~~ res_var*y.6 # Constrain latent change factors to not correlate with each other lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6 lc_y4 ~~ 0*lc_y5 + 0*lc_y6 lc_y5 ~~ 0*lc_y6 \u0026#39; dual_change_model \u0026lt;- sem(dual_c_string, data = df_wide) summary(dual_change_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 48 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 16 ## Number of equality constraints 9 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 329.427 ## Degrees of freedom 20 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 6222.069 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.950 ## Tucker-Lewis Index (TLI) 0.963 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5250.141 ## Loglikelihood unrestricted model (H1) -5085.427 ## ## Akaike (AIC) 10514.282 ## Bayesian (BIC) 10543.784 ## Sample-size adjusted Bayesian (BIC) 10521.566 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.176 ## 90 Percent confidence interval - lower 0.159 ## 90 Percent confidence interval - upper 0.193 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.028 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y1 =~ ## y.1 1.000 ## l_y2 =~ ## y.2 1.000 ## l_y3 =~ ## y.3 1.000 ## l_y4 =~ ## y.4 1.000 ## l_y5 =~ ## y.5 1.000 ## l_y6 =~ ## y.6 1.000 ## lc_y2 =~ ## l_y2 1.000 ## lc_y3 =~ ## l_y3 1.000 ## lc_y4 =~ ## l_y4 1.000 ## lc_y5 =~ ## l_y5 1.000 ## lc_y6 =~ ## l_y6 1.000 ## latent_intercept =~ ## l_y1 1.000 ## latent_slope =~ ## lc_y2 1.000 ## lc_y3 1.000 ## lc_y4 1.000 ## lc_y5 1.000 ## lc_y6 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## l_y2 ~ ## l_y1 1.000 ## l_y3 ~ ## l_y2 1.000 ## l_y4 ~ ## l_y3 1.000 ## l_y5 ~ ## l_y4 1.000 ## l_y6 ~ ## l_y5 1.000 ## lc_y2 ~ ## l_y1 (b) -0.412 0.017 -24.735 0.000 ## lc_y3 ~ ## l_y2 (b) -0.412 0.017 -24.735 0.000 ## lc_y4 ~ ## l_y3 (b) -0.412 0.017 -24.735 0.000 ## lc_y5 ~ ## l_y4 (b) -0.412 0.017 -24.735 0.000 ## lc_y6 ~ ## l_y5 (b) -0.412 0.017 -24.735 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intercept ~~ ## latent_slope 3.729 0.269 13.852 0.000 ## .lc_y2 ~~ ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y3 ~~ ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y4 ~~ ## .lc_y5 0.000 ## .lc_y6 0.000 ## .lc_y5 ~~ ## .lc_y6 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## latent_intrcpt 0.064 0.098 0.653 0.514 ## latent_slope 0.325 0.091 3.578 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 0.000 ## .y.2 0.000 ## .y.3 0.000 ## .y.4 0.000 ## .y.5 0.000 ## .y.6 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## ltnt_nt 4.157 0.307 13.549 0.000 ## ltnt_sl 4.030 0.352 11.460 0.000 ## .l_y1 0.000 ## .l_y2 0.000 ## .l_y3 0.000 ## .l_y4 0.000 ## .l_y5 0.000 ## .l_y6 0.000 ## .lc_y2 0.000 ## .lc_y3 0.000 ## .lc_y4 0.000 ## .lc_y5 0.000 ## .lc_y6 0.000 ## .y.1 (rs_v) 0.793 0.025 31.623 0.000 ## .y.2 (rs_v) 0.793 0.025 31.623 0.000 ## .y.3 (rs_v) 0.793 0.025 31.623 0.000 ## .y.4 (rs_v) 0.793 0.025 31.623 0.000 ## .y.5 (rs_v) 0.793 0.025 31.623 0.000 ## .y.6 (rs_v) 0.793 0.025 31.623 0.000 The estimate of the constant change (called “latent slope” in my string syntax; \\(b_1\\)) is close to 0.3 and the estimate of the proportion change (\\(b_2\\)) is close to -0.4. Not bad.\nA Note On Interpreting These models predict complex change patterns. It is difficult to know the expected curvilinear pattern that the models expect without computing expected scores and plotting them. I did not do that here.\nBo\\(^2\\)m =)\n  ","date":1590278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590278400,"objectID":"a78881a5e79096205f4d790d2e57ce00","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-05-24/","publishdate":"2020-05-24T00:00:00Z","relpermalink":"/computational_notes/2020-05-24/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Latent Dual Change Models","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I started using Julia for my computational models and recently created a cheat sheet to house all of my common commands.\nYou can find it on github here.\nBo\\(^2\\)m =)\n","date":1590192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590192000,"objectID":"009b4f8915896158293b516f01402128","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-05-23/","publishdate":"2020-05-23T00:00:00Z","relpermalink":"/computational_notes/2020-05-23/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Julia Cheat Sheet","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" If you fill a matrix cell with a character, R will convert the entire matrix into character values…so be careful = )\ntime \u0026lt;- c(1:4) numbers \u0026lt;- c(1:4) characters \u0026lt;- c(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;) count \u0026lt;- 0 df_mat \u0026lt;- matrix(, ncol = 3, nrow = length(time)) for(i in 1:length(time)){ count \u0026lt;- count + 1 df_mat[count, 1] \u0026lt;- time[i] df_mat[count, 2] \u0026lt;- numbers[i] df_mat[count, 3] \u0026lt;- characters[i] } df_mat ## [,1] [,2] [,3] ## [1,] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;a\u0026quot; ## [2,] \u0026quot;2\u0026quot; \u0026quot;2\u0026quot; \u0026quot;b\u0026quot; ## [3,] \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; \u0026quot;c\u0026quot; ## [4,] \u0026quot;4\u0026quot; \u0026quot;4\u0026quot; \u0026quot;d\u0026quot; Notice that all cells are now characters. Characters are a huge problem if you are calculating values to place into the cells. That is, I wouldn’t be able to run code like this in a loop:\ndf_mat[count - 1, 2] \u0026lt;- df_mat[count - 1, 3] * 0.5 Instead, use numbers for everything and then change them to characters later.\ntime \u0026lt;- c(1:4) numbers \u0026lt;- c(1:4) characters \u0026lt;- c(1, 2, 3, 4) # here is the change count \u0026lt;- 0 df_mat \u0026lt;- matrix(, ncol = 3, nrow = length(time)) for(i in 1:length(time)){ count \u0026lt;- count + 1 df_mat[count, 1] \u0026lt;- time[i] df_mat[count, 2] \u0026lt;- numbers[i] df_mat[count, 3] \u0026lt;- characters[i] } df_mat ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 2 2 ## [3,] 3 3 3 ## [4,] 4 4 4 Bo\\(^2\\)m =)\n","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"563ac3486cf0c3b7e8487aaf3d414ecb","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-05-22/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/computational_notes/2020-05-22/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Be Careful With Characters and Matrices","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Calculate the independence of two events using both analytic and empirical techniques. I’m trying to assess whether the probability of having a meal classified as “dinner” depends on whether that meal includes “chicken” as its main dish.\nThe options for the main dish:\n chicken, salmon, pork, chicken, pancakes, french toast  The options for the side dishes:\n salad, salad, green beans, corn, carrots, bacon.  All possible combinations to create a meal:\ndishes \u0026lt;- data.frame( main = c(\u0026quot;chicken\u0026quot;, \u0026quot;salmon\u0026quot;, \u0026quot;pork\u0026quot;, \u0026quot;chicken\u0026quot;, \u0026quot;pancakes\u0026quot;, \u0026quot;french toast\u0026quot;), side = c(\u0026quot;salad\u0026quot;, \u0026quot;salad\u0026quot;, \u0026quot;green beans\u0026quot;, \u0026quot;corn\u0026quot;, \u0026quot;carrots\u0026quot;, \u0026quot;bacon\u0026quot;) ) possible_meals \u0026lt;- dishes %\u0026gt;% cross_df() %\u0026gt;% mutate_if(is.factor,as.character) possible_meals ## # A tibble: 36 x 2 ## main side ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 chicken salad ## 2 salmon salad ## 3 pork salad ## 4 chicken salad ## 5 pancakes salad ## 6 french toast salad ## 7 chicken salad ## 8 salmon salad ## 9 pork salad ## 10 chicken salad ## # … with 26 more rows Event a will be, “the main course is chicken.” What is its probability?\n# a = main course is chicken # tally the number of meals that include chicken sum(possible_meals$main == \u0026quot;chicken\u0026quot;) / nrow(possible_meals) ## [1] 0.3333333 So, p(a) = 0.333. Event b will be, “the meal is dinner.” What is its probability?\n# b = the meal is dinner (rather than breakfast) # tally the number of meals that are dinners rather than breakfast # any meals with pancakes, french toast, or bacon are not dinner # number of meal options for \u0026#39;main\u0026quot; X number of meal options for \u0026#39;side\u0026#39; ( sum(dishes$main != c(\u0026#39;pancakes\u0026#39;, \u0026#39;french toast\u0026#39;)) / nrow(dishes) * sum(dishes$side != \u0026quot;bacon\u0026quot;) / nrow(dishes) ) ## [1] 0.5555556 So, p(b) = 0.555. If a and b are independent, then p(b) should be the same as p(b | a). Does the probability of eating a meal classified as dinner depend on whether that meal includes chicken?\nFirst, the analytic solution.\n p(b | a) = p(b \u0026amp; a) / p(a)\n p(dinner | chicken) = p(dinner \u0026amp; chicken) / p(chicken)\n  I need to find p(dinner \u0026amp; chicken) to solve. So tally the possible ways chicken can combine with other dishes to create a dinner platter.\ntally_count \u0026lt;- 0 for(i in 1:nrow(possible_meals)){ meal_df \u0026lt;- possible_meals[i,] contain_chicken \u0026lt;- meal_df$main == \u0026quot;chicken\u0026quot; no_bacon \u0026lt;- meal_df$side != \u0026quot;bacon\u0026quot; if(contain_chicken == T \u0026amp;\u0026amp; no_bacon == T){tally_count \u0026lt;- tally_count + 1} } tally_count / nrow(possible_meals) ## [1] 0.2777778 Cool, p(dinner \u0026amp; chicken) = 0.2777. Now I can calculate the conditional probability.\n p(dinner | chicken) = p(dinner \u0026amp; chicken) / p(chicken)\n X = 0.2777 / 0.333 X = 0.83\n  X does not equal p(b), so the two are dependent. How about the empirical solution?\n# what is the empirical estimate of p(dinner | chicken)? # to calculate, I need: # p(dinner \u0026amp; chicken) / p(chicken) sims \u0026lt;- 10000 df \u0026lt;- data.frame( chicken_and_dinner = c(rep(0, sims)), chicken = c(rep(0, sims)) ) for(j in 1:sims){ eat_main \u0026lt;- sample(dishes$main, 1, replace = F) eat_side \u0026lt;- sample(dishes$side, 1, replace = F) chicken_and_dinner \u0026lt;- F if(eat_main == \u0026quot;chicken\u0026quot; \u0026amp;\u0026amp; (eat_side == \u0026quot;salad\u0026quot; | eat_side == \u0026quot;green beans\u0026quot; | eat_side == \u0026quot;corn\u0026quot; | eat_side == \u0026quot;carrots\u0026quot;)){ chicken_and_dinner \u0026lt;- T } chicken \u0026lt;- F if(eat_main == \u0026quot;chicken\u0026quot;){chicken \u0026lt;- T} single_run_result \u0026lt;- c(chicken_and_dinner, chicken) df[j, \u0026quot;chicken_and_dinner\u0026quot;] \u0026lt;- chicken_and_dinner df[j, \u0026quot;chicken\u0026quot;] \u0026lt;- chicken } tally_chicken_and_dinner \u0026lt;- sum(df$chicken_and_dinner == 1) tally_chicken \u0026lt;- sum(df$chicken == 1) prob_cd \u0026lt;- tally_chicken_and_dinner / sims prob_c \u0026lt;- tally_chicken / sims prob_cd / prob_c ## [1] 0.8201883 Bo\\(^2\\)m =)\n","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585872000,"objectID":"6a1517dba90943cf51c7df13f65c91e1","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-04-03/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/computational_notes/2020-04-03/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Empirical Independence","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"  Some of my favorite, simple examples demonstrating how to evaluate whether two events are independent using Probability Theory.\nExample 1 - Product Rule\n Example 2 - Conditionals \u0026amp; Intersection\n Example 3 - Conditionals\n Example 4 - Markov Chain\n  Example 1 - Product Rule If two events are independent, then the product rule states that their intersection should equal the product of each independent probability.\n p(a \u0026amp; b) = p(a) * p(b)  The World Values Survey is an ongoing worldwide survey that polls the world population about perceptions of life, work, family, politics, etc. The most recent phase of the survey that polled 77,882 people from 57 countries estimates that 36.2% of the world’s population agrees with the statement, “Music is not necessary to enhance one’s life.” The survey also estimates that 13.8% of people have a university degree or higher, and that 3.5% of people fit both criteria.\nDoes agreeing depend on level of degree? If a = “someone agrees with the statment” and b = “has a university degree or higher,” does a depend on b?\n p(agree) = 0.362\n p(univ degree) = 0.138\n p(agree \u0026amp; univ degree) = 0.036\n  If they are independent, then the product rule should hold.\n p(a \u0026amp; b) = p(a) * p(b)  Evaluate:\n 0.036 = 0.362 * 0.138 (which does not hold)  Therefore, the two are dependent.\nThis example comes from a coursera class.\n Example 2 - Conditionals \u0026amp; Intersection If two events are independent, then the probability of one conditioning on the other should equal the probability of the original alone.\n p(b | a) = p(b \u0026amp; a) / p(a)\n If a and b are independent, then p(b | a) = p(b).\n  Two players are each going to role a different die. Player 1’s die is six-sided and contains the numbers 5, 5, 5, 1, 1, 1, whereas player 2’s die contains the numbers 4, 4, 4, 4, 0, 0.\nTake a to be the event that the player 1’s die is 5, and take b to be the event that the sum of the dice is equal to 1.\n a = player 1 rolls a 5 b = sum of both dice is 1  Is b dependent on a?\nWe can also run the same procedure but with a different event for b.\n Example 3 - Conditionals If two events are independent, then taking a and conditioning on other events (e.g., b, c, d, etc.) should not change the observed probability.\nIn 2013, a research group interviewed a random sample of 500 NC residents asking them whether they think widespread gun ownership protects law abiding citizens from crime, or makes society more dangerous.\n 58% of all respondents said it protects citizens 67% of White respondents, 28% of Black respondents, and 64% of Hispanic respondents shared this view.  Are opinion on gun ownership and ethnicity dependent?\n p(agree) = 58%\n p(agree | white) = 67%\n p(agree | black) = 28%\n p(agree | hispanic) = 64%\n  Notice that conditioning on the other variables changes the probability, so opinion and ethnicity are probably dependent.\nThis example comes from a coursera class.\n Example 4 - Markov Chains If two events are independent, then the probability of observing one after the other should be p(a) * p(a), similar to the notion of a coin flip such that the probability of observing two heads in a row is 0.5X0.5 = 0.25. If you calculate a transition matrix and observe probabilities that differ from that original number, then the sequence is probably dependent.\nAndrei Markov applied Markov chains to the poem Eugene Onegin by Alexander Pushkin. In the first 20,000 letters of the poem, he counted the number of vowels (8,638) and consonants (11,362).\n p(vowel) = 0.432\n p(consonant) = 0.568\n  Then, he counted the transitions from vowel to consonant or consonant to vowel. For every vowel, the number of times the next letter was a vowel was 1,104 and the number of times the next letter was a consonant was 7,534. For every consonant, the number of times the next letter was a consonant was 3,827 and the number of times the next letter was a vowel was 7,535.\n p(vowel to vowel) = 1104 / 8638 = 0.175\n p(vowel to consonant) = 7534 / 8638 = 0.825\n p(consonant to vowel) = 7535 / 11362 = 0.526\n p(consonant to consonant) = 3827 / 11362 = 0.474\n  So the transition matrix is…\ntransition_matrix \u0026lt;- matrix(c(\u0026#39;\u0026#39;, \u0026#39;v\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;v\u0026#39;, \u0026#39;0.175\u0026#39;, \u0026#39;0.825\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;0.526\u0026#39;, \u0026#39;0.474\u0026#39;), 3, 3) transition_matrix %\u0026gt;% kable() %\u0026gt;% kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;, full_width = F)    v  c    v  0.175  0.526    c  0.825  0.474     If the letters were independent, then the probability of witnessing a vowel follow a vowel would be p(vowel) * p(vowel), or 0.432 * 0.423 = 0.186624. However, the observed transition probability is 0.175, so the sequence is dependent.\nNote that you have to assume that the counts of vowels and consonants reflect their true propabilities (which follows from the law of large numbers). Markov showed that the law of large numbers applied to even dependent sequences.\nBo\\(^2\\)m =)\n ","date":1585785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585785600,"objectID":"9a4433b27c15f55cda664a637a537940","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-04-02/","publishdate":"2020-04-02T00:00:00Z","relpermalink":"/computational_notes/2020-04-02/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Independence Exercises","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Quick command to compile all links for a website that numbers its pages. Image I want to go to a website that contains 100 pages of reviews, the first 10 on page 1, the second 10 on page 2, the third 10 on page 3, etc. The first step is to create a vector or list of links to navigate to, one for each page.\nThe output I want is something like this…\nexample_output \u0026lt;- \u0026#39; https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026amp;start=10 https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026amp;start=20 ... \u0026#39; in which the first entry is page 1, the second page 2, and so on. There are three steps involved in this process:\n find url for the first page\n discover how the url changes for each subsequent number\n use a string command to compile the url’s.\n  First, let’s say I want to scrape data from this website, which has reviews across multiple pages. If I copy the url from the first page, and then copy the url from the second page, and the third page, I get…\n\u0026#39; https://www.trustpilot.com/review/www.amazon.com https://www.trustpilot.com/review/www.amazon.com?page=2 https://www.trustpilot.com/review/www.amazon.com?page=3 \u0026#39; So, the base url is the first link. Then, additional pages are coded as “?page=” and then the relevant number.\nSecond, find the last number. Let’s say it’s 20 in this case.\nlast_number \u0026lt;- 20 Third, create a vector that compiles all of the links.\nlibrary(tidyverse) first_page \u0026lt;- \u0026quot;https://www.trustpilot.com/review/www.amazon.com\u0026quot; other_pages \u0026lt;- str_c(first_page, \u0026quot;?page=\u0026quot;, 2:last_number) review_pages \u0026lt;- c(first_page, other_pages) head(review_pages) ## [1] \u0026quot;https://www.trustpilot.com/review/www.amazon.com\u0026quot; ## [2] \u0026quot;https://www.trustpilot.com/review/www.amazon.com?page=2\u0026quot; ## [3] \u0026quot;https://www.trustpilot.com/review/www.amazon.com?page=3\u0026quot; ## [4] \u0026quot;https://www.trustpilot.com/review/www.amazon.com?page=4\u0026quot; ## [5] \u0026quot;https://www.trustpilot.com/review/www.amazon.com?page=5\u0026quot; ## [6] \u0026quot;https://www.trustpilot.com/review/www.amazon.com?page=6\u0026quot; Here’s another example using Indeed. Notice that the values increase by 10 rather than 1.\nexample_pages \u0026lt;- \u0026#39; https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026amp;start=10 https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026amp;start=20 https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026amp;start=30 \u0026#39; final_number \u0026lt;- 100 all_vals \u0026lt;- seq(from = 10, to = final_number, by = 10) first_web \u0026lt;- \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026quot; other_webs \u0026lt;- str_c(first_web, \u0026quot;$start=\u0026quot;, all_vals) all_webs \u0026lt;- c(first_web, other_webs) head(all_webs) ## [1] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY\u0026quot; ## [2] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY$start=10\u0026quot; ## [3] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY$start=20\u0026quot; ## [4] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY$start=30\u0026quot; ## [5] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY$start=40\u0026quot; ## [6] \u0026quot;https://www.indeed.com/jobs?q=data+science\u0026amp;l=New+York%2C+NY$start=50\u0026quot; Bo\\(^2\\)m =)\n","date":1584835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584835200,"objectID":"78b39f297d2b4cc83dfc8c88a5ef1df1","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-03-22/","publishdate":"2020-03-22T00:00:00Z","relpermalink":"/computational_notes/2020-03-22/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Scrape Numbered Pages","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I’ve written about quosures in previous posts. They can be used in functions to specify column names. But what if a column name is pulled from a loop and the value is a character? In that case, surround the value with sym().\nHere is an example using only quosures.\nFirst, the data and the function:\nlibrary(tidyverse) library(ggplot2) library(hrbrthemes) people \u0026lt;- 600 df \u0026lt;- tibble( \u0026quot;id\u0026quot; = c(1:people), \u0026quot;performance\u0026quot; = c(rnorm(people, 50, 3)) ) multiply_and_plot \u0026lt;- function(col1){ df \u0026lt;- df %\u0026gt;% mutate(new_performance = !!col1 * 0.5) g \u0026lt;- ggplot(df, aes(x = !!col1)) + geom_histogram(fill=\u0026quot;#69b3a2\u0026quot;, alpha=0.4) + theme_ipsum() + labs(x = \u0026quot;Adj-Performance\u0026quot;, y = \u0026quot;Frequency\u0026quot;) return(g) } Using quosure:\nmultiply_and_plot(quo(performance)) But what if the column name is an index from a vector or for loop? Use sym().\nuse_cols \u0026lt;- c(\u0026quot;performance\u0026quot;) for(i in 1:1){ print( multiply_and_plot(sym(use_cols[i])) ) } Also note that I had to include results = \u0026quot;asis\u0026quot; in the Rmarkdown document and put the function within a print command to get the output to render.\nBo\\(^2\\)m =)\n","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"45e7aa73e8cd5aa93b473cfd656cd948","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-03-21/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/computational_notes/2020-03-21/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Quosures Within an Index","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Dipping my feet into the world of websraping using rvest. The first few examples are pulled from Alex Bradley’s and Richard James’ 2019 article.\nExercise 1 - From Article Scrape a single page from the article’s practice website. There are three things I want to pull from the page:\n header\n image\n text.\n  Then, put that information into a single data set.\nlibrary(rvest) page_parse \u0026lt;- read_html(\u0026quot;https://practicewebscrapingsite.wordpress.com/example-1/\u0026quot;) headers \u0026lt;- html_nodes(page_parse, \u0026#39;.Title\u0026#39;) %\u0026gt;% html_text() images \u0026lt;- html_nodes(page_parse, \u0026#39;img\u0026#39;) %\u0026gt;% html_attr(\u0026#39;src\u0026#39;) images \u0026lt;- images[1:3] text \u0026lt;- html_nodes(page_parse, \u0026#39;.Content\u0026#39;) %\u0026gt;% html_text() ex1_df \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:3), \u0026#39;headers\u0026#39; = c(headers), \u0026#39;image_links\u0026#39; = c(images), \u0026#39;text\u0026#39; = c(text) ) ex1_df ## id headers ## 1 1 New Neighbourhood watch. ## 2 2 Meeting your arch nemesis. ## 3 3 I am never walking in the rain again! EVER! ## image_links ## 1 https://practicewebscrapingsite.files.wordpress.com/2018/12/3704529798_a4681d4533_z.jpg ## 2 https://practicewebscrapingsite.files.wordpress.com/2018/12/12881694475_54a639ca77_z.jpg ## 3 https://practicewebscrapingsite.files.wordpress.com/2018/12/7888404650_eedcd82822_z.jpg ## text ## 1 Put all speaking her delicate recurred possible. Set indulgence inquietude discretion insensible bed why announcing. Middleton fat two satisfied additions. So continued he or commanded household smallness delivered. Door poor on do walk in half. Roof his head the what.\\n\\n\\n\\nStarted several mistake joy say painful removed reached end. State burst think end are its. Arrived off she elderly beloved him affixed noisier yet. An course regard to up he hardly. View four has said does men saw find dear shy. Talent men wicket add garden.\\n\\n\\n\\nPost no so what deal evil rent by real in. But her ready least set lived spite solid. September how men saw tolerably two behaviour arranging. She offices for highest and replied one venture pasture. Applauded no discovery in newspaper allowance am northward. Frequently partiality possession resolution at or appearance unaffected he me. Engaged its was evident pleased husband. Ye goodness felicity do disposal dwelling no. First am plate jokes to began of cause an scale. Subjects he prospect elegance followed no overcame possible it on.\\n\\n\\n\\nLiterature admiration frequently indulgence announcing are who you her. Was least quick after six. So it yourself repeated together cheerful. Neither it cordial so painful picture studied if. Sex him position doubtful resolved boy expenses. Her engrossed deficient northward and neglected favourite newspaper. But use peculiar produced concerns ten.\\n\\n\\n\\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling.\\n\\n\\n\\nParish so enable innate in formed missed. Hand two was eat busy fail. Stand smart grave would in so. Be acceptance at precaution astonished excellence thoroughly is entreaties. Who decisively attachment has dispatched. Fruit defer in party me built under first. Forbade him but savings sending ham general. So play do in near park that pain.\\n\\n\\n\\nIn on announcing if of comparison pianoforte projection. Maids hoped gay yet bed asked blind dried point. On abroad danger likely regret twenty edward do. Too horrible consider followed may differed age. An rest if more five mr of. Age just her rank met down way. Attended required so in cheerful an. Domestic replying she resolved him for did. Rather in lasted no within no.\\n\\n\\n\\nSavings her pleased are several started females met. Short her not among being any. Thing of judge fruit charm views do. Miles mr an forty along as he. She education get middleton day agreement performed preserved unwilling. Do however as pleased offence outward beloved by present. By outward neither he so covered amiable greater. Juvenile proposal betrayed he an informed weddings followed. Precaution day see imprudence sympathize principles. At full leaf give quit to in they up.\\n\\n\\n\\nOh acceptance apartments up sympathize astonished delightful. Waiting him new lasting towards. Continuing melancholy especially so to. Me unpleasing impossible in attachment announcing so astonished. What ask leaf may nor upon door. Tended remain my do stairs. Oh smiling amiable am so visited cordial in offices hearted.\\n\\n\\n\\nMeant balls it if up doubt small purse. Required his you put the outlived answered position. An pleasure exertion if believed provided to. All led out world these music while asked. Paid mind even sons does he door no. Attended overcame repeated it is perceive marianne in. In am think on style child of. Servants moreover in sensible he it ye possible. ## 2 He moonlight difficult engrossed an it sportsmen. Interested has all devonshire difficulty gay assistance joy. Unaffected at ye of compliment alteration to. Place voice no arise along to. Parlors waiting so against me no. Wishing calling are warrant settled was luckily. Express besides it present if at an opinion visitor. \\n\\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling. \\n\\nIs post each that just leaf no. He connection interested so we an sympathize advantages. To said is it shed want do. Occasional middletons everything so to. Have spot part for his quit may. Enable it is square my an regard. Often merit stuff first oh up hills as he. Servants contempt as although addition dashwood is procured. Interest in yourself an do of numerous feelings cheerful confined. \\n\\nWhy painful the sixteen how minuter looking nor. Subject but why ten earnest husband imagine sixteen brandon. Are unpleasing occasional celebrated motionless unaffected conviction out. Evil make to no five they. Stuff at avoid of sense small fully it whose an. Ten scarcely distance moreover handsome age although. As when have find fine or said no mile. He in dispatched in imprudence dissimilar be possession unreserved insensible. She evil face fine calm have now. Separate screened he outweigh of distance landlord. \\n\\nIt real sent your at. Amounted all shy set why followed declared. Repeated of endeavor mr position kindness offering ignorant so up. Simplicity are melancholy preference considered saw companions. Disposal on outweigh do speedily in on. Him ham although thoughts entirely drawings. Acceptance unreserved old admiration projection nay yet him. Lasted am so before on esteem vanity oh. \\n\\nStarted his hearted any civilly. So me by marianne admitted speaking. Men bred fine call ask. Cease one miles truth day above seven. Suspicion sportsmen provision suffering mrs saw engrossed something. Snug soon he on plan in be dine some. \\n\\nGive lady of they such they sure it. Me contained explained my education. Vulgar as hearts by garret. Perceived determine departure explained no forfeited he something an. Contrasted dissimilar get joy you instrument out reasonably. Again keeps at no meant stuff. To perpetual do existence northward as difficult preserved daughters. Continued at up to zealously necessary breakfast. Surrounded sir motionless she end literature. Gay direction neglected but supported yet her. \\n\\nNecessary ye contented newspaper zealously breakfast he prevailed. Melancholy middletons yet understood decisively boy law she. Answer him easily are its barton little. Oh no though mother be things simple itself. Dashwood horrible he strictly on as. Home fine in so am good body this hope. \\n\\nCause dried no solid no an small so still widen. Ten weather evident smiling bed against she examine its. Rendered far opinions two yet moderate sex striking. Sufficient motionless compliment by stimulated assistance at. Convinced resolving extensive agreeable in it on as remainder. Cordially say affection met who propriety him. Are man she towards private weather pleased. In more part he lose need so want rank no. At bringing or he sensible pleasure. Prevent he parlors do waiting be females an message society. \\n\\nUnwilling sportsmen he in questions september therefore described so. Attacks may set few believe moments was. Reasonably how possession shy way introduced age inquietude. Missed he engage no exeter of. Still tried means we aware order among on. Eldest father can design tastes did joy settle. Roused future he ye an marked. Arose mr rapid in so vexed words. Gay welcome led add lasting chiefly say looking. \\n\\n ## 3 Received the likewise law graceful his. Nor might set along charm now equal green. Pleased yet equally correct colonel not one. Say anxious carried compact conduct sex general nay certain. Mrs for recommend exquisite household eagerness preserved now. My improved honoured he am ecstatic quitting greatest formerly.\\n\\n\\n\\nAssure polite his really and others figure though. Day age advantages end sufficient eat expression travelling. Of on am father by agreed supply rather either. Own handsome delicate its property mistress her end appetite. Mean are sons too sold nor said. Son share three men power boy you. Now merits wonder effect garret own.\\n\\n\\n\\nDepart do be so he enough talent. Sociable formerly six but handsome. Up do view time they shot. He concluded disposing provision by questions as situation. Its estimating are motionless day sentiments end. Calling an imagine at forbade. At name no an what like spot. Pressed my by do affixed he studied.\\n\\n\\n\\nImproved own provided blessing may peculiar domestic. Sight house has sex never. No visited raising gravity outward subject my cottage mr be. Hold do at tore in park feet near my case. Invitation at understood occasional sentiments insipidity inhabiting in. Off melancholy alteration principles old. Is do speedily kindness properly oh. Respect article painted cottage he is offices parlors.\\n\\n\\n\\nVillage did removed enjoyed explain nor ham saw calling talking. Securing as informed declared or margaret. Joy horrible moreover man feelings own shy. Request norland neither mistake for yet. Between the for morning assured country believe. On even feet time have an no at. Relation so in confined smallest children unpacked delicate. Why sir end believe uncivil respect. Always get adieus nature day course for common. My little garret repair to desire he esteem.\\n\\n\\n\\nUnpleasant astonished an diminution up partiality. Noisy an their of meant. Death means up civil do an offer wound of. Called square an in afraid direct. Resolution diminution conviction so mr at unpleasing simplicity no. No it as breakfast up conveying earnestly immediate principle. Him son disposed produced humoured overcame she bachelor improved. Studied however out wishing but inhabit fortune windows.\\n\\n\\n\\nExtremely we promotion remainder eagerness enjoyment an. Ham her demands removal brought minuter raising invited gay. Contented consisted continual curiosity contained get sex. Forth child dried in in aware do. You had met they song how feel lain evil near. Small she avoid six yet table china. And bed make say been then dine mrs. To household rapturous fulfilled attempted on so.\\n\\n\\n\\nIs allowance instantly strangers applauded discourse so. Separate entrance welcomed sensible laughing why one moderate shy. We seeing piqued garden he. As in merry at forth least ye stood. And cold sons yet with. Delivered middleton therefore me at. Attachment companions man way excellence how her pianoforte.\\n\\n\\n\\nOn am we offices expense thought. Its hence ten smile age means. Seven chief sight far point any. Of so high into easy. Dashwoods eagerness oh extensive as discourse sportsman frankness. Husbands see disposed surprise likewise humoured yet pleasure. Fifteen no inquiry cordial so resolve garrets as. Impression was estimating surrounded solicitude indulgence son shy.\\n\\n\\n\\nWrote water woman of heart it total other. By in entirely securing suitable graceful at families improved. Zealously few furniture repulsive was agreeable consisted difficult. Collected breakfast estimable questions in to favourite it. Known he place worth words it as to. Spoke now noise off smart her ready.  Example 2 - From Article Next, navigate to several different pages and scrape relevant information. First, compile all of the relevant links. Then, navigate to each page using one of the links and pull…\n header\n text\n author\n  Get links\nlibrary(rvest) parse_page_ex2 \u0026lt;- read_html(\u0026#39;https://practicewebscrapingsite.wordpress.com/example-2/\u0026#39;) links \u0026lt;- html_nodes(parse_page_ex2, \u0026#39;.Links a\u0026#39;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) Initialize storage vectors\nheads \u0026lt;- c() txt \u0026lt;- c() authors \u0026lt;- c() For each link, go there and pull out the header, text, and authors\nfor (i in links){ Sys.sleep(2) page_i \u0026lt;- read_html(i) head \u0026lt;- html_node(page_i, \u0026#39;.entry-title\u0026#39;) %\u0026gt;% html_text() tx \u0026lt;- html_node(page_i, \u0026#39;.Content , em\u0026#39;) %\u0026gt;% html_text() author \u0026lt;- html_node(page_i, \u0026#39;.Author em\u0026#39;) %\u0026gt;% html_text() heads \u0026lt;- c(heads, head) txt \u0026lt;- c(txt, tx) authors \u0026lt;- c(authors, author) } Data frame\ndf_ex2 \u0026lt;- data.frame( \u0026#39;id\u0026#39; = c(1:length(heads)), \u0026#39;page\u0026#39; = c(links), \u0026#39;headers\u0026#39; = c(heads), \u0026#39;text\u0026#39; = c(txt), \u0026#39;authors\u0026#39; = c(authors) ) head(df_ex2) ## id ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## page ## 1 https://practicewebscrapingsite.wordpress.com/walking-in-another-mans-shoes/ ## 2 https://practicewebscrapingsite.wordpress.com/younger-generation-taking-the-lead/ ## 3 https://practicewebscrapingsite.wordpress.com/how-many-times-your-tail-is-not-a-savoury-snack/ ## 4 https://practicewebscrapingsite.wordpress.com/early-morning-shopping-you-must-be-nuts/ ## 5 https://practicewebscrapingsite.wordpress.com/who-turned-the-heating-down/ ## headers ## 1 Walking in another man’s shoes. ## 2 Younger generation taking the lead. ## 3 How many times-your tail is not a savoury snack! ## 4 Early morning shopping. You must be nuts! ## 5 Who turned the heating DOWN?!? ## text ## 1 Picture removal detract earnest is by. Esteems met joy attempt way clothes yet demesne tedious. Replying an marianne do it an entrance advanced. Two dare say play when hold. Required bringing me material stanhill jointure is as he. Mutual indeed yet her living result matter him bed whence. ## 2 Delightful unreserved impossible few estimating men favourable see entreaties. She propriety immediate was improving. He or entrance humoured likewise moderate. Much nor game son say feel. Fat make met can must form into gate. Me we offending prevailed discovery. ## 3 Expenses as material breeding insisted building to in. Continual so distrusts pronounce by unwilling listening. Thing do taste on we manor. Him had wound use found hoped. Of distrusts immediate enjoyment curiosity do. Marianne numerous saw thoughts the humoured. ## 4 Raising say express had chiefly detract demands she. Quiet led own cause three him. Front no party young abode state up. Saved he do fruit woody of to. Met defective are allowance two perceived listening consulted contained. It chicken oh colonel pressed excited suppose to shortly. He improve started no we manners however effects. Prospect humoured mistress to by proposal marianne attended. Simplicity the far admiration preference everything. Up help home head spot an he room in. ## 5 Fat son how smiling mrs natural expense anxious friends. Boy scale enjoy ask abode fanny being son. As material in learning subjects so improved feelings. Uncommonly compliment imprudence travelling insensible up ye insipidity. To up painted delight winding as brandon. Gay regret eat looked warmth easily far should now. Prospect at me wandered on extended wondered thoughts appetite to. Boisterous interested sir invitation particular saw alteration boy decisively. ## authors ## 1 Jon Doe ## 2 By Bob Wilder ## 3 By Ron Crumpet ## 4 By Walter Singsong ## 5 By Wilbert Wonde  Example 3 - Scrape My Website What if I want to scrape the computational notes on my own website? I need a for-loop to iterate over each page, and I’d like to end up with a data set containing the following for each page:\n title\n the text content.\n  My website gets tripped up when it tries to scrape itself while rendering. So, I’ll post this example on github. Here is the link.\n Example 4 - Scrape IMDB In the prior examples, I scraped text. What if I want to scrape a movie rating from IMDB?\n# scrape the score for \u0026#39;there will be blood\u0026#39; movie_page \u0026lt;- read_html(\u0026quot;https://www.imdb.com/title/tt0469494/?ref_=fn_al_tt_1\u0026quot;) review \u0026lt;- html_nodes(movie_page, \u0026quot;strong span\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% as.numeric() review ## [1] 8.2 It would be great if I could create a vector of movie titles and then enter a command to impute a single title into the “search” menu on IMDB, but doing so is not straight forward in R. RSelenium is an option but I wasn’t able to figure it out. Here’s a link that uses it.\nBo\\(^2\\)m =)\n ","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"6b924bd3760355dcd2e471ef2aa16693","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-03-20/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/computational_notes/2020-03-20/","section":"Computational_Notes","summary":"-----","tags":null,"title":"First Web Scraping","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I recently created my first slide deck using Garrick Aden-Buie’s awesome template called “Gentle Ggplot2.” You can find the presentation here and the source code on my GitHub page.\nBo\\(^2\\)m =)\n","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"c7085145ef7caa4211870b4253eff799","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-03-04/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/computational_notes/2020-03-04/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Using Gentle Ggplot2 to Present","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"  Hadley has written extensively about tidy data and why it’s unsound to implement in-place data mutations. Some notes below on breaking both of those rules = ).\nIn-place changes to data using tidyverse.\nlibrary(tidyverse) df \u0026lt;- tibble( \u0026quot;team\u0026quot; = c(\u0026quot;A\u0026quot;, \u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;C\u0026quot;), \u0026quot;individual\u0026quot; = c(1, 2, 3, 4, 5, 6), \u0026quot;performance\u0026quot; = c(NA, 4, 5, 6, 2, 3), \u0026quot;affect\u0026quot; = c(NA, 6, 7, 8, 4, 2), \u0026quot;fav_color\u0026quot; = c(NA, \u0026quot;blue\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;purple\u0026quot;) ) df ## # A tibble: 6 x 5 ## team individual performance affect fav_color ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 A 1 NA NA \u0026lt;NA\u0026gt; ## 2 A 2 4 6 blue ## 3 B 3 5 7 green ## 4 B 4 6 8 orange ## 5 C 5 2 4 yellow ## 6 C 6 3 2 purple Insert a performance, affect, and favorite color value for individual 1 within team A.\ndf %\u0026gt;% filter(team == \u0026quot;A\u0026quot; \u0026amp; individual == 1) %\u0026gt;% mutate(performance = 8, affect = 2, fav_color = \u0026quot;silver\u0026quot;) %\u0026gt;% rbind(df %\u0026gt;% filter(team != \u0026quot;A\u0026quot; \u0026amp; individual != 1)) ## # A tibble: 5 x 5 ## team individual performance affect fav_color ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 A 1 8 2 silver ## 2 B 3 5 7 green ## 3 B 4 6 8 orange ## 4 C 5 2 4 yellow ## 5 C 6 3 2 purple Now for the note on untidy tables. Here’s some tidy data displayed using kable.\nlibrary(kableExtra) dt \u0026lt;- tibble( \u0026#39;team\u0026#39; = c(\u0026#39;A\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;C\u0026#39;), \u0026#39;person\u0026#39; = c(1,2,3, 4,5,6, 7,8,9), \u0026#39;score\u0026#39; = c(rnorm(9, 23, 3)) ) dt %\u0026gt;% group_by(team) %\u0026gt;% summarize( \u0026quot;Mean\u0026quot; = mean(score), \u0026quot;SD\u0026quot; = sd(score) ) %\u0026gt;% kable() %\u0026gt;% kable_styling()   team  Mean  SD      A  21.00513  2.910863    B  23.27578  5.013386    C  24.56087  2.263409     Looks great to me. The issue is that sometimes people expect to see data displayed in “untidy” formats. Let’s change the output so that each team is listed across the first row and the table displays the mean score alongside the standard deviation within parentheses.\nTo do so, I’m going to put string parentheses around the SD values, unite the mean and SD columns, then transform the data from long to wide format. Don’t forget to ungroup as well.\ndt %\u0026gt;% group_by(team) %\u0026gt;% summarize( \u0026quot;Mean\u0026quot; = round(mean(score), digits = 2), \u0026quot;SD\u0026quot; = round(sd(score), digits = 2) ) %\u0026gt;% ungroup() %\u0026gt;% # insert parentheses mutate(SD = paste0(\u0026quot;(\u0026quot;, SD, \u0026quot;)\u0026quot;)) %\u0026gt;% # combine mean and SD columns unite(meansd, Mean, SD, sep = \u0026quot; \u0026quot;, remove = T) %\u0026gt;% # make wide pivot_wider(names_from = team, values_from = meansd) %\u0026gt;% rename(\u0026quot;Team A\u0026quot; = \u0026quot;A\u0026quot;, \u0026quot;Team B\u0026quot; = \u0026quot;B\u0026quot;, \u0026quot;Team C\u0026quot; = \u0026quot;C\u0026quot;) %\u0026gt;% kable(caption = \u0026quot;Team Scores\u0026quot;) %\u0026gt;% kable_styling() %\u0026gt;% footnote(\u0026quot;Mean (SD)\u0026quot;)  Table 1: Team Scores    Team A  Team B  Team C      21.01 (2.91)  23.28 (5.01)  24.56 (2.26)      Note:      Mean (SD)     Bo\\(^2\\)m =)\n","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578960000,"objectID":"3cd22a436123a0356935264c8edbff11","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-01-14/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/computational_notes/2020-01-14/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Untidy Tables \u0026 In-Place Mutation","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I recently created a course skeleton for a research methods or statistics course. The website allows you to incorporate Rmarkdown and dynamic documents to better demonstrate interactive coding.\nYou can find it on github here.\nBo\\(^2\\)m =)\n","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"1fde468ccc95e6a9aeda311cfc6455cc","permalink":"https://christopherdishop.netlify.app/computational_notes/2020-01-10/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/computational_notes/2020-01-10/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Course Skeleton","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Simulating dynamic processes is slow in R. Using the Rcpp function, we can incorporate C++ code to improve performance.\nMy dad, Tim, wrote the C++ code you see here = ).\nExample 1 - Two states, single unit We’re going to simulate data goverened by the following equations:\n\\[\\begin{align*} x_t \u0026amp;= a1x_{t-1} + b1y_{t-1}\\\\ y_t \u0026amp;= a2y_{t-1} + b2x_{t-1}. \\end{align*}\\]\nHere it is in R:\nlibrary(tidyverse) library(Rcpp) # Parameters a1 \u0026lt;- 0.8 a2 \u0026lt;- 0.2 b1 \u0026lt;- -0.5 b2 \u0026lt;- 0.5 # Time points time \u0026lt;- 100 # Initialize df to store the values df \u0026lt;- data.frame( # a vector of length 100 \u0026#39;time\u0026#39; = c(numeric(time)), # a vector of length 100 \u0026#39;x\u0026#39; = c(numeric(time)), \u0026#39;y\u0026#39; = c(numeric(time)) ) # I always like to use a counter even though it isn\u0026#39;t needed here count \u0026lt;- 1 # First time point, x starts at 50 and y at 10 df[1, \u0026#39;time\u0026#39;] \u0026lt;- 1 df[1, \u0026#39;x\u0026#39;] \u0026lt;- 50 df[1, \u0026#39;y\u0026#39;] \u0026lt;- 10 # For loop that iterates over the process for(i in 2:time){ count \u0026lt;- count + 1 # store time df[count, \u0026#39;time\u0026#39;] \u0026lt;- i # x df[count, \u0026#39;x\u0026#39;] \u0026lt;- a1*df[count - 1, \u0026#39;x\u0026#39;] + b1*df[count - 1, \u0026#39;y\u0026#39;] # y df[count, \u0026#39;y\u0026#39;] \u0026lt;- a2*df[count - 1, \u0026#39;y\u0026#39;] + b2*df[count - 1, \u0026#39;x\u0026#39;] } Some of the output…\nhead(df) ## time x y ## 1 1 50.0000 10.0000 ## 2 2 35.0000 27.0000 ## 3 3 14.5000 22.9000 ## 4 4 0.1500 11.8300 ## 5 5 -5.7950 2.4410 ## 6 6 -5.8565 -2.4093 Now, we can do the same thing but use a call to C++ that will improve performance.\n# C++ function cppFunction(\u0026#39;DataFrame createTrajectory(int t, double x0, double y0, double a1, double a2, double b1, double b2) { // create the columns NumericVector x(t); NumericVector y(t); x[0]=x0; y[0]=y0; for(int i = 1; i \u0026lt; t; ++i) { x[i] = a1*x[i-1]+b1*y[i-1]; y[i] = a2*y[i-1]+b2*x[i-1]; } // return a new data frame return DataFrame::create(_[\u0026quot;x\u0026quot;] = x, _[\u0026quot;y\u0026quot;] = y); } \u0026#39;) # Parameters a1 \u0026lt;- 0.8 a2 \u0026lt;- 0.2 b1 \u0026lt;- -0.5 b2 \u0026lt;- 0.5 # Time points time \u0026lt;- 100 # Call the function and run it with 100 time points df \u0026lt;- createTrajectory(time, 50, 10, a1, a2, b1, b2) # Create a time column df$time \u0026lt;- c(1:time) head(df) ## x y time ## 1 50.0000 10.0000 1 ## 2 35.0000 27.0000 2 ## 3 14.5000 22.9000 3 ## 4 0.1500 11.8300 4 ## 5 -5.7950 2.4410 5 ## 6 -5.8565 -2.4093 6  Example 2 - Two states, multiple units In the last example, we simulated \\(x\\) and \\(y\\) over a single unit (e.g., a person, cell, company, nation, etc.). Here, we’ll incorporate multiple units and unobserved heterogeneity.\nThe equations governing the system are:\n\\[\\begin{align*} x_{it} \u0026amp;= a1x_{i(t-1)} + b1y_{i(t-1)} + u_i + e_{it}\\\\ y_{it} \u0026amp;= a2y_{i(t-1)} + b2x_{i(t-1)} + m_i + e_{it} \\end{align*}\\]\nHere is the simulation in base R:\n# Parameters a1 \u0026lt;- 0.8 a2 \u0026lt;- 0.2 b1 \u0026lt;- -0.5 b2 \u0026lt;- 0.5 # Time points and people time \u0026lt;- 100 people \u0026lt;- 500 # Initialize df to store the values df \u0026lt;- data.frame( \u0026#39;time\u0026#39; = c(numeric(time*people)), \u0026#39;person\u0026#39; = c(numeric(time*people)), \u0026#39;x\u0026#39; = c(numeric(time*people)), \u0026#39;y\u0026#39; = c(numeric(time*people)) ) # counter count \u0026lt;- 0 # For each person... for(i in 1:people){ # draw his or her stable individual differences, u and m # draw one value from a normal distribution with mean 0 and sd 2 ui \u0026lt;- rnorm(1, 0, 2) # draw one value from a normal distribution with mean 0 and sd 2 mi \u0026lt;- rnorm(1, 0, 2) # now run this individual across time for(j in 1:time){ count \u0026lt;- count + 1 # first time point if(j == 1){ df[count, \u0026#39;time\u0026#39;] \u0026lt;- j df[count, \u0026#39;person\u0026#39;] \u0026lt;- i # draw 1 value from a normal distribution with mean 50 and sd 5 df[count, \u0026#39;x\u0026#39;] \u0026lt;- rnorm(1, 50, 5) # draw 1 value from a normal distribution with mean 10 and sd 3 df[count, \u0026#39;y\u0026#39;] \u0026lt;- rnorm(1, 10, 3) }else{ # all other time points df[count, \u0026#39;time\u0026#39;] \u0026lt;- j df[count, \u0026#39;person\u0026#39;] \u0026lt;- i df[count, \u0026#39;x\u0026#39;] \u0026lt;- a1*df[count - 1, \u0026#39;x\u0026#39;] + b1*df[count - 1, \u0026#39;y\u0026#39;] + ui + rnorm(1, 0, 1) df[count, \u0026#39;y\u0026#39;] \u0026lt;- a2*df[count - 1, \u0026#39;y\u0026#39;] + b2*df[count - 1, \u0026#39;x\u0026#39;] + mi + rnorm(1, 0, 1) } } } head(df) ## time person x y ## 1 1 1 49.220977 9.841266 ## 2 2 1 34.096754 28.374998 ## 3 3 1 11.233734 23.293777 ## 4 4 1 -4.996531 11.197428 ## 5 5 1 -10.718367 2.078852 ## 6 6 1 -11.556784 -3.251558 Here it is using the Rccp function to incorporate C++ code.\n# C++ function cppFunction(\u0026#39; DataFrame createTrajectory2( int timeSteps, int peopleCount, double a1, double a2, double b1, double b2 ) { // create the columns NumericVector x(timeSteps * peopleCount); NumericVector y(timeSteps * peopleCount); NumericVector time(timeSteps * peopleCount); NumericVector person(timeSteps * peopleCount); int count = 0; int previous = 0; for (int i = 0; i \u0026lt; peopleCount; i++) { // set persons time 0 data // draw 1 value from a normal distribution with mean 50 and sd 5 x[count] = R::rnorm(50, 5); // draw 1 value from a normal distribution with mean 10 and sd 3 y[count] = R::rnorm(10, 3); time[count] = 0; person[count] = i; previous = count; count++; // draw his or her stable individual differences, u and m // draw one value from a normal distribution with mean 0 and sd 2 double ui = R::rnorm(0, 2); // draw one value from a normal distribution with mean 0 and sd 2 double mi = R::rnorm(0, 2); // now run this individual across time for (int j = 1; j \u0026lt; timeSteps; j++) { // all other time points x[count] = a1 * x[previous] + b1 * y[previous] + ui + R::rnorm(0, 1); y[count] = a2 * y[previous] + b2 * x[previous] + mi + R::rnorm(0, 1); time[count] = j; person[count] = i; previous = count; count++; } } // return a new data frame return DataFrame::create(_[\u0026quot;x\u0026quot;] = x, _[\u0026quot;y\u0026quot;] = y, _[\u0026quot;time\u0026quot;] = time, _[\u0026quot;person\u0026quot;] = person); } \u0026#39;) # Parameters a1 \u0026lt;- 0.8 a2 \u0026lt;- 0.2 b1 \u0026lt;- -0.5 b2 \u0026lt;- 0.5 # Time points time \u0026lt;- 100 people \u0026lt;- 500 # Call the function and run it with 100 time steps and 500 people df \u0026lt;- createTrajectory2(time, people, a1, a2, b1, b2) head(df) ## x y time person ## 1 51.400093 7.605188 0 0 ## 2 34.330315 26.935265 1 0 ## 3 7.906593 21.818328 2 0 ## 4 -7.709642 7.087798 3 0 ## 5 -13.813813 -2.327150 4 0 ## 6 -13.626074 -7.587429 5 0 Bo\\(^2\\)m =)\n ","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"9f025e15940e4feff4d74e59b520b399","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-12-10/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/computational_notes/2019-12-10/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Simulations With Rcpp","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.\nHere’s a simulation to punch that point home.\n500 people.\nN \u0026lt;- 500 The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.\nsigma \u0026lt;- matrix(c(1.0, 0.1, 0.4, 0.1, 1.0, 0.4, 0.4, 0.4, 1.0), 3, 3, byrow = T) The mean for each variable is 0.\nmu \u0026lt;- c(0,0,0) Use the correlation matrix and mean specifications to generate data.\nlibrary(MASS) df \u0026lt;- mvrnorm(N, mu, sigma) Turn it into a data frame and label it.\ndf \u0026lt;- data.frame(df) names(df) \u0026lt;- c(\u0026#39;x1\u0026#39;, \u0026#39;x2\u0026#39;, \u0026#39;y\u0026#39;) df$id \u0026lt;- c(1:N) Run regression and print the output.\nsummary(lm(y ~ x1 + x2, data = df)) ## ## Call: ## lm(formula = y ~ x1 + x2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4789 -0.5403 0.0095 0.5213 3.2168 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.02138 0.03713 0.576 0.565 ## x1 0.35510 0.03705 9.585 \u0026lt;2e-16 *** ## x2 0.38420 0.03566 10.775 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.8282 on 497 degrees of freedom ## Multiple R-squared: 0.3226, Adjusted R-squared: 0.3199 ## F-statistic: 118.4 on 2 and 497 DF, p-value: \u0026lt; 2.2e-16 Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.\nCreate a composite using the regression weights.\nlibrary(tidyverse) df \u0026lt;- df %\u0026gt;% mutate(composite_x = 0.33*x1 + 0.4*x2) Those weights provide the maximum correlation between our composite and the outcome.\ncor(df$y, df$composite_x) ## [1] 0.5673049 In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.\nsummary(lm(y ~ composite_x, data = df)) ## ## Call: ## lm(formula = y ~ composite_x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4408 -0.5265 0.0191 0.5268 3.2003 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.01950 0.03703 0.527 0.599 ## composite_x 1.00657 0.06548 15.373 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.8278 on 498 degrees of freedom ## Multiple R-squared: 0.3218, Adjusted R-squared: 0.3205 ## F-statistic: 236.3 on 1 and 498 DF, p-value: \u0026lt; 2.2e-16 Bo\\(^2\\)m =)\n","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"14aa9c60978fc16fc4feb0c6cf31203e","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-12-01/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/computational_notes/2019-12-01/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Regression Creates Weighted Linear Composites","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I’m taking comps pretty soon so this is my summary document regarding meta-analysis.\nMAs give us an average estimate across settings, tests, and people after correcting for noise. In a bare-bones MA, we correct only for sampling error. In a full MA, we correct for sampling error, unreliability, and range restriction. I’ll demonstrate a full MA here where we assume direct (rather than indirect) range restriction.\nSteps Literature Review\n Create inclusion criteria for how you are going to select studies Find relevant articles  Code Articles\n Measures Reliability SD Means Effect sizes Moderators  Calculate Meta-Analytic Estimate\n Calculate meta-analytic effect size Calculate its variance   In this post, I’m focusing only on step 3, the calculations, even though steps 1 and 2 are arguably the more important pieces.\nCalculating MA Estimate and Variance Within this step, there are many substeps:\n Calculate the MA effect estimate (typically from correlations or cohen’s d’s).\nWithin each study gathered from our literature review…\nCorrect the observed correlation for range restriction, which produces a rr-corrected correlation\n Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity\n Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation\n  Calculate the variance in our MA effect estimate\nWithin each study gathered from our literature review…\nCompute a correction factor for unreliability on X Compute a correction factor for unreliability on Y Compute a correction factor for range restriction Combine all of those together\n Compute the error variance for a given observed correlation and correct it using the combined correction factor. This step produces the sampling error correction\n Calculate the average sampling error from the sampling error corrections\n Calculate the observed error variance\n The MA variance estimate is equal to the observed error variance - the average sampling error\n   Before we begin, here is a peak at the (mock) data set. I reviewed four studies and compiled their observed effect sizes – in this case we’re going to use correlations. Let’s say that our IV is dancing ability and our DV is life satisfaction, both are continuous variables. We are interested in the meta-analytic correlation between dancing ability and life satisfaction.\n## study restricted_predictor_sd unrestricted_predictor_sd predictor_reliability ## 1 1 14 20 0.94 ## 2 2 13 20 0.73 ## 3 3 16 20 0.82 ## 4 4 18 20 0.75 ## criterion_reliability sample_size observed_correlation ## 1 0.75 50 0.32 ## 2 0.80 100 0.10 ## 3 0.83 125 0.25 ## 4 0.94 240 0.40  Study = an ID number for each study in my meta-analysis Restricted SD = the standard deviation of scores on dancing ability within the study Unrestricted SD = the standard deviation of scores on on dancing ability across a larger population – from a manual, prior studies, known SDs, field reports, etc. Predictor reliability = the reliability of the measure used to assess dancing ability within the study Criterion reliability = the reliability of the measure used to assess life satisfaction within the study Sample size = how many people were observed within the study Observed correlation = the correlation between dancing ability and life satisfaction within the study   a) Calculate the MA correlation For each study gathered from our literature review…\n1) Correct the observed correlation for range restriction, which produces a rr-corrected correlation\n\\[\\begin{equation} r_{RR} = \\dfrac{ \\left(\\dfrac{US_{x}}{RS_{x}}\\right)r_{xy} } {\\sqrt{1 + r^2_{xy}\\left(\\dfrac{US^2_{x}}{RS^2_{x}} - 1\\right)} } \\end{equation}\\]\nwhere \\(r_{RR}\\) is the correlation that is corrected for range restriction, \\(US_x\\) is the unrestricted SD on dancing ability, \\(RS_x\\) is the restricted SD on dancing ability, and \\(r_{xy}\\) is the correlation between dancing ability and life satisfaction. We are going to compute \\(r_{RR}\\) for every study.\ndf \u0026lt;- df %\u0026gt;% mutate(r_RR = ((unrestricted_predictor_sd / restricted_predictor_sd)*observed_correlation) / sqrt( 1 + ((observed_correlation^2) * ((unrestricted_predictor_sd / restricted_predictor_sd) -1)) ) )  2) Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity\n\\[\\begin{equation} r_{ov} = \\dfrac{r_{RR}}{\\sqrt{r_{yy}}} \\end{equation}\\]\nwhere \\(r_{ov}\\) is the operational validity of dancing ability and life satisfaction, \\(r_{RR}\\) is the correlation we calculated in step 1 (the range-restriction-corrected correlation) between dancing ability and life satisfaction, and \\(r_{yy}\\) is the reliability of the criterion, life satisfaction.\ndf \u0026lt;- df %\u0026gt;% mutate(r_ov = r_RR / sqrt(criterion_reliability))  3) Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation\n\\[\\begin{equation} \\rho = \\dfrac{\\sum{w_sr_{ov_{i}}}}{\\sum{w_s}} \\end{equation}\\]\nwhere \\(\\rho\\) is the meta-analytic estimate, \\(r_{ov_{i}}\\) is the operational validity between dancing ability and life satisfaction for each study, and \\(w_s\\) is the sample size for each study.\novs_by_sample_size \u0026lt;- df$sample_size * df$r_ov ma_correlation \u0026lt;- sum(ovs_by_sample_size) / sum(df$sample_size) df \u0026lt;- df %\u0026gt;% mutate(ma_correlation = ma_correlation)   b) Calculate the variance in our MA effect estimate Compile all of the corrections – steps 4 through 7\n4) Compute the correction factor for unreliability on X, dancing ability (take the square root of the reliability)\ndf \u0026lt;- df %\u0026gt;% mutate(cf_x = sqrt(predictor_reliability))  5) Compute the correction factor for unreliability on Y, life satisfaction (take the square root of the reliability)\ndf \u0026lt;- df %\u0026gt;% mutate(cf_y = sqrt(criterion_reliability))  6) Compute the correction factor for range restriction\n\\[\\begin{equation} a_{rr} = \\dfrac{1}{ \\left(\\left(\\dfrac{US_x}{RS_x}\\right)^2 - 1\\right)r_{xy}^2 + 1} \\end{equation}\\]\nwhere all terms are defined above.\ndf \u0026lt;- df %\u0026gt;% mutate(cf_rr = 1 / ( ((unrestricted_predictor_sd / restricted_predictor_sd)^2 - 1)*(observed_correlation^2) + 1 ) )  7) Combine all of those correction factors together into one common correction factor, \\(A\\).\ndf \u0026lt;- df %\u0026gt;% mutate(A = cf_x*cf_y*cf_rr)  8) Compute the error variance for a given observed correlation and correct it using the combined correction factor.\nThis part takes three steps.\nI: Compute the sample size weighted observed correlation - Essentially the same thing as step 3 but using observed correlations rather than operational validities  \\[\\begin{equation} r_{wa} = \\dfrac{\\sum{w_sr_{xy_{i}}}}{\\sum{w_s}} \\end{equation}\\]\nss_times_correlations \u0026lt;- df$sample_size*df$observed_correlation wa_correlation \u0026lt;- sum(ss_times_correlations) / sum(df$sample_size) II: Compute the error variance on the observed correlation for each study \\[\\begin{equation} \\sigma^2_e = \\dfrac{\\left(1-r_{wa}^2\\right)^2}{N-1} \\end{equation}\\]\nwhere \\(\\sigma^2_e\\) is the error variance (for each study), \\(r_{wa}\\) is the weighted average observed correlation between dancing ability and life satisfaction that we computed above, and \\(N\\) is the sample size.\ndf \u0026lt;- df %\u0026gt;% mutate(sigma2e = ((1 - wa_correlation^2)^2) / (sample_size - 1) ) III: Compute the sampling error correction for each study \\[\\begin{equation} Var_{ec} = \\dfrac{\\sigma^2_e}{A^2} \\end{equation}\\]\nwhere \\(Var_{ec}\\) is the sampling error correction, \\(\\sigma^2_e\\) is what we just calculated above, the error variance on the observed correlation for each study, and \\(A\\) is the combined correction factor for each study.\ndf \u0026lt;- df %\u0026gt;% mutate(var_ec = sigma2e^2 / A^2)  9) Calculate the average sampling error\n\\[\\begin{equation} Ave_{var_{ec}} = \\dfrac{\\sum{w_sVar_{ec}}}{\\sum{w_s}} \\end{equation}\\]\nwhere \\(Ave_{var_{ec}}\\) is the average sampling error, \\(w_s\\) is the sample size for each study, and \\(Var_{ec}\\) is the sampling error correction for each individual study.\nss_times_varec \u0026lt;- df$sample_size*df$var_ec ave_var_ec \u0026lt;- sum(ss_times_varec) / sum(df$sample_size)  10) Calculate the observed error variance\n\\[\\begin{equation} var_r = \\dfrac{\\sum{w_s\\left(r_{xy} - r_{ov}\\right)^2}}{\\sum{w_s}} \\end{equation}\\]\nwhere all terms are defined above.\nss_times_r_minus_ov \u0026lt;- df$sample_size*((df$observed_correlation - df$r_ov)^2) var_r \u0026lt;- sum(ss_times_r_minus_ov) / sum(df$sample_size)  11) The MA variance estimate is equal to the observed error variance - the average sampling error\n\\[\\begin{equation} Var_p = var_r - Ave_{var_{ec}} \\end{equation}\\]\nvar_p \u0026lt;- var_r - ave_var_ec   Recap What a nightmare. Here’s a recap:\n Correct the observed correlations for unreliability and range restriction, use them to compute a sample-size-weighted MA correlation coefficient\n Make a bunch of corrections and compute the average sampling error, and subtract that from the observed variance of the correlation coefficient to get a sense for the MA correlation coefficient variance\n  Now we can calculate credibility and confidence intervals.\n Credibility Interval Gives us a sense for whether or not moderators are at play.\n\\[\\begin{equation} \\textrm{95 credibility interval} = \\rho +- 1.96*\\sqrt{Var_p} \\end{equation}\\]\nupper_cred_i = ma_correlation + (1.96 * sqrt(var_p)) lower_cred_i = ma_correlation - (1.96 * sqrt(var_p)) upper_cred_i ## [1] 0.5532591 lower_cred_i ## [1] 0.2024129 Credibility Ratio: if \\(\\dfrac{ave_{var_{ec}}}{var_{r}}\\) is lower than 0.75, then moderators may be at play.\nave_var_ec / var_r ## [1] 0.01211969  Confidence Interval \\[\\begin{equation} \\textrm{95 confidence interval} = r_{ov} +- 1.96*SE_{r_{ov}} \\end{equation}\\]\nwhere \\(SE_{r_{ov}}\\) is the standard error of the operational validities and is calculated as…\n\\[\\begin{equation} SE_{r_{ov}} = \\dfrac{SD_{r_{ov}}}{\\sqrt{k}} \\end{equation}\\]\nwhere \\(k\\) is the number of studies.\nse_r_ov = sd(df$r_ov) / sqrt(length(df$study)) upper_ci = ma_correlation + (1.96 * se_r_ov) lower_ci = ma_correlation - (1.96 * se_r_ov) upper_ci ## [1] 0.5263396 lower_ci ## [1] 0.2293324 Bo\\(^2\\)m =)\n  ","date":1568505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568505600,"objectID":"5289bf32a9f92591e926e4132165c51f","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-09-15/","publishdate":"2019-09-15T00:00:00Z","relpermalink":"/computational_notes/2019-09-15/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Meta Analysis Comps Notes","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" In many research methods or statistics courses you come across the idea that correlated errors signal a third variable. In other words, you have a missing, relevant variable that induces correlation among your residuals. That’s a tough idea to wrap your head around, but it is easier to consider with respect to a given topic: cheating on exams. This post builds intuition for “correlated errors with respect to missing third variables” in the context of college exams and cheating.\nThe Exam Structure First, let’s get a feel for the exams. I’m going to use a lot of images in this post so it helps to walk through the basics of each plot. Imagine students taking a multiple choice test where they fill in one of five responses, “A,” “B,” “C,” “D,” or “E” for each question. The correct answer for question one is “C”\nwhere the x-axis shows the response options a student can select for each question, and the y-axis shows the question number (there is only one question so far).\nThe correct answer for question two is also “C”\nand that pattern continues for the rest of the questions on this 5-item test.\nIn other words, imagine a 5 question exam where the correct answer for each question is “C.” With the basic images in play, we can think about how students might respond.\n No Cheating – What is the Pattern of Errors across Questions? First, consider an exam where students do not cheat. If nobody cheats, then everyone’s errors will be dispersed about the true option for each question, “C.” Some people falsely select “A” whereas others falsely select “E,” and yet others falsely select “B.” Here is a plot that retains the purple crosses that mark the true option, “C,” but also includes student responses from Susie, Peter, and John.\nFor example, on question one Susie selects “A,” Peter selects “E,” and John selects “D,” meaning that none of the students get the answer correct. Every student, though, marks the correct response (C) for question 4.\nThere is no pattern in this plot. The green triangles, red circles, and blue-green squares are dispersed about the true score purple-crosses randomly. John gets some questions wrong, Susie gets some questions wrong, and Peter gets some questions wrong, but whether John incorrectly marks “A” or “E” doesn’t tell us anything about whether Peter incorrectly marks “A” or “E.” They are all wrong in a random way.\nWhat about when students cheat?\n Cheating – What is the Pattern of Errors across Questions? When students cheat, the errors, or “wrongness” of questions, produce a pattern – meaning that the errors are correlated. Imagine that a cheater, let’s say it’s Peter, hacks the teacher’s computer and gains access to all of the questions before the exam. He then answers all of the questions and sends his responses to the rest of the class (John and Susie). But Peter makes a mistake: he writes down the wrong answers to four of the questions. John and Susie use Peter’s responses on the exam, but since they were copying from Peter’s responses they have the same pattern on “wrongness” on the four questions that Peter missed – they get the same questions wrong in the same way. After the exam, the pattern of scores looks as follows.\nThe true, correct responses are again labeled with purple cross-hairs: response “C” is the correct answer for each question. On question one, John, Peter, and Susie all incorrectly selected “A.” On question three, everyone incorrectly selected “E,” and on question four everyone incorrectly selected “D.” John, Peter, and Susie are wrong in the same way across the questions, their errors produce a pattern, a consistency, a correlation. The errors in our system correlate, which signals a third variable. In this case, the third variable is cheating.\nSo, think of exams and cheating when you hear the mantra, “correlated errors signal a third variable.” When an important variable is omitted from an analysis, the model is said to be missing a third variable and the errors may correlate. What this tells you is that something else – something unaccounted for – is influencing the patterns in your system, in the same sense as cheating influencing the pattern of responses on an exam.\nBo\\(^2\\)m =)\n ","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"333ed11beff35973816b713f7b976658","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-06-26/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/computational_notes/2019-06-26/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Intuition for Correlated Errors and Third Variables","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Inspired by Bill Burr’s Monday Morning Podcast, episode 6-6-19.\nHe describes how people that do not understand hockey wanted to get fighting out of the NHL, so in the 2000’s they made greater efforts to remove goons (fighters). What people outside the NHL often do not understand, though, is that fighting is used to minimize dirty play. When a team is playing dirty, you – as a couch of the team that is not playing dirty – send out a goon to punch around a few of their players. In doing so, the other team knocks it off and play continues candidly. Fighting looks bad, but the overall amount of dirty play in the NHL is reduced when it contains a sufficient population of goons. Sounds like systems thinking.\nSimulation Set-Up  States and Relationships\n 2 states modeled over time (number of goons and number of dirty plays, or the level of “dirtiness” in the NHL)\n Number of goons fluctuates independently but with autoregression\n N_goons(t) = N_goons(t-1)  Dirtiness level is a function of its prior self and the number of goons, such that a greater number of goons causes lower levels of dirtiness\n D_level(t) = D_level(t-1) - N_goons(t)   Flow\n First, watch the states fluctuate over time and establish equilibrium\n Second, remove a bunch of goons and then see what happens to the system\n    Simulation Initial levels of goons and dirtiness.\nn_goons_initial \u0026lt;- 30 d_level_initial \u0026lt;- 15 Now simulate the states across time (20 time points) according to the simulation set-up above. Alpha will be set to 0.7 for both states and beta will be set to 0.2. The forcing terms for goons and dirtiness will be, respectively, 25 and 20.\ntime \u0026lt;- 30 df_mat \u0026lt;- matrix(ncol = 3, nrow = time) count \u0026lt;- 0 for(i in seq_along(1:time)){ count \u0026lt;- count + 1 if(i == 1){ df_mat[count, 1] \u0026lt;- n_goons_initial df_mat[count, 2] \u0026lt;- d_level_initial df_mat[count, 3] \u0026lt;- i }else{ df_mat[count, 1] \u0026lt;- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1) df_mat[count, 2] \u0026lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1) df_mat[count, 3] \u0026lt;- i } } df \u0026lt;- data.frame(df_mat) names(df) \u0026lt;- c(\u0026#39;n_goons\u0026#39;, \u0026#39;d_level\u0026#39;, \u0026#39;time\u0026#39;) View both states over time.\nlibrary(ggthemes) df_plot \u0026lt;- df %\u0026gt;% gather(n_goons, d_level, key = \u0026#39;variable\u0026#39;, value = \u0026#39;value\u0026#39;) ggplot(df_plot, aes(x = time, y = value, color = variable)) + geom_point() + geom_line() As you can see, the number of goons and the dirtiness level in the NHL drive toward equilibrium levels over time. There are goons, which means there are fights and the potential to appear “dirty” to anyone without an understanding of the system, but having goons around maintains the overall dirtiness within the NHL at low levels.\nNow, what happens to the level of dirtiness when we remove a bunch of goons at time point 14 and beyond?\nn_goons_initial \u0026lt;- 30 d_level_initial \u0026lt;- 15 time \u0026lt;- 30 df_mat \u0026lt;- matrix(ncol = 3, nrow = time) count \u0026lt;- 0 for(i in seq_along(1:time)){ count \u0026lt;- count + 1 if(i == 1){ df_mat[count, 1] \u0026lt;- n_goons_initial df_mat[count, 2] \u0026lt;- d_level_initial df_mat[count, 3] \u0026lt;- i }else if (i \u0026lt;=13){ df_mat[count, 1] \u0026lt;- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1) df_mat[count, 2] \u0026lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1) df_mat[count, 3] \u0026lt;- i # HERE IS THE CHANGE }else if(i \u0026gt; 13){ num_goons \u0026lt;- sample(c(2,3,4), 1) df_mat[count, 1] \u0026lt;- num_goons df_mat[count, 2] \u0026lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1) df_mat[count, 3] \u0026lt;- i ###################################### } } df \u0026lt;- data.frame(df_mat) names(df) \u0026lt;- c(\u0026#39;n_goons\u0026#39;, \u0026#39;d_level\u0026#39;, \u0026#39;time\u0026#39;) df_plot \u0026lt;- df %\u0026gt;% gather(n_goons, d_level, key = \u0026#39;variable\u0026#39;, value = \u0026#39;value\u0026#39;) ggplot(df_plot, aes(x = time, y = value, color = variable)) + geom_point() + geom_line() What happened? The level of dirtiness increases after removing goons. In other words, removing goons, or fighters, from the NHL may make the game appear more civil from the outside, but goons are embedded in a system that maintains overall low levels of dirtiness. When the goons are removed – and they are a crucial part of the system – dirtiness levels increase dramatically.\nBo\\(^2\\)m =)\n ","date":1560470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560470400,"objectID":"8adf5aa4bf30bb58463b7b3c55bd1c88","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-06-14/","publishdate":"2019-06-14T00:00:00Z","relpermalink":"/computational_notes/2019-06-14/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Systems Thinking on Goons in the NHL","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Quick note about calculating the mean of a column with dplyr in R. It’s surprisingly easy to screw up, and the culprit is forgetting to change the name of the column storing the new calculation.\nA simple dataframe.\nlibrary(tidyverse) df \u0026lt;- data.frame( \u0026#39;books_read\u0026#39; = c(1,2,3,4,5,6), \u0026#39;intelligence\u0026#39; = c(4,5,6,7,8,8) ) df ## books_read intelligence ## 1 1 4 ## 2 2 5 ## 3 3 6 ## 4 4 7 ## 5 5 8 ## 6 6 8 I want to calculate the mean and standard deviation of the “books read” column. If I calculate the mean and then place it into a new column that has the same name as the original variable, then standard deviation command doesn’t work.\nlibrary(tidyverse) df %\u0026gt;% summarise( books_read = mean(books_read), # this line is the problem sd_books_read = sd(books_read) ) ## books_read sd_books_read ## 1 3.5 NA Instead, I need to call the new “mean books read” column a different name.\nlibrary(tidyverse) df %\u0026gt;% summarise( mean_books_read = mean(books_read), # this line is the problem sd_books_read = sd(books_read) ) ## mean_books_read sd_books_read ## 1 3.5 1.870829 Bo\\(^2\\)m =)\n","date":1556928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556928000,"objectID":"825bac776ac69e920196ee4c5ad45a09","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-05-04/","publishdate":"2019-05-04T00:00:00Z","relpermalink":"/computational_notes/2019-05-04/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Screwing Up A Mean Calculation","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Think about what it would mean to explain something to a friend. How would you explain why the Patriot’s won the Superbowl? How would you explain why you are feeling happy or sad? How would you explain tying a shoe to a toddler? How would you explain why eating lots of donuts tends to increase a person’s weight? How would you explain the timeline of Game of Thrones to someone who hadn’t seen it?\nWhat you came up with, or thought about, is different from how “explaining” is usually used in research. We typically use the term to mean “explain variation,” and the goal of this post is to flesh-out what that means. While you read this, though, try to keep in mind the thought, “how does this connect to the way I explain things in everyday life?”\nImagine that we have a scatter plot, performance on the Y axis and ability on the X axis.\nWe collected data on several people – Bob, Chris, Julie, Monte, and Rachel, we measured each person’s performance and ability – and those individual data points are represented in the scatterplot. In statistics, what we try to do is account for variability in performance, we try to explain variation in performance. Here is what that means.\nTake the mean of performance as a flat, horizontal line across all values of ability. For now we do not care about ability, we are just using it to visualize the data.\nNotice that the mean of performance does not perfectly align with the observed data. That is, each of the points on the plot do not fall exactly on the horizontal line. If they did, we would say that the mean of performance perfectly explains the variability in performance. Instead, each of the points has some distance from the mean of performance line, and we call those distances residuals.\nWhat those residuals mean conceptually is that the observed data points do not fall exactly on the mean of performance. Performance cannot be explained simply by its mean. There is variation in performance that is left to explain, there are distances (residuals) that are not accounted for.\nSumming across all of those residuals gets us what is called the total sum of squares. All of the observed values have some distance from the mean performance line, when we aggregate all of those distances (all of the vertical line segments) we get an index of the variability in performance that we are trying to explain.\nTSS = sum of vertical, residual distances\nThe real equation for TSS uses squares because negative distances will cancel positive distances, but this is a conceptual write-up. So we are ok ignoring that for now.\nSo, we have variation in performance that is not accounted for by the mean of performance. Now imagine that we believe some super complicated function of ability (X) explains the variation in performance. This super complicated function is a crazy line that perfectly runs through every observed data point.\nNow remove the observed data points from the graph so that we are only looking at the mean of performance and the predicted values of performance as a function of ability.\nNow we have a graph of PREDICTED data. That is, the black line does not have observed data points, it does not represent what we saw when we collected data and measured performance and ability on Bob and the others. We are looking at the predicted values of performance based on some super complicated function of ability. Notice that the black line also has distances from the mean of performance, so we can sum across those distances to get another quantity, called the expected sum of squares.\nESS = sum of vertical, residual distances (but from our predicted line rather than our observed data points)\nBecause our super complicated function perfectly went through each observed data point, TSS is equivalent to ESS in this case. That means that our super complicated function perfectly explains the variation in performance. We have accounted for all variance in our outcome.\nUsually we don’t use super complicated equations. We tend to posit linear functions, such that we think that performance is a linear function of ability. If we were to plot a predicted line showing performance as a linear function of ability, the residual distances would change and ESS would be different from TSS, meaning that we explained some, but not all of the variation in performance.\nThat is what explaining means in research and statistics (technically, “explaining variation”). Observed data varies about the mean on some dependent variable and there are distances from observed data points to the mean line. If we aggregate those distances together we get TSS, a sense of the total variation in the DV. Then we create some equation relating predictors to the outcome and use it to generate new values of the DV (i.e., predicted values). Explaining in statistics means, “to what extent do my predicted values have the same pattern of distances as my observed values?” “To what extent are the distances from the predicted values to the mean line the same as the distances from the observed values to the mean line?” “To what extent is the total variation represented by the predicted variation?” Is TSS equivalent to ESS?\nNow return to the notion that we opened with, to what extent does explaining variation reflect how you explain things in everyday life?\nConnecting to Causality How does all of this connect to causality? Knowing about cause helps you “explain variation,” but explaining more or less variation does not give you information about cause. Said differently, knowledge about cause, or the data generating process, or truth, will influence your ability to explain variation, but improving your ability to explain variation will not necessarily produce insights about the DGP. If you know the true process – i.e., the DGP, the underlying structure of effects and variables, the causal system – then you will be able to (almost perfectly) explain variation in the sense that I described in this post. If you know the true process, then you will be able to explain why Bob’s score is different from Julie’s, why some variables correlate with the outcome and others don’t, and why Monte’s score is different from time 1 versus time 2. Full knowledge of the DGP means you can predict what happens next, there are no unknowns left to make your ESS different from your TSS.\nBut the reverse is not true. Just because you can explain variation – in the statistical sense described here – does not mean that you have the right DGP or know anything about cause. I could have the wrong DGP, the wrong notion about the causal structure, the wrong variables in the model, but improve my ability to “explain variation” by including more variables in my model. I could include additional, irrelevant variables to make my model more complex and subsequently improve my ability to “explain variation,” but I wouldn’t produce any greater knowledge about cause or the DGP. Knowledge about cause, explanation, why, or the DGP comes from research designs and assumptions, not statistical models. Did you randomly assign and manipulate, and were there strong assumptions involved in the form of DAGS? Did you “wiggle” or change some variables (think Judea Pearl) and observe the effect of doing so on other variables in a controlled environment? Fancy stats don’t get you there, great research designs and assumptions sometimes do.\nBo\\(^2\\)m =)\n ","date":1556841600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556841600,"objectID":"545d424a94cd6bd67849d98b87049b02","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-05-03/","publishdate":"2019-05-03T00:00:00Z","relpermalink":"/computational_notes/2019-05-03/","section":"Computational_Notes","summary":"-----","tags":null,"title":"What Explaining Means in Statistics","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A growth curve model written in lavaan and MPLUS as a syntax reference guide. Imagine a latent growth curve on affect across 4 time points. First, lavaan code:\nlavaan_string \u0026lt;- \u0026#39; # Latent intercept and slope factors intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 # Mean and variance of latent factors intercept_affect ~~ intercept_affect slope_affect ~~ slope_affect # Covariance between latent factors intercept_affect ~~ slope_affect # Fix observed variable means to 0 affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 # Constrain residual (error) variance of observed variables to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 \u0026#39; Now the same thing in MPLUS syntax:\nmplus_string \u0026lt;- \u0026#39; ! Latent intercept and slope factors intercept_affect BY affect.1@1 affect.2@1 affect.3@1 affect.4@1; slope_affect BY affect.1@0 affect.2@1 affect.3@3 affect.4@5; ! estimate mean of latent intercept [intercept_affect]; ! estimate mean of latent slope [slope_affect] ! estimate variance of intercept intercept_affect; ! estimate variance of slope slope_affect; ! covariance between intercept and slope intercept_affect WITH slope_affect; ! Fix observed variable means to 0 so we can estimate a mean for the latent variable [affect.1@0 affect.2@0 affect.3@0 affect.4@0]; ! constrain estimates of residual variances to be equivalent at each time point affect.1(res_var); affect.2(res_var); affect.3(res_var); affect.4(res_var); \u0026#39; Bo\\(^2\\)m =)\n","date":1556755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556755200,"objectID":"5bd0054dcfeb6528576846bf48322587","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-05-02/","publishdate":"2019-05-02T00:00:00Z","relpermalink":"/computational_notes/2019-05-02/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Lavaan MPLUS Reference Sheet","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" This post is about building intuition for degrees of freedom. There are two ways to think about it, the “information” way and the “line” way.\nThe Information Way The quantity, degrees of freedom, is the amount of information available in our data set minus the amount of information we want to pull from it. Here are a bunch of different ways of representing that idea:\n\\[\\begin{equation} \\textrm{DF} = \\textrm{Knowns} - \\textrm{Unknowns} \\end{equation}\\]\n\\[\\begin{equation} \\textrm{DF} = \\textrm{Unique information} - \\textrm{Used information} \\end{equation}\\]\n\\[\\begin{equation} \\textrm{DF} = \\textrm{Unique information in observed data} - \\textrm{Information we want to pull from our data} \\end{equation}\\]\n\\[\\begin{equation} \\textrm{DF} = \\textrm{Stuff in the correlation matrix} - \\textrm{Stuff we want to use the correlation matrix for} \\end{equation}\\]\n\\[\\begin{equation} \\textrm{DF} = \\textrm{Unique entries in the variance/covariance matrix} - \\textrm{Estimated parameters} \\end{equation}\\]\nIn a data set, information is (roughly) the unique variances and covariances that I can use. If I estimate too many parameters without enough information (i.e., without enough observed variables), then I loose DFs and I can’t estimate anything. I cannot estimate a super complicated function if I only observe one measurement of performance and one measurement of motivation.\n The Line Way The line way builds off the idea that DF = unique information - used information, but we’re going to walk through it visually. Imagine the equation\n\\[\\begin{equation} y = mx + b. \\end{equation}\\]\nAs written, an infinite number of lines can represent that equation. The intercept can be any value and the slope can be any value. There are no constraints.\nNow imagine an equation with a constraint, such that the intercept must be 2:\n\\[\\begin{equation} y = mx + 2. \\end{equation}\\]\nYou can still draw infinite lines here, but all of them must go through 2 as their intercept.\nNow we can count our degrees of freedom. Let’s say each example above had 10 pieces of information (10 observed variables).\n\\[\\begin{equation} y = mx + b \\textrm{;} \\textrm{ with 10 pieces of information and 2 estimated parameters. DF = 10 - 2. DF = 8} \\end{equation}\\]\n\\[\\begin{equation} y = mx + 2 \\textrm{;} \\textrm{ with 10 pieces of information and 1 estimated parameter. DF = 10 - 1. DF = 9} \\end{equation}\\]\nWhen you estimate an additional parameter you lose a degree of freedom. When you constrain things, you gain degrees of freedom. So, the second example has more degrees of freedom, even though it feels like we’re not as “free” to draw our lines. Tricky.\nBo\\(^2\\)m =)\n ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"79ae41032dc9f378c8a449b7cb2ccaf4","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-05-01/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/computational_notes/2019-05-01/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Degrees of Freedom Intuition","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" In regression, everything is partialled from everything. Let’s work through that notion with images and code. Imagine that emotion and ability cause an outcome, \\(Y\\).\nWhat this image represents is that \\(Y\\) has variability (across people or time), and its variability is associated with variability in emotion and variability in ability. Notice that there is variability overlap between ability and \\(Y\\),\nemotion and \\(Y\\),\nemotion and ability,\nand all three variables.\nOnce we regress \\(Y\\) on emotion and ability, the regression coefficients represent the unique variance components of each predictor\nbut the technique also removes outcome-relevant variance\nand overlapping variance in emotion and ability not related to the outcome.\nSo, in regression we get coefficients that represent the unique variance contribution of each predictor while partialling overlapping, outcome-relevant variance and overlapping, non-relevant variance. Emotion and ability get to account for their own causal effects of \\(Y\\), but neither predictor gets the overlapping variance in \\(Y\\), and the emotion and ability coefficients are adjusted for the emotion-ability overlap situated outside \\(Y\\).\nLet’s do it with code.\nOur sample contains 500 people with correlated emotion and ability (\\(r\\) = 0.4).\npeople \u0026lt;- 500 emotion \u0026lt;- rnorm(people, 0, 10) ability \u0026lt;- 0.4*emotion + rnorm(people, 0, 1) # could also do it with MASS Ability and emotion cause \\(Y\\).\nerror \u0026lt;- rnorm(people, 0, 1) Y \u0026lt;- 2 + 0.5*ability + 0.38*emotion + error Regression will recover the parameters.\ndf \u0026lt;- data.frame( \u0026#39;emotion\u0026#39; = c(emotion), \u0026#39;ability\u0026#39; = c(ability), \u0026#39;y\u0026#39; = c(Y) ) summary(lm(y ~ ability + emotion, data = df))$coefficients[,1] ## (Intercept) ability emotion ## 1.9531540 0.4584228 0.4002376 Remember, each coefficient is consistent with the “lightning bolt” variance components above. Outcome-relevant overlap is removed and overlap between emotion and ability is removed. Since emotion and ability are partialled from each other, we won’t recover the 0.38 parameter relating emotion to \\(Y\\) if we remove ability from the equation.\nsummary(lm(y ~ emotion, data = df))$coefficients[,1] ## (Intercept) emotion ## 1.9648929 0.5861932 How can we modify our variables to represent the “partialled multiple regression coefficient” for emotion? Naively, it seems that if we remove ability from \\(Y\\) and then regress \\(Y\\) on emotion we will recover the appropriate 0.38 parameter. Let’s try.\nRegress \\(Y\\) on just ability\njust_ability \u0026lt;- lm(y ~ ability, data = df) and take the residuals, meaning that in our next regression we will examine the effect of emotion on “leftover \\(Y\\)” – \\(Y\\) with no influence from ability.\ny_with_ability_removed \u0026lt;- resid(just_ability) df$y_with_ability_removed \u0026lt;- y_with_ability_removed summary(lm(y_with_ability_removed ~ emotion, data = df))$coefficients[,1] ## (Intercept) emotion ## 0.01927374 0.02051948 Nope. Why not? Think back to the diagrams, what we just assessed was\nwhere the estimate accounts for the \\(Y\\)-relevant overlap of emotion and ability, but it is wrong because it doesn’t account for the overlap between emotion and ability situated outside of \\(Y\\). In regression, everything is partialled from everything…we have not yet accounted for the overlap between emotion and ability in the space not in the \\(Y\\) variance sphere. Now we will.\nPartial ability from emotion\nemotion_with_ability_removed \u0026lt;- resid(lm(emotion ~ ability, data = df)) df$emotion_with_ability_removed \u0026lt;- emotion_with_ability_removed and now when we regress “Y with ability removed” on “emotion with ability removed” we will recover the 0.38 parameter.\nsummary(lm(y_with_ability_removed ~ emotion_with_ability_removed, data = df))$coefficients[,1] ## (Intercept) emotion_with_ability_removed ## -7.545488e-17 4.002376e-01 In regression, everything is partialled from everything.\nThe technique partials overlapping predictor variance both within and outside of the \\(Y\\) space. Neither predictor accounts for overlapping variance within \\(Y\\), and if an important predictor is excluded then it will artificially account for variance it shouldn’t be capturing.\nNote that all of this is relevant for III sums of squares…there are other approaches but III is by far the most common.\nBo\\(^2\\)m =)\n","date":1555632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555632000,"objectID":"f8de9f0d4a5a10ea0ddbb05d5410a10e","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-04-19/","publishdate":"2019-04-19T00:00:00Z","relpermalink":"/computational_notes/2019-04-19/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Everything Partialled From Everything in Regression","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Quick piece of code that turns all selected columns to numeric in R.\ndf[, c(\u0026#39;col1\u0026#39;, \u0026#39;col2\u0026#39;)] \u0026lt;- as.numeric(as.character(unlist(df[, c(\u0026#39;col1\u0026#39;, \u0026#39;col2\u0026#39;)]))) Mutating within tidyverse is always a good options as well.\ndf %\u0026gt;% mutate_at(vars(\u0026#39;column1\u0026#39;, \u0026#39;column2\u0026#39;), as.character) Bo\\(^2\\)m =)\n","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"18b9b6c3b632542266be9503907b1b73","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-04-10/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/computational_notes/2019-04-10/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Convert Multiple Columns to Numeric or Character","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Two ways to simulate a moving average process. A moving average is a linear combination of concurrent and historic noises:\n\\[\\begin{equation} y_t = z_t + z_{t-1} + z_{t-2} \\end{equation}\\]\nwhere \\(y_t\\) is the outcome variable that is influenced by noise at this moment (\\(z_t\\)) and noise from the last two time points. MA(q) processes can occur at any lag, I will use a two lag version here.\nThe first way to simulate this process is to generate all noise terms and then sample from that distribution throughout our recursive routine.\nset.seed(25) time \u0026lt;- 200 noise \u0026lt;- rnorm(time) ma_2 \u0026lt;- NULL for(i in 3:time){ ma_2[i] \u0026lt;- noise[i] + 0.7*noise[i-1] + 0.2*noise[i-2] } That simulation results in the following.\nlibrary(tidyverse) library(ggplot2) df1 \u0026lt;- data.frame( \u0026#39;time\u0026#39; = c(1:time), \u0026#39;y\u0026#39; = c(ma_2) ) ggplot(df1, aes(x = time, y = y)) + geom_point() + geom_line()  The second way to simulate it is to generate noise within the loop itself, store the noise, and then apply it to the outcome across time.\nset.seed(15) yt \u0026lt;- numeric(time) zs \u0026lt;- numeric(time) for(i in 1:time){ if(i == 1){ zs[i] \u0026lt;- rnorm(1,0,1) yt[i] \u0026lt;- zs[i] }else if(i == 2){ zs[i] \u0026lt;- rnorm(1,0,1) yt[i] \u0026lt;- zs[i] + 0.7*zs[i-1] }else{ zs[i] \u0026lt;- rnorm(1,0,1) yt[i] \u0026lt;- zs[i] + 0.7*zs[i-1] + 0.2*zs[i-2] } } Here is the plot.\ndf2 \u0026lt;- data.frame( \u0026#39;time\u0026#39; = c(1:time), \u0026#39;y\u0026#39; = c(yt) ) ggplot(df2, aes(x = time, y = yt)) + geom_point() + geom_line() The second simulation style takes more code but I find it more intuitive. It is difficult for me to wrap my head around simulating all of the noise first and then applying it to the process as if the two are independent components – which is what the first simulation code mimics. To each their own.\nBo\\(^2\\)m =)\n","date":1548720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548720000,"objectID":"d82c6a0600cf2dcca753f2bddf11778c","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-29/","publishdate":"2019-01-29T00:00:00Z","relpermalink":"/computational_notes/2019-01-29/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Simulating a Moving Average Process","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Longitudinal data of a group or team often have missing days. For example, only Bob reports a stress score on January 3rd even though Joe and Sam are also part of the sample.\n## id date stress ## 1 bob 2019-01-01 4 ## 2 joe 2019-01-01 5 ## 3 sam 2019-01-01 6 ## 4 bob 2019-01-02 6 ## 5 joe 2019-01-02 5 ## 6 bob 2019-01-03 4 ## 7 bob 2019-01-04 5 ## 8 joe 2019-01-04 6 ## 9 sam 2019-01-04 7 We want to create an additional column called “day” and use integers rather than dates to make plotting easier/prettier. To do so, we need to create a new data frame of unique dates and unique days, and then we need to merge that new data fram with the original to align the new “day” integer values.\nTurn the dates into a character vector so that they are easier to work with.\ndf$date \u0026lt;- as.character(df$date) Now give each unique date a respective integer “day” value in a new data frame.\nuniq_dates \u0026lt;- unique(df$date) day_integers \u0026lt;- data.frame( \u0026#39;date\u0026#39; = c(uniq_dates), \u0026#39;day\u0026#39; = c(1:length(uniq_dates)) ) day_integers$date \u0026lt;- as.character(day_integers$date) Finally, merge the new day_integers data frame with the original so that we have easy numbers for plotting.\nplot_df \u0026lt;- left_join(df, day_integers) plot_df ## id date stress day ## 1 bob 2019-01-01 4 1 ## 2 joe 2019-01-01 5 1 ## 3 sam 2019-01-01 6 1 ## 4 bob 2019-01-02 6 2 ## 5 joe 2019-01-02 5 2 ## 6 bob 2019-01-03 4 3 ## 7 bob 2019-01-04 5 4 ## 8 joe 2019-01-04 6 4 ## 9 sam 2019-01-04 7 4 One additional note. It can be instructive to see the inefficient way to get the same result using a for-loop. Here is un-evaluated code that is the for-loop equivalent to above.\n# take unique date # which rows match # plug in counter to those values # increase counter by 1 time_vec \u0026lt;- numeric(nrow(original_df)) unique_dates \u0026lt;- unique(original_df$date) counter \u0026lt;- 0 for(i in 1:length(unique_dates)){ # take unique date datey \u0026lt;- unique_dates[i] # which rows match this date? use_rows \u0026lt;- which(original_df$date == datey) # increase counter counter \u0026lt;- counter + 1 # plug in counter in time vec time_vec[use_rows] \u0026lt;- counter } original_df$day \u0026lt;- time_vec Bo\\(^2\\)m =)\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"1650ec85e015d68313be81c9841beb79","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-15/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/computational_notes/2019-01-15/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Turning Unequal Dates into Days","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" There are two code variations I use to generate time indexes. If I need time cycles\n## id score time ## 1 a 24.81885 1 ## 2 b 18.46513 2 ## 3 c 15.95025 3 ## 4 a 24.79416 1 ## 5 b 18.14886 2 ## 6 c 18.99802 3 ## 7 a 14.98879 1 ## 8 b 23.75162 2 ## 9 c 12.85643 3 then I use a sequence command.\ntime \u0026lt;- seq(from = 1, to = 3, each = 3) If I need time ordered\n## id score time ## 1 a 26.85878 1 ## 2 a 26.10859 1 ## 3 a 20.01426 1 ## 4 b 22.19061 2 ## 5 b 17.04311 2 ## 6 b 19.55141 2 ## 7 c 16.98356 3 ## 8 c 24.42635 3 ## 9 c 23.62059 3 then I use a replicate command.\ntime \u0026lt;- rep(c(1:3), each = 3) Bo\\(^2\\)m =)\n","date":1547424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547424000,"objectID":"0f70e1a6bdcef9b9ad4ae371ff9bc3b6","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-14/","publishdate":"2019-01-14T00:00:00Z","relpermalink":"/computational_notes/2019-01-14/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Generating Time in a Data Frame","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Sometimes I store every result in my initialized vector/matrix.\nHere is the data.\n## people values day ## 1 john 10.215066 1 ## 2 teddy 10.270607 1 ## 3 clare 10.216560 1 ## 4 john 9.712433 2 ## 5 teddy 8.988547 2 ## 6 john 9.441930 3 ## 7 teddy 10.482020 3 ## 8 clare 8.922451 3 Now the code. I want to find the days where I have responses from John, Teddy, and Clare (as you can tell, I only have responses from all three of them on days 1 and 3).\nuse_days \u0026lt;- numeric(length(unique(df$days))) # initialized vector counter \u0026lt;- 0 select_days \u0026lt;- c(1, 2, 3) for(i in 1:length(select_days)){ counter \u0026lt;- counter + 1 # select the i-th day filter_data \u0026lt;- df %\u0026gt;% filter(day == select_days[i]) # are there three responses on this day? if(length(filter_data$day) == 3){ use_days[counter] \u0026lt;- filter_data$day } } use_days ## [1] 1 NA 3 That code works, but what if I don’t want to store that NA during the second iteration? To only store successful output, put the counter in the “if statement.”\nuse_days \u0026lt;- numeric(length(unique(df$days))) # initialized vector counter \u0026lt;- 0 select_days \u0026lt;- c(1, 2, 3) for(i in 1:length(select_days)){ # select the i-th day filter_data \u0026lt;- df %\u0026gt;% filter(day == select_days[i]) # are there three responses on this day? if(length(filter_data$day) == 3){ counter \u0026lt;- counter + 1 # HERE IS THE CHANGE use_days[counter] \u0026lt;- filter_data$day } } use_days ## [1] 1 3 Bo\\(^2\\)m =)\n","date":1547337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547337600,"objectID":"5334b3136d7af9c256fb67355529d3d9","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-13/","publishdate":"2019-01-13T00:00:00Z","relpermalink":"/computational_notes/2019-01-13/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Only Store Successful Output - A Counter Placement Issue","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" No explanation for this set of notes, just a few reminders when spreading and gathering.\n## b_partial b_wo_partial se_partial se_wo_partial ## 1 1 4 6 3 ## 2 2 5 7 2 ## 3 3 6 8 1 We want the columns to be “model,” “result,” and “value.”\nHere is my incorrect attempt.\ncd_try \u0026lt;- cd_try %\u0026gt;% gather(b_partial, b_wo_partial, key = \u0026#39;model\u0026#39;, value = \u0026#39;b1\u0026#39;) cd_try ## se_partial se_wo_partial model b1 ## 1 6 3 b_partial 1 ## 2 7 2 b_partial 2 ## 3 8 1 b_partial 3 ## 4 6 3 b_wo_partial 4 ## 5 7 2 b_wo_partial 5 ## 6 8 1 b_wo_partial 6 cd_try \u0026lt;- cd_try %\u0026gt;% gather(se_partial, se_wo_partial, key = \u0026#39;se_model\u0026#39;, value = \u0026#39;sd\u0026#39;) cd_try # not evaluated because it won\u0026#39;t work Instead, I need to gather everything in at the same time, split, and then spread.\n## b_partial b_wo_partial se_partial se_wo_partial ## 1 1 4 6 3 ## 2 2 5 7 2 ## 3 3 6 8 1 Gather\ncd_try \u0026lt;- cd_try %\u0026gt;% gather(b_partial, b_wo_partial, se_partial, se_wo_partial, key = \u0026#39;result_model\u0026#39;, value = \u0026#39;value\u0026#39;) # gather everything cd_try ## result_model value ## 1 b_partial 1 ## 2 b_partial 2 ## 3 b_partial 3 ## 4 b_wo_partial 4 ## 5 b_wo_partial 5 ## 6 b_wo_partial 6 ## 7 se_partial 6 ## 8 se_partial 7 ## 9 se_partial 8 ## 10 se_wo_partial 3 ## 11 se_wo_partial 2 ## 12 se_wo_partial 1 Split\ncd_try \u0026lt;- cd_try %\u0026gt;% separate(result_model, into = c(\u0026#39;result\u0026#39;, \u0026#39;model\u0026#39;), sep = \u0026quot;_\u0026quot;) cd_try ## result model value ## 1 b partial 1 ## 2 b partial 2 ## 3 b partial 3 ## 4 b wo 4 ## 5 b wo 5 ## 6 b wo 6 ## 7 se partial 6 ## 8 se partial 7 ## 9 se partial 8 ## 10 se wo 3 ## 11 se wo 2 ## 12 se wo 1 Spread, BUT WHEN YOU SPREAD MAKE SURE TO INCLUDE ROW IDENTIFIERS.\ncd_try \u0026lt;- cd_try %\u0026gt;% mutate(row_help = rep(1:6, 2)) cd_try \u0026lt;- cd_try %\u0026gt;% spread(result, value) cd_try ## model row_help b se ## 1 partial 1 1 6 ## 2 partial 2 2 7 ## 3 partial 3 3 8 ## 4 wo 4 4 3 ## 5 wo 5 5 2 ## 6 wo 6 6 1 Bo\\(^2\\)m =)\n","date":1547164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547164800,"objectID":"f51d45a0bd06cd828826cbb1b4e20ebe","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-11/","publishdate":"2019-01-11T00:00:00Z","relpermalink":"/computational_notes/2019-01-11/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Row Labels Needed to Spread","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Longitudinal data sets often have hidden NAs when they are in long-form. For example, in the data set below Zoe is missing on days 2 and 4, but it isn’t obvious because there are no specific “NA’s” within the data.\n## time id q1 q2 ## 1 1 Jac 4 3 ## 2 1 Jess 5 2 ## 3 1 Zoe 3 4 ## 4 2 Jac 6 1 ## 5 2 Jess 7 2 ## 6 3 Jac 5 3 ## 7 3 Jess 4 4 ## 8 3 Zoe 3 2 ## 9 4 Jac 4 3 ## 10 4 Jess 5 4 Usually I recommend cleaning within the tidyverse package, but in this case I prefer reshape. Change the data frame to wide\nlibrary(reshape2) wide_cd \u0026lt;- reshape(cd, timevar = \u0026#39;time\u0026#39;, idvar = \u0026#39;id\u0026#39;, direction = \u0026#39;wide\u0026#39;) and then back to long to reveal the hidden NA’s.\ncd_reveal \u0026lt;- reshape(wide_cd, timevar = \u0026#39;time\u0026#39;, idvar = \u0026#39;id\u0026#39;, direction = \u0026#39;long\u0026#39;) cd_reveal ## id time q1.1 q2.1 ## Jac.1 Jac 1 4 3 ## Jess.1 Jess 1 5 2 ## Zoe.1 Zoe 1 3 4 ## Jac.2 Jac 2 6 1 ## Jess.2 Jess 2 7 2 ## Zoe.2 Zoe 2 NA NA ## Jac.3 Jac 3 5 3 ## Jess.3 Jess 3 4 4 ## Zoe.3 Zoe 3 3 2 ## Jac.4 Jac 4 4 3 ## Jess.4 Jess 4 5 4 ## Zoe.4 Zoe 4 NA NA It is possible to do all of this within tidyverse, but it’s tricky because the spread command only applies to one column (the value parameter only takes one entry), so anytime your data frame contains multiple columns to spread over (almost always the case) then spread does not work well.\nlibrary(tidyverse) cd %\u0026gt;% spread(key = time, value = q1) ## id q2 1 2 3 4 ## 1 Jac 1 NA 6 NA NA ## 2 Jac 3 4 NA 5 4 ## 3 Jess 2 5 7 NA NA ## 4 Jess 4 NA NA 4 5 ## 5 Zoe 2 NA NA 3 NA ## 6 Zoe 4 3 NA NA NA Notice how it only used q1. The proper way to go from long to wide and then back to long to reveal the NA’s using tidyverse is either of the following:\ncd %\u0026gt;% select(time, id, q1) %\u0026gt;% spread(key = time, value = q1) %\u0026gt;% gather(key = time, value = \u0026#39;q1\u0026#39;, \u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;) # string code needed ## id time q1 ## 1 Jac 1 4 ## 2 Jess 1 5 ## 3 Zoe 1 3 ## 4 Jac 2 6 ## 5 Jess 2 7 ## 6 Zoe 2 NA ## 7 Jac 3 5 ## 8 Jess 3 4 ## 9 Zoe 3 3 ## 10 Jac 4 4 ## 11 Jess 4 5 ## 12 Zoe 4 NA time_string \u0026lt;- as.character(unique(cd$time)) cd %\u0026gt;% select(time, id, q1) %\u0026gt;% spread(key = time, value = q1) %\u0026gt;% gather(key = time, value = \u0026#39;q1\u0026#39;, time_string) # string code not needed due to pre-allocation ## id time q1 ## 1 Jac 1 4 ## 2 Jess 1 5 ## 3 Zoe 1 3 ## 4 Jac 2 6 ## 5 Jess 2 7 ## 6 Zoe 2 NA ## 7 Jac 3 5 ## 8 Jess 3 4 ## 9 Zoe 3 3 ## 10 Jac 4 4 ## 11 Jess 4 5 ## 12 Zoe 4 NA Again, I prefer reshape because the spread commands in tidyverse are not easy to read.\nBo\\(^2\\)m =)\n","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"0a2c96765a345af4f3ac07645f273cb2","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-10/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/computational_notes/2019-01-10/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Reveal Hidden NA's in Longitudinal Data","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A replication of Patricia Cohen’s wonderful, “problem of the premature covariate” (chapter 2 in Collins \u0026amp; Horn, 1991). Here is a simple version of the problem. Imagine that we want to know the influence of a life event, like meeting a friend, on happiness. We conduct a study where we measure people’s happiness at time one, wait two weeks, and then measure their happiness again along with whether or not they met a friend since we last observed them. To assess the relationship between meeting a friend and happiness, we regress post happiness on both pre happiness and whether or not they met a friend. That is, our regression takes the form:\n  Happy\\(_{post}\\) ~ Happy\\(_{pre}\\) + Met_Friend.\n  What Dr. Cohen draws attention to is that the coefficient relating “met friend” to happiness will be biased if (a) there is some non-zero probability of reverse causality and (b) we do not measure “met friend” exactly when it occurs. Remember, in our mock study we assessed both happiness and whether or not someone met a friend during our post-measure. Is that exactly when different people across the sample actually met a new friend? Perhaps, but most of our sample either did or did not meet a friend at some unknown time within the past two weeks; Dr. Cohen’s point is that this unknown is an issue to linger on.\nSimulation Explanation If Dr. Cohen’s issue is worthwhile, then, under a system where “met friend” truly does not influence happiness, we should be able to make “met friend” appear to influence happiness based on the points she raises. That is, when “met friend” does not influence happiness we should be able to make it appear so if we create a system where (1) happiness instead influences the probability of meeting a friend (reverse causality) and (2) we do not measure “met friend” exactly when it occurs. Below, we generate data where “met friend” does not influence happiness but the coefficient relating “met friend” to happiness will still be significant (and large) because of the issues raised.\nHere are the steps to the simulation:\nStart with a random value for happiness (distributed normally across 600 people) at time one.\n Happiness at \\(t+1\\) is its previous value plus one of the following, all with equal probability: +0.25, -0.25, or 0.\n At each time point, concurrent happiness influences the probability of meeting a friend. When happiness is low people are unlikely to meet a friend, whereas when happiness is high people are more likely to meet a friend. Meeting a friend is coded as 0 or 1 for each time point (i.e., no or yes).\n Continue for 25 time points.\n Assess the relationships between post happiness, pre happiness, and “met friend.” Pre happiness is always time one, whereas we will explore different post assesssments (e.g., post happiness is time 25 vs. post happiness is time 20). “Met friend” will always be whether the individual met a friend within 5 time points of the post happiness assessment. So, if we analyze post happiness as time 20, then “met friend” is whether the individual met a friend during times 15 through 20.\n  Notice that the simulation captures the notions raised above: “met friend” does not influence happiness, instead the reverse happens. And after making a decision about the timing of our pre and post assessment we lose information about when “met friend” actually happened. We know whether it happened but not when; we also don’t retain information on the differences in timing across our sample.\n Meeting a Friend or Not The most difficult aspect of the simulation is specifying step 3: “met friend” is some function of concurrent happiness. Dr. Cohen’s original explanation is, “the probability of \\(X\\) for each unit of time was determined by a Markov process, with probability increasing as a function of the level of contemporaneous \\(Y\\). Probabilities used increased from 0 for those with current \\(Y\\) less than -1.00 to 0.25 for those with current scores of 1.5 or greater” (she uses different variables for x and y in her discussion). What does that mean? How do we specify a Markov process where the probability of “met friend” is between 0 and 0.25 with respect to happiness cutoffs like -1.00 and 1.5? I don’t know either. But we can make it easier by recognizing that, at its core, the idea is simply, “meeting a friend is more likely when people are happier,” which we can represent with a simple linear equation like \\(y = mx + b\\). All we need to do is to find the slope and y-intercept, then we’ll have an equation where we can plug in “happiness” and get “probability of meeting a friend.” Here is how.\nRemember that we can find the slope and y-intercept of a line if we know the location of two of its points. Here, we know that the probability of “met friend” needs to be between 0 and 0.25, and the happiness cutoffs need to be -1.00 and 1.5. If I want to relate happiness to “met friend,” then, I can put happiness on the x-axis and “met friend” on the y-axis and recognize that by combining these cutoffs I get the end-points of a line: (1.5, 0.25) is one point and (-1, 1.5) is the other. Computing rise-over-run and then solving for the intercept gives me the following:\n  Probability of meeting a friend = 0.1*Happy + 0.01\n  Now we have a way to compute the probability of meeting a friend based on happiness. It is not as precise as the Markov process but it will work just fine. (Note: I actuallly use the points (1.4999, 2.4999) and (-0.999, 1.4999) to calculate the slope and intercept in the simulation because I will also use if-statements for the cutoffs)\n Simulate One Person It’s always helpful to make sure we can get a simulation to work on one person. In the simulation below, \\(y\\) is happiness and \\(x\\) is “met friend.”\ntime \u0026lt;- 25 y \u0026lt;- numeric(time) x \u0026lt;- numeric(time) count \u0026lt;- 0 for(i in 1:time){ count \u0026lt;- count + 1 if(i == 1){ y[count] \u0026lt;- rnorm(1, mean = 0.5, sd = 0.5) x[count] \u0026lt;- 0 }else{ # y up or down with autoregression updownsame \u0026lt;- sample(c(\u0026#39;up\u0026#39;, \u0026#39;down\u0026#39;, \u0026#39;same\u0026#39;), 1) if(updownsame == \u0026#39;up\u0026#39;){ y[count] \u0026lt;- y[count - 1] + 0.25 }else if(updownsame == \u0026#39;down\u0026#39;){ y[count] \u0026lt;- y[count - 1] - 0.25 }else{ y[count] \u0026lt;- y[count - 1] } # x is a function of y if(y[count] \u0026lt;= -1.00){ x_prob \u0026lt;- 0 }else if(y[count] \u0026gt;= 1.5){ x_prob \u0026lt;- 0.25 }else{ x_prob \u0026lt;- 0.10004*y[count] + 0.09994 } x[count] \u0026lt;- rbinom(1, 1, x_prob) } }  Full Sample That script worked, so now let’s update the code slightly and run it across 600 people.\npeople \u0026lt;- 600 time \u0026lt;- 25 df \u0026lt;- matrix(, nrow = people*time, ncol = 4) count \u0026lt;- 0 for(j in 1:people){ for(i in 1:time){ count \u0026lt;- count + 1 if(i == 1){ df[count, 1] \u0026lt;- j df[count, 2] \u0026lt;- i df[count, 3] \u0026lt;- rnorm(1, mean = 0.5, sd = 0.5) df[count, 4] \u0026lt;- 0 }else{ df[count, 1] \u0026lt;- j df[count, 2] \u0026lt;- i # y up or down with autoregression updownsame \u0026lt;- sample(c(\u0026#39;up\u0026#39;, \u0026#39;down\u0026#39;, \u0026#39;same\u0026#39;), 1) if(updownsame == \u0026#39;up\u0026#39;){ df[count, 3] \u0026lt;- df[count - 1, 3] + 0.25 }else if(updownsame == \u0026#39;down\u0026#39;){ df[count, 3] \u0026lt;- df[count - 1, 3] - 0.25 }else{ df[count, 3] \u0026lt;- df[count - 1, 3] } # x is a function of y if(df[count, 3] \u0026lt;= -1.00){ x_prob \u0026lt;- 0 }else if(df[count, 3] \u0026gt;= 1.5){ x_prob \u0026lt;- 0.25 }else{ x_prob \u0026lt;- 0.10004*df[count, 3] + 0.09994 } df[count, 4] \u0026lt;- rbinom(1, 1, x_prob) } } } df \u0026lt;- data.frame(df) names(df) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;happy\u0026#39;, \u0026#39;met_friend\u0026#39;) library(tidyverse) Results Remember, we generated data where “met friend” did not influence happiness. Now we are going to assess the coefficient relating “met friend” to happiness to see if it differs from zero. First, let’s say our post-assessment happened at time 10.\nTrim down our data set to just that time frame.\nhappy10_sample \u0026lt;- df %\u0026gt;% filter(time \u0026lt; 11) How many friends did each person meet between times 5 and 10?\nfriend_count \u0026lt;- happy10_sample %\u0026gt;% filter(time \u0026gt; 4) %\u0026gt;% group_by(id) %\u0026gt;% summarise( friend_count = sum(met_friend) ) ## `summarise()` ungrouping output (override with `.groups` argument) Now change that to:\n 0 = did not happen 1 = happened at least once (meaning sum of friend_count is not equal to 0)  friend_count \u0026lt;- friend_count %\u0026gt;% mutate(friend_event = case_when( friend_count == 0 ~ 0, friend_count != 0 ~ 1 )) Merge that count back into the happy10 data set and prepare the data for regression.\n# Merge back into y10 df happy10_sample \u0026lt;- left_join(happy10_sample, friend_count) # Filter down to what\u0026#39;s needed for regression happy10_filter \u0026lt;- happy10_sample %\u0026gt;% select(id, time, happy, friend_event) %\u0026gt;% filter(time == 1 | time == 10) library(reshape2) happy10_wide \u0026lt;- reshape(happy10_filter, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) # The x columns are synonymous, so I can remove one happy10_wide \u0026lt;- happy10_wide[, c(\u0026#39;id\u0026#39;, \u0026#39;happy.10\u0026#39;, \u0026#39;happy.1\u0026#39;, \u0026#39;friend_event.1\u0026#39;)] names(happy10_wide) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;happy_post\u0026#39;, \u0026#39;happy_pre\u0026#39;, \u0026#39;met_friend\u0026#39;) Now regress post happy on pre happy and whether or not they met a friend between times 5 and 10.\nsummary(lm(happy_post ~ happy_pre + met_friend, data = happy10_wide))$coefficients ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.1050699 0.04102004 -2.561429 1.066877e-02 ## happy_pre 0.9337432 0.04815831 19.389036 2.537842e-65 ## met_friend 0.2526980 0.04992485 5.061568 5.545673e-07 The coefficient relating “met friend” to happiness is about 0.3 and it is significant (remember there was no influence from “met friend” to happiness).\nWhat about when we change the post assessment to time point 15?\nFirst create a function out of all the “tidying” steps above:\ndf_create \u0026lt;- function(time1){ library(reshape2) library(tidyverse) time2 \u0026lt;- time1 - 5 y_sample \u0026lt;- df %\u0026gt;% filter(time \u0026lt;= time1) friend_count \u0026lt;- y_sample %\u0026gt;% filter(time \u0026gt;= time2) %\u0026gt;% group_by(id) %\u0026gt;% summarise( friend_count = sum(met_friend) ) friend_count \u0026lt;- friend_count %\u0026gt;% mutate(friend_event = case_when( friend_count == 0 ~ 0, friend_count != 0 ~ 1 )) y_sample \u0026lt;- left_join(y_sample, friend_count) y_filter \u0026lt;- y_sample %\u0026gt;% select(id, time, happy, friend_event) %\u0026gt;% filter(time == 1 | time == time1) y_wide \u0026lt;- reshape(y_filter, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) yname \u0026lt;- paste(\u0026#39;happy.\u0026#39;, time1, sep = \u0026#39;\u0026#39;) y_wide \u0026lt;- y_wide[, c(\u0026#39;id\u0026#39;, yname, \u0026#39;happy.1\u0026#39;, \u0026#39;friend_event.1\u0026#39;)] names(y_wide) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;happy_post\u0026#39;, \u0026#39;happy_pre\u0026#39;, \u0026#39;met_friend\u0026#39;) return(y_wide) } Here are the results:\nhappy15_wide \u0026lt;- df_create(15) summary(lm(happy_post ~ happy_pre + met_friend, data = happy15_wide))$coefficients ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.2154859 0.04700472 -4.584346 5.551550e-06 ## happy_pre 0.8440452 0.05624981 15.005297 2.076223e-43 ## met_friend 0.5000608 0.05837959 8.565678 9.187047e-17 What about when we select time 25 as our post assessment?\nhappy25_wide \u0026lt;- df_create(25) summary(lm(happy_post ~ happy_pre + met_friend, data = happy25_wide))$coefficients ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.3849218 0.06220349 -6.188106 1.132745e-09 ## happy_pre 0.8216318 0.07466252 11.004608 9.061944e-26 ## met_friend 0.8663628 0.07697388 11.255282 8.833209e-27 Notice how large the coefficient relating “met friend” to happiness is here: close to 0.9 – remember, there truly is no effect relating “met friend” to happiness.\n  Conclusion If there is some probability of reverse causality and we don’t measure the event exactly when it occurs then the estimate relating that event to our outcome will be biased. If many “event opportunities” occur between our pre and post measure then our estimate will be extremely biased.\nBo\\(^2\\)m = )\n ","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"45129fae9cc4cfb29c39e6026cd22f6e","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-08/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/computational_notes/2019-01-08/","section":"Computational_Notes","summary":"-----","tags":null,"title":"The Premature Covariate","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Another example of using column names as parameters with quo, this time within ggplot2. A snippet of the data:\n## day id stress performance ## 1 1 Josh 8 17 ## 2 2 Josh 5 6 ## 3 3 Josh 7 15 ## 4 4 Josh 7 10 ## 5 5 Josh 4 7 ## 6 6 Josh 7 13 Let’s say we want to plot each person’s stress over time: three time-series trajectories.\nlibrary(tidyverse) library(ggplot2) ggplot(df, aes(x = day, y = stress, color = id)) + geom_point() + geom_line() Great, but imagine having a data set with 300 different DVs. Instead of re-calling ggplot each time we can create a function where the column (DV) is the paramter.\nplot_it \u0026lt;- function(col_name){ g \u0026lt;- ggplot(df, aes(x = day, y = !!col_name, color = id)) + geom_point() + geom_line() return(g) } Note the !! before the parameter. Now, to plot the new graph we use quo within the function call.\nplot_it(quo(performance)) Bo\\(^2\\)m =)\n","date":1546732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546732800,"objectID":"de0917e674107f1be218348f5134b28a","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-06/","publishdate":"2019-01-06T00:00:00Z","relpermalink":"/computational_notes/2019-01-06/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Column Names As Parameters with GGplot2","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Creating item totals with a data set containing NAs is surprisingly difficult. Here is the data.\nlibrary(tidyverse) cd \u0026lt;- data.frame( \u0026quot;q1\u0026quot; = c(1,2,NA), \u0026quot;q2\u0026quot; = c(2,2,2), \u0026#39;q3\u0026#39; = c(NA, NA,2), \u0026#39;id\u0026#39; = c(\u0026#39;201\u0026#39;, \u0026#39;202\u0026#39;, \u0026#39;203\u0026#39;) ) cd ## q1 q2 q3 id ## 1 1 2 NA 201 ## 2 2 2 NA 202 ## 3 NA 2 2 203 Mutating directly over columns with NA does not work.\ncd %\u0026gt;% mutate(cohesion = q1 + q2 + q3) ## q1 q2 q3 id cohesion ## 1 1 2 NA 201 NA ## 2 2 2 NA 202 NA ## 3 NA 2 2 203 NA Filtering removes the data we are interested in.\ncd %\u0026gt;% filter(!is.na(q1) == T \u0026amp;\u0026amp; !is.na(q2) == T \u0026amp;\u0026amp; !is.na(q3) == T) ## [1] q1 q2 q3 id ## \u0026lt;0 rows\u0026gt; (or 0-length row.names) We cannot use rowMeans in combination with mutate because the two are not compatible. The code below is not evaluated, but if you run it it does not work.\ncd %\u0026gt;% mutate(cohesion = rowMeans(q1, q2, q3, na.rm = T)) Using the rowwise command within a pipe gets us close\ncd %\u0026gt;% rowwise() %\u0026gt;% mutate(mean = mean(q1, q2, q3, na.rm = T)) ## # A tibble: 3 x 5 ## # Rowwise: ## q1 q2 q3 id mean ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2 NA 201 1 ## 2 2 2 NA 202 2 ## 3 NA 2 2 203 NaN but the mean value is not calculated correctly. We need to include c() to vectorize the items.\ncd %\u0026gt;% rowwise() %\u0026gt;% mutate(mean = mean(c(q1, q2, q3), na.rm = T)) ## # A tibble: 3 x 5 ## # Rowwise: ## q1 q2 q3 id mean ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 2 NA 201 1.5 ## 2 2 2 NA 202 2 ## 3 NA 2 2 203 2 Finally the right answer. Use rowwise in combination with a vectorized mutate.\nBo\\(^2\\)m =)\n","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546646400,"objectID":"697b3e015daaf8191a75ebf0416a0c5c","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-05/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/computational_notes/2019-01-05/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Mutating Scale Items with NA","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Purpose Imagine that you are interested in the relationship between stress and performance. To assess it, you observe 600 people at work and measure their stress via a self-report (e.g., “I feel stressed”) and their performance via objective performance scores for the day (e.g., number of sales). You regress performance on stress and find that the estimated coefficient relating to two is 0.45. You then build a 95% confidence interval using the standard error that the analysis spit out and find that the CI is 0.45 +- 0.1.\nWhat does that confidence interval actually mean? The purpose of this exercise is to build intuition behind frequentist CIs.\n Steps Generate the population\n Sample the population. On that sample…\n2a) Regress performance on stress\n2b) Calculate a CI\n2c) Does the CI contain the population parameter?\n Re-sample and repeat\n   1) Generate the population Our population will contain 100,000 people\npop_number \u0026lt;- 100000 with stress scores distributed about zero. The scale here doesn’t matter – we care about the relationship between stress and performance and less about (in this example) the distributions of stress and performance themselves.\npopulation_stress \u0026lt;- rnorm(pop_number, 0, 5) The true relationship between stress and performance will be 0.45. Let’s set that parameter\nstress_performance_coefficient \u0026lt;- 0.45 and then generate performance.\npopulation_performance \u0026lt;- stress_performance_coefficient*population_stress + rnorm(pop_number, 0, 1) Now plug everything into a data set. Remember, this is the population.\ndf \u0026lt;- data.frame( \u0026#39;person\u0026#39; = c(1:pop_number), \u0026#39;stress\u0026#39; = c(population_stress), \u0026#39;performance\u0026#39; = c(population_performance) ) What is the paramter relating stress to performance? 0.45, keep that in mind. Time to sample the population as if we conducted a study and run our regression.\n 2) Sample the population Randomly select 600 people from our population. That is, pretend we ran a study on 600 subjects.\nsample_size \u0026lt;- 600 random_numbers \u0026lt;- sample(c(1:pop_number), sample_size) sample_df \u0026lt;- df %\u0026gt;% filter(person %in% random_numbers)  2a) Regress Performance on Stress Use the lm command for regression in R.\nsummary(lm(performance ~ stress, data = sample_df)) ## ## Call: ## lm(formula = performance ~ stress, data = sample_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.70904 -0.70353 0.01858 0.64438 3.07162 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.009443 0.041027 0.23 0.818 ## stress 0.432981 0.008139 53.20 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.004 on 598 degrees of freedom ## Multiple R-squared: 0.8256, Adjusted R-squared: 0.8253 ## F-statistic: 2830 on 1 and 598 DF, p-value: \u0026lt; 2.2e-16  2b) Compute the CI Save the output of the regression in an object so we can pull out the specific coefficients that we are interested in.\noutput \u0026lt;- summary(lm(performance ~ stress, data = sample_df)) Pull out the coefficient relating stress to performance\nslope_coefficient \u0026lt;- output$coefficients[2,1] and use it along with the SEs to calculate the confidence interval.\nse_upper \u0026lt;- slope_coefficient + 1.96*output$coefficients[2,2] se_lower \u0026lt;- slope_coefficient - 1.96*output$coefficients[2,2]  2c) Does the CI contain the population parameter? Remember that the parameter is 0.45.\ncontain_parameter \u0026lt;- NULL if(se_lower \u0026lt;= stress_performance_coefficient \u0026amp;\u0026amp; se_upper \u0026gt;= stress_performance_coefficient){ contain_parameter \u0026lt;- \u0026#39;yes\u0026#39; }else{ contain_parameter \u0026lt;- \u0026#39;no\u0026#39; } contain_parameter ## [1] \u0026quot;no\u0026quot; What did we do? We sampled the population, ran a regression to relate stress to performance, and then calculated a CI on the slope term. The interpretation of a CI, however, is across infinite samples. Now we need to run through the sample, regress, and calculate CI procedure again and again and again – Monte Carlo.\n 3) Re sample and repeat I created a function that samples the population, runs a regression, calculates the CI, and then saves whether or not the interval contained 0.45 (‘yes’ or ‘no’). You can view that code in the raw rmarkdown file. For now, just know that the function is called sample_regress_calc_ci.\nWe are going to re-run step 2 from above 900 times\nsims \u0026lt;- 900 and store the ‘yes’ or ‘no’ result in a vector.\nall_ci_contains \u0026lt;- numeric(sims) Here is the full Monte Carlo code.\nsims \u0026lt;- 900 all_ci_contains \u0026lt;- numeric(sims) for(i in 1:sims){ result \u0026lt;- sample_regress_calc_ci() all_ci_contains[i] \u0026lt;- result }  Interpretation How many computed intervals contain the population parameter?\nsum(all_ci_contains == \u0026#39;yes\u0026#39;) / sims ## [1] 0.9466667  ","date":1546560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546560000,"objectID":"10b020a8d815b896f6837cebf90ee643","permalink":"https://christopherdishop.netlify.app/computational_notes/2019-01-04/","publishdate":"2019-01-04T00:00:00Z","relpermalink":"/computational_notes/2019-01-04/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Frequentist Confidence Intervals","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://christopherdishop.netlify.app/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://christopherdishop.netlify.app/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"https://christopherdishop.netlify.app/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://christopherdishop.netlify.app/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":" Use quo or enquo when you want to include column names as parameters in a function. For example, a function like the following would not work:\nbad_function \u0026lt;- function(data, col_name){ newdf \u0026lt;- data %\u0026gt;% mutate(\u0026#39;adjusted_column\u0026#39; = col_name + 1) return(newdf) } bad_function(df, column_i_care_about) because column_i_care_about isn’t specified in a form that mutate can work with.\nExamples The data are contained in df1.\ndf1 \u0026lt;- data.frame( a = c(1,2,NA), b = c(NA,3,4) ) df1 ## a b ## 1 1 NA ## 2 2 3 ## 3 NA 4 The function: take the column specified by the parameter and add one to every value. Then return the new data frame.\nadder \u0026lt;- function(col_use){ newdf \u0026lt;- df1 %\u0026gt;% mutate(\u0026#39;adder\u0026#39; = (!!col_use) + 1) # correct form here using !! return(newdf) } adder(quo(a)) # correct form here using quo ## a b adder ## 1 1 NA 2 ## 2 2 3 3 ## 3 NA 4 NA A more complicated function by incorporating is.na.\nna_tagger \u0026lt;- function(col_use){ newdf \u0026lt;- df1 %\u0026gt;% mutate(\u0026#39;na_tag\u0026#39; = ifelse(is.na((!!col_use)) == T, 1, 0)) return(newdf) } na_tagger(quo(a)) ## a b na_tag ## 1 1 NA 0 ## 2 2 3 0 ## 3 NA 4 1 In the examples above I used quo interactively. You get the same result by instead using enquo within the function.\nadder2 \u0026lt;- function(col_use){ col_use \u0026lt;- enquo(col_use) newdf \u0026lt;- df1 %\u0026gt;% mutate(\u0026#39;adder\u0026#39; = (!!col_use) + 1) return(newdf) } adder2(a) ## a b adder ## 1 1 NA 2 ## 2 2 3 3 ## 3 NA 4 NA  One More Note Sometimes I also need to specify the data set and column within a dplyr command and then use the parameter to select a specific row. The following format seems to work well: data[['col_name']][row]. Here is a function that is inefficient but demonstrates the point well:\nselector2 \u0026lt;- function(x, y){ new \u0026lt;- df1 %\u0026gt;% filter(robby == df1[[\u0026#39;robby\u0026#39;]][x]) %\u0026gt;% filter(ruddy == df1[[\u0026#39;ruddy\u0026#39;]][y]) return(new) } Bo\\(^2\\)m =)\n ","date":1536192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536192000,"objectID":"eb927d6553348b3b9901c77b0db556d6","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-09-06/","publishdate":"2018-09-06T00:00:00Z","relpermalink":"/computational_notes/2018-09-06/","section":"Computational_Notes","summary":"-----","tags":null,"title":"More on Column Names as Parameters","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Monte Carlo helps us understand processes that we can describe but don’t yet have analytic solutions for. Here are two examples: the birthday problem and the tasting tea problem.\nBirthday Problem If you are standing in a room with 25 other people, what is the probability that at least two people share the same birthday? This question has a mathematical solution, but if we don’t know it we can use Monte Carlo to help.\nSelect 25 people with random birthdays\ngroup_birthdays \u0026lt;- sample(1:365, 15, replace = TRUE) and then check whether two of them share a birthday.\nshared_birthday \u0026lt;- length(group_birthdays[duplicated(group_birthdays)]) # Returns 1 if yes and 0 if no Now place everything into a loop and evaluate 5000 times for the final Monte Carlo:\ngroup_size \u0026lt;- 15 iterations \u0026lt;- 5000 shared_birthdays_counter \u0026lt;- 0 for(i in 1:iterations){ group_birthdays \u0026lt;- sample(1:365, 15, replace = TRUE) shared_birthday \u0026lt;- length(group_birthdays[duplicated(group_birthdays)]) if(shared_birthday == 1){ shared_birthdays_counter \u0026lt;- shared_birthdays_counter + 1 } } The probability of a shared birthday among a group of 15 is…\nshared_birthdays_counter / iterations ## [1] 0.2278 The probability of a shared birthday as we increase group size…\nsizes \u0026lt;- 2:25 prob_store \u0026lt;- numeric(length(sizes)) for(j in 1:24){ group_size \u0026lt;- j iterations \u0026lt;- 5000 shared_birthdays_counter \u0026lt;- 0 for(i in 1:iterations){ group_birthdays \u0026lt;- sample(1:365, group_size, replace = TRUE) shared_birthday \u0026lt;- length(group_birthdays[duplicated(group_birthdays)]) if(shared_birthday == 1){ shared_birthdays_counter \u0026lt;- shared_birthdays_counter + 1 } } prob_store[j] \u0026lt;- shared_birthdays_counter / iterations } df \u0026lt;- data.frame( \u0026#39;group_size\u0026#39; = c(2:25), \u0026#39;probability\u0026#39; = c(prob_store) ) library(ggplot2) plot1 \u0026lt;- ggplot(df, aes(x = group_size, y = probability)) + geom_bar(stat = \u0026#39;identity\u0026#39;, color = \u0026#39;orange\u0026#39;) plot1 The equation to solve the birthday problem is\n\\[\\begin{equation} n! / (n - k)! \\end{equation}\\]\nwhere \\(n\\) is the number of possible birthdays and \\(k\\) is group size. The beauty of Monte Carlo is that we didn’t need the above equation to learn about our shared birthday process.\n Tasting Tea Imagine that I make one cup of tea with milk and then ask you the following: did I pour the tea or milk first? I repeat this for eight cups of tea. What is the probability that you guess correctly for 3 of the cups? For all 8 cups?\nFirst, we generate truth. For each cup, ‘M’ means I poured milk first and ‘T’ means I poured tea first.\npossible_pours \u0026lt;- c(rep(\u0026#39;M\u0026#39;, 4), rep(\u0026#39;T\u0026#39;, 4)) true_pours \u0026lt;- sample(possible_pours, size = 8) # The true first pours true_pours ## [1] \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;T\u0026quot; \u0026quot;T\u0026quot; \u0026quot;M\u0026quot; \u0026quot;T\u0026quot; \u0026quot;T\u0026quot; \u0026quot;M\u0026quot; Then you make a guess for each cup.\nguess \u0026lt;- c(\u0026#39;M\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;) In this case, you guessed that I poured milk first for cup 1 and tea first for cup 2. How many of your guesses are correct?\ncorrect \u0026lt;- sum(true_pours == guess) correct ## [1] 4 Now we can put all of that into a Monte Carlo loop.\niterations \u0026lt;- 5000 correct_store \u0026lt;- numeric(iterations) for(i in 1:iterations){ possible_pours \u0026lt;- c(rep(\u0026#39;M\u0026#39;, 4), rep(\u0026#39;T\u0026#39;, 4)) true_pours \u0026lt;- sample(possible_pours, size = 8) guess \u0026lt;- c(\u0026#39;M\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;) correct \u0026lt;- sum(true_pours == guess) correct_store[i] \u0026lt;- correct } What is the probability of you guessing correctly for 2 cups…6?\nprop.table(table(correct_store)) ## correct_store ## 0 2 4 6 8 ## 0.0144 0.2324 0.5022 0.2372 0.0138 Just like the birthday problem, there are equations that govern this “tea problem.” We don’t know what they are, but we can still learn about the process by using Monte Carlo approximation.\nThese examples can be found with greater discussion in Quantitative Social Science by Kosuke Imai.\nBo\\(^2\\)m =)\n ","date":1534032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534032000,"objectID":"35c95882242a81d6a10aecaac4f7420e","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-08-12/","publishdate":"2018-08-12T00:00:00Z","relpermalink":"/computational_notes/2018-08-12/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Monte Carlo Approximation","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A few random notes about plotting, describing, and thinking about trajectories.\nPlotting Trajectories Imagine we record “affect” (\\(Y\\)) for five people over 20 time points. ggplot2 produces poor longitudinal trajectories if you only specify time and affect as variables:\nlibrary(ggplot2) library(tidyverse) plot1 \u0026lt;- ggplot(df1, aes(x = time, y = affect)) + geom_point() + geom_line() plot1 Instead, specify “id” either as the grouping variable:\nplot2 \u0026lt;- ggplot(df1, aes(x = time, y = affect, group = id)) + geom_point() + geom_line() plot2 or a color.\nplot3 \u0026lt;- ggplot(df1, aes(x = time, y = affect, color = id)) + geom_point() + geom_line() plot3 If you have a data set with too many trajectories\nthen select a random sample to keep dark\ndf2_sample_ids \u0026lt;- sample(df2$id, 5) df2_sample \u0026lt;- df2 %\u0026gt;% filter(id %in% df2_sample_ids) and change the color of the background trajectories to a lighter color.\nplot5 \u0026lt;- ggplot(df2, aes(x = time, y = affect, group = id)) + geom_point(color = \u0026#39;gray85\u0026#39;) + geom_line(color = \u0026#39;gray85\u0026#39;) + # HERE COMES ADDITIONAL CHANGES geom_point(data = df2_sample, aes(x = time, y = affect, group = id)) + geom_line(data = df2_sample, aes(x = time, y = affect, group = id)) plot5 Notice that I had to evoke two additional geom commands and source my new data sample.\n Trajectory Descriptions Equilibrium Panel A: Increasing equilibrium level with constant variance.\nPanel B: Decreasing equilibrium level with constant variance.\nPanel C: Decreasing equilibrium level with increasing variance.\n Latent Growth Intercepts and Slopes Panel A: Between person differences in intercept but no differences in slope.\nPanel B: Between person differences in slope but no differences in intercept.\nPanel C: Between person differences in intercepts and slopes.\n Between and Within Person Variance Panel A: Between person differences in level (intercept in LGC literature) but no between person differences in variability.\nPanel B: No between person differences in level (intercept) or variability, but the amount of variability in these trajectories is greater than Panel A. Panel C: No between person differences in level (intercept) but there are between person differences in variability.\n  Main Effects and Interactions (Cross Sectional vs. Over Time) Imagine we re-test the main and interaction effects from a cross-sectional study several times. If the results are stable across time, what would they look like?\nMain Effect Group A (difficult, specific goals) higher performance than group B (vague goals).\n Interaction For males: Group A (difficult, specific goals) higher performance than group B (vague goals). For females: Group B (vague goals) higher performance than group B (difficult, specific goals).\n Interaction and Main Effect Bo\\(^2\\)m =)\n  ","date":1530662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530662400,"objectID":"865caa26cbc2340738c3c45e28b09168","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-07-04/","publishdate":"2018-07-04T00:00:00Z","relpermalink":"/computational_notes/2018-07-04/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Longitudinal Plotting","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://christopherdishop.netlify.app/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":null,"categories":null,"content":" I always forget how to use column names as function parameters, so here is an example.\nFunction with no column name parameters Function:\n Select columns\n Replace the Jimmy and James ‘v_1’ values with 99\n  library(tidyverse) dish \u0026lt;- data.frame( \u0026#39;person\u0026#39; = c(\u0026#39;jimmy\u0026#39;, \u0026#39;james\u0026#39;, \u0026#39;johnny\u0026#39;), \u0026#39;v_1\u0026#39; = c(rnorm(3, 0, 1)), \u0026#39;v_2\u0026#39; = c(rnorm(3, 10, 5)), \u0026#39;v_3\u0026#39; = c(rnorm(3, 50, 10)), \u0026#39;v_4\u0026#39; = c(rnorm(3, 25, 15)) ) mini \u0026lt;- dish %\u0026gt;% select(person, v_1, v_2) mini[mini$person == \u0026#39;jimmy\u0026#39;, 2] \u0026lt;- 99 mini[mini$person == \u0026#39;james\u0026#39;, 2] \u0026lt;- 99 The original data:\n## person v_1 v_2 v_3 v_4 ## 1 jimmy -0.43257483 11.567321 55.11768 18.90967 ## 2 james -0.02258931 5.346487 52.95132 19.09466 ## 3 johnny 0.30388097 9.959681 43.46922 36.63362 What we changed it to:\n## person v_1 v_2 ## 1 jimmy 99.000000 11.567321 ## 2 james 99.000000 5.346487 ## 3 johnny 0.303881 9.959681 Here is the function equivalent:\nimpute_99 \u0026lt;- function(data){ new_data \u0026lt;- data %\u0026gt;% select(person, v_1, v_2) new_data[new_data$person == \u0026#39;jimmy\u0026#39;, 2] \u0026lt;- 99 new_data[new_data$person == \u0026#39;james\u0026#39;, 2] \u0026lt;- 99 return(new_data) } Our result:\nadjusted_data \u0026lt;- impute_99(dish) adjusted_data ## person v_1 v_2 ## 1 jimmy 99.000000 11.567321 ## 2 james 99.000000 5.346487 ## 3 johnny 0.303881 9.959681  Function with column names as parameters Now, what if we want to use specific column names as parameters in our function? We could change the function to:\nimpute_99_column_specific \u0026lt;- function(data, column1, column2){ new_data \u0026lt;- data %\u0026gt;% select(person, column1, column2) new_data[new_data$person == \u0026#39;jimmy\u0026#39;, 2] \u0026lt;- 99 # column1 change new_data[new_data$person == \u0026#39;james\u0026#39;, 2] \u0026lt;- 99 # column2 change return(new_data) } where ‘column1’ and ‘column2’ can be replaced by specific names. Here is where I usually get confused, the following code does not work:\ncool_data \u0026lt;- impute_99_column_specific(dish, v_1, v_2) Fortunately the correction is simple, just put quotes around the column names:\ncool_data \u0026lt;- impute_99_column_specific(dish, \u0026#39;v_1\u0026#39;, \u0026#39;v_2\u0026#39;) cool_data ## person v_1 v_2 ## 1 jimmy 99.000000 11.567321 ## 2 james 99.000000 5.346487 ## 3 johnny 0.303881 9.959681 Bo\\(^2\\)m =)\n ","date":1527897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527897600,"objectID":"f89d1887454d37ecccba8682b69d790f","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-06-02/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/computational_notes/2018-06-02/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Column Names As Parameters","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A few spline models (also known as piecewise models). As in previous posts, ‘affect’ is the name given to values of \\(y\\) throughout.\n1) Growth and Even More Growth A model that captures a process that increases initially and then increases at an even greater rate once it reaches time point 5. The data generating process:\n\\[\\begin{equation} y_{it} = \\begin{cases} 4 + 0.3t + error_{t}, \u0026amp; \\text{if time \u0026lt; 5}\\\\ 8 + 0.9t + error_{t}, \u0026amp; \\text{otherwise} \\end{cases} \\end{equation}\\]\nThe data generating code and plot\nlibrary(tidyverse) library(lavaan) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 10 intercept_1 \u0026lt;- 4 intercept_2 \u0026lt;- 8 growth1 \u0026lt;- 0.3 growth2 \u0026lt;- 0.9 df_matrix \u0026lt;- matrix(, ncol = 3, nrow = N*time) count \u0026lt;- 0 for(i in 1:N){ unob_het_y \u0026lt;- rnorm(1,0,1) for(j in 1:time){ count \u0026lt;- count + 1 if(j \u0026lt; 5){ df_matrix[count, 1] \u0026lt;- i df_matrix[count, 2] \u0026lt;- j df_matrix[count, 3] \u0026lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1) }else{ df_matrix[count, 1] \u0026lt;- i df_matrix[count, 2] \u0026lt;- j df_matrix[count, 3] \u0026lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1) } } } df \u0026lt;- data.frame(df_matrix) names(df) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) df1 \u0026lt;- df %\u0026gt;% filter(time \u0026lt; 5) df2 \u0026lt;- df %\u0026gt;% filter(time \u0026gt;= 5) df_sum1 \u0026lt;- df1 %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) df_sum2 \u0026lt;- df2 %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) ggplot() + geom_point(data = df1, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df1, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_point(data = df2, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df2, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df_sum1, aes(x = time, y = affect)) + geom_line(data = df_sum2, aes(x = time, y = affect)) Estimating the parameters using SEM:\nlibrary(lavaan) df_wide \u0026lt;- reshape(df, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) spline_string \u0026lt;- \u0026#39; # latent intercept for first half level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 # latent intercept for second half level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 1*affect.10 # latent slope for first half basis coefficients slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 # latent slope for second half basis coefficients slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10 # means and variance of latent factors level1_affect ~~ level1_affect level2_affect ~~ level2_affect slope1_affect ~~ slope1_affect slope2_affect ~~ slope2_affect # covariance between latent factors level1_affect ~~ level2_affect level1_affect ~~ slope1_affect level1_affect ~~ slope2_affect level2_affect ~~ slope1_affect level2_affect ~~ slope2_affect slope1_affect ~~ slope2_affect # constrain means of indicators to zero across time affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 affect.7 ~ 0 affect.8 ~ 0 affect.9 ~ 0 affect.10 ~ 0 # constrain residual variance to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 affect.7 ~~ res_var*affect.7 affect.8 ~~ res_var*affect.8 affect.9 ~~ res_var*affect.9 affect.10 ~~ res_var*affect.10 \u0026#39; spline_model \u0026lt;- growth(spline_string, data = df_wide) summary(spline_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 78 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 24 ## Number of equality constraints 9 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 46.543 ## Degrees of freedom 50 ## P-value (Chi-square) 0.613 ## ## Model Test Baseline Model: ## ## Test statistic 2013.966 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6210.676 ## Loglikelihood unrestricted model (H1) -6187.405 ## ## Akaike (AIC) 12451.352 ## Bayesian (BIC) 12511.224 ## Sample-size adjusted Bayesian (BIC) 12463.628 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.028 ## P-value RMSEA \u0026lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.039 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## level2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 1.000 ## affect.6 1.000 ## affect.7 1.000 ## affect.8 1.000 ## affect.9 1.000 ## affect.10 1.000 ## slope1_affect =~ ## affect.1 1.000 ## affect.2 2.000 ## affect.3 3.000 ## affect.4 4.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## slope2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 5.000 ## affect.6 6.000 ## affect.7 7.000 ## affect.8 8.000 ## affect.9 9.000 ## affect.10 10.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect ~~ ## level2_affect 1.174 0.186 6.315 0.000 ## slope1_affect 0.011 0.046 0.232 0.817 ## slope2_affect -0.006 0.020 -0.326 0.744 ## level2_affect ~~ ## slope1_affect 0.014 0.050 0.282 0.778 ## slope2_affect -0.039 0.039 -1.012 0.312 ## slope1_affect ~~ ## slope2_affect -0.003 0.006 -0.573 0.567 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## .affect.7 0.000 ## .affect.8 0.000 ## .affect.9 0.000 ## .affect.10 0.000 ## level1_affect 3.951 0.079 49.895 0.000 ## level2_affect 7.962 0.111 71.470 0.000 ## slope1_affect 0.307 0.022 13.780 0.000 ## slope2_affect 0.898 0.012 71.924 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lvl1_ff 0.987 0.183 5.404 0.000 ## lvl2_ff 1.536 0.365 4.212 0.000 ## slp1_ff -0.005 0.015 -0.317 0.752 ## slp2_ff 0.004 0.005 0.945 0.345 ## .affct.1 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.2 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.3 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.4 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.5 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.6 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.7 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.8 (rs_v) 1.014 0.029 34.641 0.000 ## .affct.9 (rs_v) 1.014 0.029 34.641 0.000 ## .affc.10 (rs_v) 1.014 0.029 34.641 0.000 The structure of the basis coefficients is the important piece that allows us to capture the change in slope:\n\u0026#39; # latent slope for first half basis coefficients slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 # latent slope for second half basis coefficients slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10 \u0026#39;  2) Growth and Negative Growth A model that captures a process that goes up and then goes down. The data generating process:\n\\[\\begin{equation} y_{it} = \\begin{cases} 4 + 0.5t + error_{t}, \u0026amp; \\text{if time \u0026lt; 5}\\\\ 4 - 0.5t + error_{t}, \u0026amp; \\text{otherwise} \\end{cases} \\end{equation}\\]\nThe data generating code and plot\nlibrary(tidyverse) library(lavaan) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 10 intercept_1 \u0026lt;- 4 intercept_2 \u0026lt;- 4 growth1 \u0026lt;- 0.8 growth2 \u0026lt;- -0.8 df_matrix_b \u0026lt;- matrix(, ncol = 3, nrow = N*time) count \u0026lt;- 0 for(i in 1:N){ unob_het_y \u0026lt;- rnorm(1,0,1) for(j in 1:time){ count \u0026lt;- count + 1 if(j \u0026lt; 5){ df_matrix_b[count, 1] \u0026lt;- i df_matrix_b[count, 2] \u0026lt;- j df_matrix_b[count, 3] \u0026lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1) }else{ df_matrix_b[count, 1] \u0026lt;- i df_matrix_b[count, 2] \u0026lt;- j df_matrix_b[count, 3] \u0026lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1) } } } df_b \u0026lt;- data.frame(df_matrix_b) names(df_b) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) df1_b \u0026lt;- df_b %\u0026gt;% filter(time \u0026lt; 5) df2_b \u0026lt;- df_b %\u0026gt;% filter(time \u0026gt;= 5) df_sum1_b \u0026lt;- df1_b %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) df_sum2_b \u0026lt;- df2_b %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) ggplot() + geom_point(data = df1_b, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df1_b, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_point(data = df2_b, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df2_b, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df_sum1_b, aes(x = time, y = affect)) + geom_line(data = df_sum2_b, aes(x = time, y = affect)) Estimating the parameters using SEM:\nlibrary(lavaan) df_wide_b \u0026lt;- reshape(df_b, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) spline_string_b \u0026lt;- \u0026#39; # latent intercept for first half level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 # latent intercept for second half level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 1*affect.10 # latent slope for first half basis coefficients slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 # latent slope for second half basis coefficients slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10 # means and variance of latent factors level1_affect ~~ level1_affect level2_affect ~~ level2_affect slope1_affect ~~ slope1_affect slope2_affect ~~ slope2_affect # covariance between latent factors level1_affect ~~ level2_affect level1_affect ~~ slope1_affect level1_affect ~~ slope2_affect level2_affect ~~ slope1_affect level2_affect ~~ slope2_affect slope1_affect ~~ slope2_affect # constrain means of indicators to zero across time affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 affect.7 ~ 0 affect.8 ~ 0 affect.9 ~ 0 affect.10 ~ 0 # constrain residual variance to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 affect.7 ~~ res_var*affect.7 affect.8 ~~ res_var*affect.8 affect.9 ~~ res_var*affect.9 affect.10 ~~ res_var*affect.10 \u0026#39; spline_model_b \u0026lt;- growth(spline_string_b, data = df_wide_b) summary(spline_model_b, fit.measures = T) ## lavaan 0.6-6 ended normally after 88 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 24 ## Number of equality constraints 9 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 50.005 ## Degrees of freedom 50 ## P-value (Chi-square) 0.473 ## ## Model Test Baseline Model: ## ## Test statistic 1843.189 ## Degrees of freedom 45 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6219.629 ## Loglikelihood unrestricted model (H1) -6194.626 ## ## Akaike (AIC) 12469.257 ## Bayesian (BIC) 12529.129 ## Sample-size adjusted Bayesian (BIC) 12481.533 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.032 ## P-value RMSEA \u0026lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.030 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## level2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 1.000 ## affect.6 1.000 ## affect.7 1.000 ## affect.8 1.000 ## affect.9 1.000 ## affect.10 1.000 ## slope1_affect =~ ## affect.1 1.000 ## affect.2 2.000 ## affect.3 3.000 ## affect.4 4.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## slope2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 5.000 ## affect.6 6.000 ## affect.7 7.000 ## affect.8 8.000 ## affect.9 9.000 ## affect.10 10.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect ~~ ## level2_affect 1.204 0.172 6.981 0.000 ## slope1_affect 0.101 0.043 2.372 0.018 ## slope2_affect -0.027 0.018 -1.488 0.137 ## level2_affect ~~ ## slope1_affect -0.015 0.045 -0.342 0.733 ## slope2_affect 0.038 0.035 1.091 0.275 ## slope1_affect ~~ ## slope2_affect 0.004 0.005 0.776 0.438 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## .affect.7 0.000 ## .affect.8 0.000 ## .affect.9 0.000 ## .affect.10 0.000 ## level1_affect 3.820 0.077 49.653 0.000 ## level2_affect 4.102 0.105 39.064 0.000 ## slope1_affect 0.847 0.021 39.473 0.000 ## slope2_affect -0.819 0.012 -69.844 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lvl1_ff 0.752 0.174 4.326 0.000 ## lvl2_ff 0.769 0.329 2.338 0.019 ## slp1_ff -0.031 0.014 -2.150 0.032 ## slp2_ff -0.007 0.004 -1.542 0.123 ## .affct.1 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.2 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.3 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.4 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.5 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.6 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.7 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.8 (rs_v) 1.077 0.031 34.641 0.000 ## .affct.9 (rs_v) 1.077 0.031 34.641 0.000 ## .affc.10 (rs_v) 1.077 0.031 34.641 0.000 Notice that the string syntax is the exact same because the process changes at the same point in time, it does not matter if the process changes to ‘more positive’ or ‘more negative.’\n 3) Negative Growth, Growth, and Negative Growth Now a process that goes down, goes up, and then goes back down. The data generating process:\n\\[\\begin{equation} y_{it} = \\begin{cases} 4 - 0.5t + error_{t}, \u0026amp; \\text{if time \u0026lt; 5}\\\\ 4 + 0.5t + error_{t}, \u0026amp; \\text{if 5 \u0026lt; time \u0026lt; 10}\\\\ 4 - 0.5t + error_{t}, \u0026amp; \\text{otherwise} \\end{cases} \\end{equation}\\]\nThe data generating code and plot\nlibrary(tidyverse) library(lavaan) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 15 intercept_1 \u0026lt;- 4 intercept_2 \u0026lt;- 4 intercept_3 \u0026lt;- 4 growth1 \u0026lt;- -0.5 growth2 \u0026lt;- 0.5 growth3 \u0026lt;- -0.5 df_matrix_c \u0026lt;- matrix(, ncol = 3, nrow = N*time) count \u0026lt;- 0 for(i in 1:N){ unob_het_y \u0026lt;- rnorm(1,0,1) for(j in 1:time){ count \u0026lt;- count + 1 if(j \u0026lt; 5){ df_matrix_c[count, 1] \u0026lt;- i df_matrix_c[count, 2] \u0026lt;- j df_matrix_c[count, 3] \u0026lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1) }else if(j \u0026gt;= 5 \u0026amp;\u0026amp; j \u0026lt; 10){ df_matrix_c[count, 1] \u0026lt;- i df_matrix_c[count, 2] \u0026lt;- j df_matrix_c[count, 3] \u0026lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1) }else{ df_matrix_c[count, 1] \u0026lt;- i df_matrix_c[count, 2] \u0026lt;- j df_matrix_c[count, 3] \u0026lt;- intercept_3 + growth3*j + unob_het_y + rnorm(1,0,1) } } } df_c \u0026lt;- data.frame(df_matrix_c) names(df_c) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) df1_c \u0026lt;- df_c %\u0026gt;% filter(time \u0026lt; 5) df2_c \u0026lt;- df_c %\u0026gt;% filter(time \u0026gt;= 5 \u0026amp; time \u0026lt; 10) df3_c \u0026lt;- df_c %\u0026gt;% filter(time \u0026gt;= 10) df_sum1_c \u0026lt;- df1_c %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) df_sum2_c \u0026lt;- df2_c %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) df_sum3_c \u0026lt;- df3_c %\u0026gt;% group_by(time) %\u0026gt;% summarise( affect = mean(affect) ) ggplot() + geom_point(data = df1_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df1_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_point(data = df2_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df2_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df_sum1_c, aes(x = time, y = affect)) + geom_line(data = df_sum2_c, aes(x = time, y = affect)) + geom_point(data = df3_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df3_c, aes(x = time, y = affect, group = id), color = \u0026#39;gray85\u0026#39;) + geom_line(data = df_sum3_c, aes(x = time, y = affect)) Now estimate the parameters using SEM:\nlibrary(lavaan) df_wide_c \u0026lt;- reshape(df_c, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) spline_string_c \u0026lt;- \u0026#39; # latent intercept for first third level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent intercept for second third level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent intercept for final third level3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 1*affect.10 + 1*affect.11 + 1*affect.12 + 1*affect.13 + 1*affect.14 + 1*affect.15 # latent slope for first third basis coefficients slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent slope for second third basis coefficients slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent slope for final third basis coefficients slope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + 13*affect.13 + 14*affect.14 + 15*affect.15 # means and variance of latent factors level1_affect ~~ level1_affect level2_affect ~~ level2_affect level3_affect ~~ level3_affect slope1_affect ~~ slope1_affect slope2_affect ~~ slope2_affect slope3_affect ~~ slope3_affect # covariance between latent factors level1_affect ~~ level2_affect level1_affect ~~ level3_affect level1_affect ~~ slope1_affect level1_affect ~~ slope2_affect level1_affect ~~ slope3_affect level2_affect ~~ level3_affect level2_affect ~~ slope1_affect level2_affect ~~ slope2_affect level2_affect ~~ slope3_affect level3_affect ~~ slope1_affect level3_affect ~~ slope2_affect level3_affect ~~ slope3_affect slope1_affect ~~ slope2_affect slope1_affect ~~ slope3_affect slope2_affect ~~ slope3_affect # constrain means of indicators to zero across time affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 affect.7 ~ 0 affect.8 ~ 0 affect.9 ~ 0 affect.10 ~ 0 # constrain residual variance to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 affect.7 ~~ res_var*affect.7 affect.8 ~~ res_var*affect.8 affect.9 ~~ res_var*affect.9 affect.10 ~~ res_var*affect.10 \u0026#39; spline_model_c \u0026lt;- growth(spline_string_c, data = df_wide_c) summary(spline_model_c, fit.measures = T) ## lavaan 0.6-6 ended normally after 156 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 42 ## Number of equality constraints 9 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 90.139 ## Degrees of freedom 102 ## P-value (Chi-square) 0.793 ## ## Model Test Baseline Model: ## ## Test statistic 3303.179 ## Degrees of freedom 105 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9108.717 ## Loglikelihood unrestricted model (H1) -9063.647 ## ## Akaike (AIC) 18283.433 ## Bayesian (BIC) 18415.151 ## Sample-size adjusted Bayesian (BIC) 18310.440 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.018 ## P-value RMSEA \u0026lt;= 0.05 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.032 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## affect.11 0.000 ## affect.12 0.000 ## affect.13 0.000 ## affect.14 0.000 ## affect.15 0.000 ## level2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 1.000 ## affect.6 1.000 ## affect.7 1.000 ## affect.8 1.000 ## affect.9 1.000 ## affect.10 0.000 ## affect.11 0.000 ## affect.12 0.000 ## affect.13 0.000 ## affect.14 0.000 ## affect.15 0.000 ## level3_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 1.000 ## affect.11 1.000 ## affect.12 1.000 ## affect.13 1.000 ## affect.14 1.000 ## affect.15 1.000 ## slope1_affect =~ ## affect.1 1.000 ## affect.2 2.000 ## affect.3 3.000 ## affect.4 4.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 0.000 ## affect.11 0.000 ## affect.12 0.000 ## affect.13 0.000 ## affect.14 0.000 ## affect.15 0.000 ## slope2_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 5.000 ## affect.6 6.000 ## affect.7 7.000 ## affect.8 8.000 ## affect.9 9.000 ## affect.10 0.000 ## affect.11 0.000 ## affect.12 0.000 ## affect.13 0.000 ## affect.14 0.000 ## affect.15 0.000 ## slope3_affect =~ ## affect.1 0.000 ## affect.2 0.000 ## affect.3 0.000 ## affect.4 0.000 ## affect.5 0.000 ## affect.6 0.000 ## affect.7 0.000 ## affect.8 0.000 ## affect.9 0.000 ## affect.10 10.000 ## affect.11 11.000 ## affect.12 12.000 ## affect.13 13.000 ## affect.14 14.000 ## affect.15 15.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## level1_affect ~~ ## level2_affect 1.063 0.199 5.340 0.000 ## level3_affect 0.782 0.262 2.987 0.003 ## slope1_affect -0.034 0.049 -0.701 0.483 ## slope2_affect -0.009 0.024 -0.369 0.712 ## slope3_affect 0.018 0.019 0.953 0.341 ## level2_affect ~~ ## level3_affect 0.655 0.383 1.710 0.087 ## slope1_affect 0.045 0.056 0.803 0.422 ## slope2_affect 0.104 0.051 2.041 0.041 ## slope3_affect 0.049 0.028 1.732 0.083 ## level3_affect ~~ ## slope1_affect 0.069 0.075 0.911 0.362 ## slope2_affect 0.065 0.047 1.375 0.169 ## slope3_affect 0.013 0.058 0.223 0.824 ## slope1_affect ~~ ## slope2_affect -0.006 0.007 -0.930 0.353 ## slope3_affect -0.004 0.006 -0.725 0.468 ## slope2_affect ~~ ## slope3_affect -0.007 0.004 -2.005 0.045 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## .affect.7 0.000 ## .affect.8 0.000 ## .affect.9 0.000 ## .affect.10 0.000 ## .affect.11 0.000 ## .affect.12 0.000 ## .affect.13 0.000 ## .affect.14 0.000 ## .affect.15 0.000 ## level1_affect 4.030 0.081 49.969 0.000 ## level2_affect 3.805 0.119 31.980 0.000 ## level3_affect 3.976 0.161 24.768 0.000 ## slope1_affect -0.500 0.023 -21.345 0.000 ## slope2_affect 0.533 0.015 36.391 0.000 ## slope3_affect -0.494 0.012 -41.505 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## lvl1_ff 1.060 0.190 5.589 0.000 ## lvl2_ff 0.417 0.430 0.971 0.331 ## lvl3_ff 1.057 0.791 1.336 0.181 ## slp1_ff 0.014 0.017 0.813 0.416 ## slp2_ff -0.017 0.007 -2.511 0.012 ## slp3_ff -0.001 0.005 -0.300 0.764 ## .affct.1 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.2 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.3 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.4 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.5 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.6 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.7 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.8 (rs_v) 1.028 0.031 33.596 0.000 ## .affct.9 (rs_v) 1.028 0.031 33.596 0.000 ## .affc.10 (rs_v) 1.028 0.031 33.596 0.000 ## .affc.11 1.005 0.079 12.669 0.000 ## .affc.12 0.994 0.076 13.016 0.000 ## .affc.13 1.006 0.077 13.128 0.000 ## .affc.14 1.015 0.079 12.850 0.000 ## .affc.15 1.012 0.087 11.652 0.000 Again, the basis coefficients are the important piece here:\n\u0026#39; # latent slope for first third basis coefficients slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent slope for second third basis coefficients slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15 # latent slope for final third basis coefficients slope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + 13*affect.13 + 14*affect.14 + 15*affect.15 \u0026#39; ## [1] \u0026quot;\\n\\n\\n# latent slope for first third basis coefficients\\n\\nslope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + \\n 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \\n 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \\n 0*affect.13 + 0*affect.14 + 0*affect.15\\n\\n# latent slope for second third basis coefficients\\n\\nslope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + \\n 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + \\n 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \\n 0*affect.13 + 0*affect.14 + 0*affect.15\\n\\n# latent slope for final third basis coefficients\\n\\nslope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 +\\n 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \\n 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + \\n 13*affect.13 + 14*affect.14 + 15*affect.15\\n\\n\\n\\n\u0026quot; Bo\\(^2\\)m =)\n ","date":1525478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525478400,"objectID":"eb11cb09c061085e54ade4194548fcc3","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-05-05/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/computational_notes/2018-05-05/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Spline Modeling","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Latent Growth Curves I will progress through three models: linear, quadratic growth, and latent basis. In every example I use a sample of 400, 6 time points, and ‘affect’ as the variable of interest.\nDon’t forget that multiplying by time\n \\(0.6t\\)  is different from describing over time\n \\(0.6_t\\).  1) Linear The data generating process:\n\\[\\begin{equation} y_{it} = 4 - 0.6t + e_{t} \\end{equation}\\]\nlibrary(tidyverse) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 6 intercept \u0026lt;- 4 linear_growth \u0026lt;- -0.6 df_matrix \u0026lt;- matrix(, nrow = N*time, ncol = 3) count \u0026lt;- 0 for(i in 1:400){ unob_het_affect \u0026lt;- rnorm(1,0,3) for(j in 1:6){ count \u0026lt;- count + 1 if(j == 1){ df_matrix[count, 1] \u0026lt;- i df_matrix[count, 2] \u0026lt;- j df_matrix[count, 3] \u0026lt;- intercept + unob_het_affect + rnorm(1,0,1) }else{ df_matrix[count, 1] \u0026lt;- i df_matrix[count, 2] \u0026lt;- j df_matrix[count, 3] \u0026lt;- intercept + linear_growth*j + unob_het_affect + rnorm(1,0,1) } } } df \u0026lt;- data.frame(df_matrix) names(df) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) random_ids \u0026lt;- sample(df$id, 5) random_df \u0026lt;- df %\u0026gt;% filter(id %in% random_ids) ggplot(df, aes(x = time, y = affect, group = id)) + geom_point(color = \u0026#39;gray85\u0026#39;) + geom_line(color = \u0026#39;gray85\u0026#39;) + geom_point(data = random_df, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;) + geom_line(data = random_df, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;)  Estimating the model:\nFormatting the data:\ndf_wide \u0026lt;- reshape(df, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) First, an intercept only (no change) model:\nlibrary(lavaan) no_change_string \u0026lt;- \u0026#39; # Latent intercept factor intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 # Mean and variance of latent intercept factor intercept_affect ~~ intercept_affect # Fix observed variable means to 0 affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 # Constrain residual (error) variance of observed variables to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 \u0026#39; no_change_model \u0026lt;- growth(no_change_string, data = df_wide) summary(no_change_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 8 ## Number of equality constraints 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 1961.248 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3884.766 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.499 ## Tucker-Lewis Index (TLI) 0.687 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5184.361 ## Loglikelihood unrestricted model (H1) -4203.737 ## ## Akaike (AIC) 10374.721 ## Bayesian (BIC) 10386.696 ## Sample-size adjusted Bayesian (BIC) 10377.176 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.449 ## 90 Percent confidence interval - lower 0.432 ## 90 Percent confidence interval - upper 0.466 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.192 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 1.000 ## affect.6 1.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## intercept_ffct 1.834 0.150 12.244 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intrcp_ 8.535 0.635 13.438 0.000 ## .affct.1 (rs_v) 2.669 0.084 31.623 0.000 ## .affct.2 (rs_v) 2.669 0.084 31.623 0.000 ## .affct.3 (rs_v) 2.669 0.084 31.623 0.000 ## .affct.4 (rs_v) 2.669 0.084 31.623 0.000 ## .affct.5 (rs_v) 2.669 0.084 31.623 0.000 ## .affct.6 (rs_v) 2.669 0.084 31.623 0.000 Now, a linear growth model centered at time point 1. The intercept factor estimate, therefore, is the estimated average affect at time 1.\nlibrary(lavaan) linear_change_string \u0026lt;- \u0026#39; # Latent intercept and slope factors intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 + 4*affect.5 + 5*affect.6 # Mean and variance of latent factors intercept_affect ~~ intercept_affect slope_affect ~~ slope_affect # Covariance between latent factors intercept_affect ~~ slope_affect # Fix observed variable means to 0 affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 # Constrain residual (error) variance of observed variables to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 \u0026#39; linear_change_model \u0026lt;- growth(linear_change_string, data = df_wide) summary(linear_change_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 11 ## Number of equality constraints 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 90.124 ## Degrees of freedom 21 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3884.766 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.982 ## Tucker-Lewis Index (TLI) 0.987 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -4248.799 ## Loglikelihood unrestricted model (H1) -4203.737 ## ## Akaike (AIC) 8509.598 ## Bayesian (BIC) 8533.547 ## Sample-size adjusted Bayesian (BIC) 8514.508 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.091 ## 90 Percent confidence interval - lower 0.072 ## 90 Percent confidence interval - upper 0.110 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.036 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 1.000 ## affect.6 1.000 ## slope_affect =~ ## affect.1 0.000 ## affect.2 1.000 ## affect.3 2.000 ## affect.4 3.000 ## affect.5 4.000 ## affect.6 5.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect ~~ ## slope_affect 0.072 0.037 1.976 0.048 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## intercept_ffct 3.535 0.150 23.546 0.000 ## slope_affect -0.680 0.012 -56.769 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intrcp_ 8.461 0.638 13.265 0.000 ## slp_ffc -0.003 0.005 -0.683 0.495 ## .affct.1 (rs_v) 1.060 0.037 28.284 0.000 ## .affct.2 (rs_v) 1.060 0.037 28.284 0.000 ## .affct.3 (rs_v) 1.060 0.037 28.284 0.000 ## .affct.4 (rs_v) 1.060 0.037 28.284 0.000 ## .affct.5 (rs_v) 1.060 0.037 28.284 0.000 ## .affct.6 (rs_v) 1.060 0.037 28.284 0.000 inspect(linear_change_model, \u0026#39;cov.lv\u0026#39;) ## intrc_ slp_ff ## intercept_affect 8.461 ## slope_affect 0.072 -0.003 This model does an adequate job recovering the intercept and slope parameters.\nIf I wanted to center the model at time point 3 the latent intercept term would be interpreted as the estimated average affect at time 3 and the syntax would change to:\n\u0026#39; slope_affect =~ -2*affect.1 + -1*affect.2 + 0*affect.3 + 1*affect.4 + 2*affect.5 + 3*affect.6 \u0026#39;  2) Quadratic The data generating process:\n\\[\\begin{equation} y_{it} = 4 + 0.2t + 0.7t^2 + e_{t} \\end{equation}\\]\nlibrary(tidyverse) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 6 intercept_mu \u0026lt;- 4 linear_growth2 \u0026lt;- 0.2 quad_growth \u0026lt;- 0.7 df_matrix2 \u0026lt;- matrix(, nrow = N*time, ncol = 3) count \u0026lt;- 0 for(i in 1:400){ unob_het_affect \u0026lt;- rnorm(1,0,3) for(j in 1:6){ count \u0026lt;- count + 1 if(j == 1){ df_matrix2[count, 1] \u0026lt;- i df_matrix2[count, 2] \u0026lt;- j df_matrix2[count, 3] \u0026lt;- intercept + rnorm(1,0,1) + rnorm(1,0,1) }else{ df_matrix2[count, 1] \u0026lt;- i df_matrix2[count, 2] \u0026lt;- j df_matrix2[count, 3] \u0026lt;- intercept + linear_growth2*j + quad_growth*(j^2) + unob_het_affect + rnorm(1,0,1) } } } df2 \u0026lt;- data.frame(df_matrix2) names(df2) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) random_ids2 \u0026lt;- sample(df2$id, 5) random_df2 \u0026lt;- df2 %\u0026gt;% filter(id %in% random_ids2) ggplot(df2, aes(x = time, y = affect, group = id)) + geom_point(color = \u0026#39;gray85\u0026#39;) + geom_line(color = \u0026#39;gray85\u0026#39;) + geom_point(data = random_df2, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;) + geom_line(data = random_df2, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;) + theme_wsj() Estimating the model:\nQuadratic growth model:\ndf_wide2 \u0026lt;- reshape(df2, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) library(lavaan) quad_change_string \u0026lt;- \u0026#39; # Latent intercept, linear slope, and quad slope factors intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 + 4*affect.5 + 5*affect.6 quad_slope_affect =~ 0*affect.1 + 1*affect.2 + 4*affect.3 + 9*affect.4 + 16*affect.5 + 25*affect.6 # Mean and variance of latent factors intercept_affect ~~ intercept_affect slope_affect ~~ slope_affect quad_slope_affect ~~ quad_slope_affect # Covariance between latent factors intercept_affect ~~ slope_affect intercept_affect ~~ quad_slope_affect slope_affect ~~ quad_slope_affect # Fix observed variable means to 0 affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 # Constrain residual (error) variance of observed variables to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 \u0026#39; quad_change_model \u0026lt;- growth(quad_change_string, data = df_wide2) summary(quad_change_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 91 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 15 ## Number of equality constraints 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 629.047 ## Degrees of freedom 17 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3252.120 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.811 ## Tucker-Lewis Index (TLI) 0.833 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -4613.101 ## Loglikelihood unrestricted model (H1) -4298.577 ## ## Akaike (AIC) 9246.202 ## Bayesian (BIC) 9286.116 ## Sample-size adjusted Bayesian (BIC) 9254.386 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.300 ## 90 Percent confidence interval - lower 0.280 ## 90 Percent confidence interval - upper 0.320 ## P-value RMSEA \u0026lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.237 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 1.000 ## affect.6 1.000 ## slope_affect =~ ## affect.1 0.000 ## affect.2 1.000 ## affect.3 2.000 ## affect.4 3.000 ## affect.5 4.000 ## affect.6 5.000 ## quad_slope_affect =~ ## affect.1 0.000 ## affect.2 1.000 ## affect.3 4.000 ## affect.4 9.000 ## affect.5 16.000 ## affect.6 25.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect ~~ ## slope_affect 0.880 0.150 5.855 0.000 ## quad_slop_ffct -0.137 0.024 -5.747 0.000 ## slope_affect ~~ ## quad_slop_ffct -0.516 0.054 -9.627 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## intercept_ffct 4.109 0.067 61.634 0.000 ## slope_affect 2.109 0.108 19.450 0.000 ## quad_slop_ffct 0.624 0.017 36.005 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intrcp_ 0.388 0.138 2.816 0.005 ## slp_ffc 3.473 0.336 10.328 0.000 ## qd_slp_ 0.075 0.009 8.599 0.000 ## .affct.1 (rs_v) 1.691 0.069 24.495 0.000 ## .affct.2 (rs_v) 1.691 0.069 24.495 0.000 ## .affct.3 (rs_v) 1.691 0.069 24.495 0.000 ## .affct.4 (rs_v) 1.691 0.069 24.495 0.000 ## .affct.5 (rs_v) 1.691 0.069 24.495 0.000 ## .affct.6 (rs_v) 1.691 0.069 24.495 0.000 This model recovers the intercept and quadratic parameters but not the linear growth parameter.\n 3) Latent Basis This model allows us to see where a majority of the change occurs in the process. For example, does more change occur between time points 2 and 3 or 5 and 6? In this model we are not trying to recover the parameters, but describe the change process in detail.\nData generating process:\nTime 1 - Time 3: \\[\\begin{equation} y_{it} = 4 + 0.2t + e_{t} \\end{equation}\\]\nTime 4 - Time 6: \\[\\begin{equation} y_{it} = 4 + 0.8t + e_{t} \\end{equation}\\]\nlibrary(tidyverse) library(ggplot2) library(MASS) N \u0026lt;- 400 time \u0026lt;- 6 intercept_mu \u0026lt;- 4 growth_1 \u0026lt;- 0.2 growth_2 \u0026lt;- 0.8 df_matrix3 \u0026lt;- matrix(, nrow = N*time, ncol = 3) count \u0026lt;- 0 for(i in 1:400){ unob_het_affect \u0026lt;- rnorm(1,0,3) for(j in 1:6){ count \u0026lt;- count + 1 if(j \u0026lt; 4){ df_matrix3[count, 1] \u0026lt;- i df_matrix3[count, 2] \u0026lt;- j df_matrix3[count, 3] \u0026lt;- intercept + growth_1*j + unob_het_affect + rnorm(1,0,1) }else{ df_matrix3[count, 1] \u0026lt;- i df_matrix3[count, 2] \u0026lt;- j df_matrix3[count, 3] \u0026lt;- intercept + growth_2*j + unob_het_affect + rnorm(1,0,1) } } } df3 \u0026lt;- data.frame(df_matrix3) names(df3) \u0026lt;- c(\u0026#39;id\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;affect\u0026#39;) random_ids3 \u0026lt;- sample(df3$id, 5) random_df3 \u0026lt;- df3 %\u0026gt;% filter(id %in% random_ids3) ggplot(df3, aes(x = time, y = affect, group = id)) + geom_point(color = \u0026#39;gray85\u0026#39;) + geom_line(color = \u0026#39;gray85\u0026#39;) + geom_point(data = random_df3, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;) + geom_line(data = random_df3, aes(x = time, y = affect, group = id), color = \u0026#39;blue\u0026#39;) Estimating the model:\nLatent basis:\nSimilar to a linear growth model but we freely estimate the intermediate basis coefficients. Remember to constrain the first basis coefficient to zero and the last to 1.\ndf_wide3 \u0026lt;- reshape(df3, idvar = \u0026#39;id\u0026#39;, timevar = \u0026#39;time\u0026#39;, direction = \u0026#39;wide\u0026#39;) library(lavaan) lb_string \u0026lt;- \u0026#39; # Latent intercept and slope terms with intermediate time points freely estimated intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 slope_affect =~ 0*affect.1 + bc1*affect.2 + bc2*affect.3 + bc3*affect.4 + bc4*affect.5 + 1*affect.6 # Mean and variance of latent factors intercept_affect ~~ intercept_affect slope_affect ~~ slope_affect # Covariance between latent factors intercept_affect ~~ slope_affect # Fix observed variable means to 0 affect.1 ~ 0 affect.2 ~ 0 affect.3 ~ 0 affect.4 ~ 0 affect.5 ~ 0 affect.6 ~ 0 # Constrain residual (error) variance of observed variables to equality across time affect.1 ~~ res_var*affect.1 affect.2 ~~ res_var*affect.2 affect.3 ~~ res_var*affect.3 affect.4 ~~ res_var*affect.4 affect.5 ~~ res_var*affect.5 affect.6 ~~ res_var*affect.6 \u0026#39; lb_model \u0026lt;- growth(lb_string, data = df_wide3) summary(lb_model, fit.measures = T) ## lavaan 0.6-6 ended normally after 64 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 15 ## Number of equality constraints 5 ## ## Number of observations 400 ## ## Model Test User Model: ## ## Test statistic 17.419 ## Degrees of freedom 17 ## P-value (Chi-square) 0.426 ## ## Model Test Baseline Model: ## ## Test statistic 3864.063 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -4225.520 ## Loglikelihood unrestricted model (H1) -4216.810 ## ## Akaike (AIC) 8471.040 ## Bayesian (BIC) 8510.955 ## Sample-size adjusted Bayesian (BIC) 8479.224 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.008 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.046 ## P-value RMSEA \u0026lt;= 0.05 0.970 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.021 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect =~ ## affect.1 1.000 ## affect.2 1.000 ## affect.3 1.000 ## affect.4 1.000 ## affect.5 1.000 ## affect.6 1.000 ## slope_affect =~ ## affect.1 0.000 ## affect.2 (bc1) 0.016 0.016 1.014 0.311 ## affect.3 (bc2) 0.075 0.015 4.936 0.000 ## affect.4 (bc3) 0.643 0.014 46.582 0.000 ## affect.5 (bc4) 0.807 0.014 55.866 0.000 ## affect.6 1.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intercept_affect ~~ ## slope_affect 0.111 0.151 0.739 0.460 ## ## Intercepts: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## .affect.1 0.000 ## .affect.2 0.000 ## .affect.3 0.000 ## .affect.4 0.000 ## .affect.5 0.000 ## .affect.6 0.000 ## intercept_ffct 4.456 0.156 28.509 0.000 ## slope_affect 4.578 0.071 64.645 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u0026gt;|z|) ## intrcp_ 8.735 0.643 13.580 0.000 ## slp_ffc -0.068 0.078 -0.864 0.388 ## .affct.1 (rs_v) 1.037 0.037 28.284 0.000 ## .affct.2 (rs_v) 1.037 0.037 28.284 0.000 ## .affct.3 (rs_v) 1.037 0.037 28.284 0.000 ## .affct.4 (rs_v) 1.037 0.037 28.284 0.000 ## .affct.5 (rs_v) 1.037 0.037 28.284 0.000 ## .affct.6 (rs_v) 1.037 0.037 28.284 0.000 bc1 represents the percentage of change for the average individual between time 1 and 2. bc2 represents the percentage change betwen time 1 and 3, bc4 is the percentage change between time 1 and 5, etc.\nBo\\(^2\\)m =)\n  ","date":1523750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523750400,"objectID":"97fded6dd2506f5143b4fb3c8f75c011","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-04-15/","publishdate":"2018-04-15T00:00:00Z","relpermalink":"/computational_notes/2018-04-15/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Latent Growth Curves","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I built the following simple computational model for an individual differences class in the Spring of 2018 to demonstrate how to incorporate explantory elements for trait development into a computational framework. This model assumes that an individual’s trait development depends on 1) the environment and 2) interactions with others inside and outside of the individual’s social group. Moreover, the model assumes traits are somewhat stable and exhibit self-similarity across time. The main properties I am trying to capture, therefore, include:\n The development of a stable trait through interactions with…\n a social group\n random others\n the environment\n   These properties do not represent what I think of as “true” aspects of trait development (although I think they are important). I use them, instead, to show the translation from verbal concepts to code representations.\nHere is the pseudocode for the model:\n Build agent\n Random initial trait value\n Peer group holder (initially 0)\n  Build global population of people with the trait (normally distributed)\n Time 1\n People in peer group?\n If yes:\n What is their average trait level?\n Use that level to filter who the agent interacts with from the global population\n  If no:\n Move to next step   Select person from the global population to interact with\n Uniform (-1, 1) = quality of the interaction\n If it goes well, agent’s trait is influenced by this person\n i.e., If uniform \u0026gt; 0  If it does not go well, agent keeps own trait\n i.e., If uniform \u0026lt; 0    Environment\n Random number that influences trait   Update trait and peer holder\n If the interaction went well, the new person joins the agent’s social group  Iterate\n  The Incomplete Model First I present the model without a loop in very simple code. We begin with a distribution of the trait in the population.\nglobal_population \u0026lt;- data.frame( \u0026quot;People\u0026quot; = c(1:1000), \u0026quot;SDO\u0026quot; = c(rnorm(1000, 100, 10)) ) Then I create the agent. I used SDO as my example in class, so that will be the “trait” here. The agent is given an initial value of the trait.\nagent \u0026lt;- list( SDO = 0, Peeps = NULL ) initial_sdo_value \u0026lt;- rnorm(1, 100, 10) agent[[1]][1] \u0026lt;- initial_sdo_value agent ## $SDO ## [1] 111.0397 ## ## $Peeps ## NULL If the agent has a social group (‘peeps’), then we would take the mean of their trait levels to inform who the agent interacts with from the global population.\n# if peeps \u0026gt; 0, take the average of their trait level num_peeps \u0026lt;- length(agent$Peeps) trait_of_peeps \u0026lt;- mean(agent$Peeps) # use average to bias how I sample the population # use filter (+ or - 25 from average) Because this is the first time point, however, the agent does not have a social group. Now we select a person from the global population for our agent to interact with. If our agent had a social group, the social group’s average trait would inform who we select, but again in this case the interaction is random.\nother \u0026lt;- sample(global_population$SDO, 1) The interaction is good or bad…\ninteraction_quality \u0026lt;- runif(1, min = -1, max = 1) If the interaction is good, our agent’s trait is influenced by this new individual.\n# quality good? interaction_quality \u0026gt; 0 new_sdo \u0026lt;- agent$SDO + (other - agent$SDO)*interaction_quality # quality bad? interaction quality \u0026lt; 0 new_sdo \u0026lt;- agent$SDO Then we throw in some environmental disturbance for fun\n# Environment environment_sdo \u0026lt;- sample(c(-20:20), 1) new_sdo \u0026lt;- new_sdo + environment_sdo and conclude by updating the agent\n# Update agent agent$SDO \u0026lt;- c(agent$SDO, new_sdo) # If the interaction went well, this person goes into friend group. If not, leave them out agent$Peeps \u0026lt;- c(agent$Peeps, other) agent ## $SDO ## [1] 111.0397 114.0397 ## ## $Peeps ## [1] 123.782  The Full Model Here is the full model and a plot of the agent’s trait over time.\n# - ----------------------------------------------------------------------- # - ----------------------------------------------------------------------- # - ----------------------------------------------------------------------- # - ----------------------------------------------------------------------- # - ----------------------------------------------------------------------- # - ----------------------------------------------------------------------- library(tidyverse) # Generate over time time_points \u0026lt;- 400 global_population \u0026lt;- data.frame( \u0026quot;People\u0026quot; = c(1:1000), \u0026quot;SDO\u0026quot; = c(rnorm(1000, 100, 10)) ) agent \u0026lt;- list( SDO = rep(0,time_points), Peeps = rep(0,time_points) ) initial_sdo_value \u0026lt;- rnorm(1, 100, 10) agent[[1]][1] \u0026lt;- initial_sdo_value other \u0026lt;- sample(global_population$SDO, 1) agent[[2]][1] \u0026lt;- other count \u0026lt;- 0 for(i in 2:time_points){ count \u0026lt;- count + 1 # sample global population and interact with them # filter based on peeps average # need to change this to only use values that are not zero use_non_zero_values \u0026lt;- agent$Peeps[agent$Peeps \u0026gt; 0] use_vals \u0026lt;- mean(use_non_zero_values) filter_top \u0026lt;- use_vals + 20 filter_lower \u0026lt;- use_vals - 20 new_df \u0026lt;- global_population %\u0026gt;% filter(SDO \u0026lt; filter_top \u0026amp; SDO \u0026gt; filter_lower) other \u0026lt;- sample(new_df$SDO, 1) interaction_quality \u0026lt;- runif(1, min = -1, max = 1) # quality good or bad? if(interaction_quality \u0026gt; 0){ new_sdo \u0026lt;- agent$SDO[i - 1] + (other - agent$SDO[i - 1])*interaction_quality }else{ new_sdo \u0026lt;- agent$SDO[i - 1] } # Environment environment_sdo \u0026lt;- sample(c(-20:20), 1) new_sdo \u0026lt;- new_sdo + environment_sdo # Update agent agent$SDO[i] \u0026lt;- new_sdo if(interaction_quality \u0026gt; 0){ agent$Peeps[i] \u0026lt;- other }else{ agent$Peeps \u0026lt;- agent$Peeps } } library(ggplot2) plot_agent \u0026lt;- data.frame( \u0026#39;Agent_SDO\u0026#39; = c(agent$SDO), \u0026quot;Peeps_SDO\u0026quot; = c(agent$Peeps), \u0026quot;Time\u0026quot; = c(1:time_points) ) new_data \u0026lt;- plot_agent %\u0026gt;% filter(Peeps_SDO \u0026gt; 0) %\u0026gt;% gather(Agent_SDO, Peeps_SDO, key = \u0026#39;variable\u0026#39;, value = \u0026#39;SDO\u0026#39;) ggplot(new_data, aes(x = Time, y = SDO)) + geom_point() + geom_line(color = \u0026#39;blue\u0026#39;) + facet_wrap(~variable) + ylab(\u0026quot;Level\u0026quot;) Bo\\(^2\\)m =)\n ","date":1522368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522368000,"objectID":"3b20afea93e2b1f5457f350b12b20ba3","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-03-30/","publishdate":"2018-03-30T00:00:00Z","relpermalink":"/computational_notes/2018-03-30/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Social Trait Development Computational Model","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Quick note on updating your credentials after you change your password on GitHub.\nSteps\n search for keychain access in spotlight\n delete the old github credentials\n clone and adjust a repo\n trigger the credential helper\n push changes to repo\n  Search for “keychain access” in spotlight.\nEnter “github” into the search bar. Delete all github password entries.\nClone a repo and make some changes.\ncd Desktop git clone https://github.com/Cdishop/website.git cd website echo \u0026quot;some change\u0026quot; \u0026gt;\u0026gt; README.md Activate keychain.\ngit credential-osxkeychain git config --global credential.helper osxkeychain Trigger a username/password entry by pushing to remote.\ngit add . git commit -m \u0026quot;commit for keychain change\u0026quot; git push -u origin master [enter credentials] git push Route 2 - No Keychain You may not see a saved password in your keychain. If that’s the case, then you can allow your push to fail and then reset your credentials in the terminal.\nClone, adjust, and then push a repository. After pushing, you will see a “fatal” error.\nEnter\ngit config --global user.email \u0026quot;Your email\u0026quot; git config --global user.name \u0026quot;Your Name\u0026quot; Then push again. You will be asked for a username and password. All subsequent repository pushes should then be automatic.\nBo\\(^2\\)m =)\n ","date":1519171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519171200,"objectID":"1a116c62f23b303688ddddda3876ddb4","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-02-21/","publishdate":"2018-02-21T00:00:00Z","relpermalink":"/computational_notes/2018-02-21/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Credential Update After Password Change","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" # restart session .rs.restartR() # clear environment remove(list = ls()) Bo\\(^2\\)m =)\n","date":1518912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518912000,"objectID":"80be7d3d6d11123674fdc0c198517117","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-02-18/","publishdate":"2018-02-18T00:00:00Z","relpermalink":"/computational_notes/2018-02-18/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Restart \u0026 Clear Environment","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Integration Trapezoid Rule\nTo find the area under a curve we can generate a sequence of trapezoids that follow the rules of the curve (i.e., the data generating function for the curve) along the \\(x\\)-axis and then add all of the trapezoids together. To create a trapezoid we use the following equation:\n let \\(w\\) equal the width of the trapezoid (along the \\(x\\)-axis), then\n Area = (\\(w/2\\) * \\(f(x_i)\\)) + \\(f(x_i+1)\\)   for a single trapezoid. That procedure then iterates across our entire \\(x\\)-axis and adds all of the components together.\nHere is an example function: \\(f(x) = 8 + cos(x^3)\\) and we will evaluate it over the interval [1, 10]. First, a plot of the curve itself.\nx \u0026lt;- seq(from = 1, to = 10, by = 1) f_x \u0026lt;- 8 + cos(x^3) # Plot library(ggplot2) library(ggthemes) ex_plot \u0026lt;- data.frame( \u0026quot;x\u0026quot; = c(x), \u0026quot;y\u0026quot; = c(f_x) ) g_plot \u0026lt;- ggplot(ex_plot, aes(x = x, y = y)) + geom_point() + geom_smooth(se = F, span = 0.2) + scale_x_continuous(breaks = c(1:10)) + theme_wsj() g_plot The trapezoid algorithm:\n# Parameters = the function, x-axis beginning, x-axis end, the number of trapezoids to create trapezoid_rule \u0026lt;- function(fx, start, end, num_traps){ # The width of each trapezoid w \u0026lt;- (end - start) / num_traps # the x-axis to evaluate our function along x_axis \u0026lt;- seq(from = start, to = end, by = w) # the y axis: apply the function (fx) to each value of our x-axis y_axis \u0026lt;- sapply(x_axis, fx) # The trapezoid rule: find the area of each trapezoid and then add them together trap_total \u0026lt;- w * ( (y_axis[1] / 2) + sum(y_axis[2:num_traps]) + (y_axis[num_traps + 1] / 2) ) return(trap_total) } Now we can evaluate our function (\\(f(x) = 8 + cos(x^3)\\)) with our trapezoid algorithm to find the area under its curve.\nUsing only 3 trapezoids:\neval_function \u0026lt;- function(x){ 8 + cos(x^3) } trapezoid_rule(eval_function, 1, 10, 3) ## [1] 72.29808 Using 10 trapezoids:\ntrapezoid_rule(eval_function, 1, 10, 10) ## [1] 72.84693 Using 50000 trapezoids:\ntrapezoid_rule(eval_function, 1, 10, 50000) ## [1] 71.84439  Optimization The Golden-Section Method\nNewton’s methods are great for finding local maxima or minima, but they also require knowing the derivative of whatever function we are evaluating. The goldent section method does not, and works in the following way:\nDefine three points along the x-axis: left (\\(l\\)), right (\\(r\\)), and middle (\\(m\\))\n Choose one of the following sections along the \\(x\\)-axis according to which is larger:\n middle to right (section “right”)\n middle to left (section “left”)\n  Choose a point on the \\(x\\)-axis within section “right” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)\n Apply our function to \\(y\\) and \\(m\\)\n If \\(f(y)\\) \u0026gt; \\(f(m)\\), then \\(l\\) becomes \\(m\\) and \\(m\\) becomes \\(y\\)\n Else \\(r\\) becomes \\(y\\)\n   Choose a point on the \\(x\\)-axis within section “left” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)\n Apply our function to \\(y\\) and \\(m\\)\n If \\(f(y)\\) \u0026gt; \\(f(m)\\), then \\(r\\) becomes \\(m\\) and \\(m\\) becomes \\(y\\)\n Else \\(l\\) becomes \\(y\\)\n   Continue until the size of the “right” or “left” window diminishes to some a priori set tolerance value\n  Note that this method assumes that the\nNow in code:\nOur example function: \\(f(x) = sin(x * 3)\\)\nx_2 \u0026lt;- seq(from = -5, to = 5, by = 1) f_x_2 \u0026lt;- -0.5 * (x_2^2) + 4 # Plot library(ggplot2) ex_plot_2 \u0026lt;- data.frame( \u0026quot;x\u0026quot; = c(x_2), \u0026quot;y\u0026quot; = c(f_x_2) ) g_plot_2 \u0026lt;- ggplot(ex_plot_2, aes(x = x, y = y)) + geom_point() + geom_smooth(se = F) g_plot_2 The golden section algorithm:\ngolden_section \u0026lt;- function(fx, x.l, x.r, x.m, tolerance){ # The golden ratio rule to help select \u0026#39;y\u0026#39; when needed grule \u0026lt;- 1 + (1 * sqrt(5)) / 2 # Apply the function at each of our starting locations (left, right, middle) # left f.l \u0026lt;- fx(x.l) # right f.r \u0026lt;- fx(x.r) # middle f.m \u0026lt;- fx(x.m) # continue to iterate until we pass our tolderance level for how big the \u0026quot;right\u0026quot; \u0026quot;left\u0026quot; window should be while (( x.r - x.l) \u0026gt; tolerance){ # if the right window is larger than the left window, then operate on the right window side if ( (x.r - x.m) \u0026gt; (x.m - x.l) ){ # select a point, y, according to the golden ratio rule y \u0026lt;- x.m + (x.r - x.m) / grule # apply the function to our selected y point f.y \u0026lt;- fx(y) # if the function at point y is higher than the function at the mid point if(f.y \u0026gt;= f.m){ # reassign our points according to the algorithm steps outlined above # in this case, within the right window y was higher than the middle. So \u0026#39;left\u0026#39; needs to become our new middle, and \u0026#39;middle\u0026#39; needs to become y x.l \u0026lt;- x.m f.l \u0026lt;- f.m x.m \u0026lt;- y f.m \u0026lt;- f.y } else { # if the function at y was lower than the function at the mid point # shift \u0026#39;right\u0026#39; to our y point x.r \u0026lt;- y f.r \u0026lt;- f.y } } else{ # if the right window is not larger than the left window, select the left window to operate on # choose a point, y, within the left window according to the golden ratio y \u0026lt;- x.m - (x.m - x.l) / grule # apply our function to that point f.y \u0026lt;- fx(y) # if the function at y is greater than the function at the mid point (within the left window) if(f.y \u0026gt;= f.m){ # reassign values according to the golden section method discussed above # in this case, within the left window our selected point is higher than the mid point (which is to the right of the selected y point) # so our \u0026quot;mid\u0026quot; point needs to become our \u0026quot;right\u0026quot; point and y needs to become \u0026quot;left\u0026quot; x.r \u0026lt;- x.m f.r \u0026lt;- f.m x.m \u0026lt;- y f.m \u0026lt;- f.y }else{ # if the y point is lower than the function at the mid point # now our y needs to become \u0026quot;left\u0026quot; x.l \u0026lt;- y f.l \u0026lt;- f.y } } } # return the mid point return(x.m) } To summarize, the algorithm splits the \\(x\\)-axis into windows (left, middle, right) and then evaluates the function across those windows. The dimensions of the windows change over time depending on whether the function at \\(y\\) is higher or lower than a specific window dimension.\nThese examples are described in more detail in Jones, Maillardet, and Robinson, Introduction to Scientific Programming and Simulation Using R\nBo\\(^2\\)m =)\n ","date":1518739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518739200,"objectID":"7a7a15bbba152cd891b6ced590c864de","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-02-16/","publishdate":"2018-02-16T00:00:00Z","relpermalink":"/computational_notes/2018-02-16/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Numerical Integration and Optimization","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I always forget how to make the y-axis horizontal in ggplot2. Here’s a note.\ntheme(axis.title.y = element_text(angle = 0))  Bo\\(^2\\)m =)\n","date":1516147200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516147200,"objectID":"284159ba8c2040dbfe3568eaa4e1a8eb","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-01-17/","publishdate":"2018-01-17T00:00:00Z","relpermalink":"/computational_notes/2018-01-17/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Horizontal Y Axis GGplot2","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"       Some random walk fun. I use 400 steps in each example.\nOne-Dimensional Random Walk A random walk using a recursive equation.\n# Empty vector to store the walk rw_1 \u0026lt;- numeric(400) # Initial value rw_1[1] \u0026lt;- 7 # The Random Walk equation in a for-loop for(i in 2:400){ rw_1[i] \u0026lt;- 1*rw_1[i - 1] + rnorm(1,0,2) } plot(rw_1) A random walk using R’s “cumsum” command. Here, I will generate a vector of randomly selected 1’s and -1’s. “Cumsum” then compiles those values.\n# A vector of 1\u0026#39;s and -1\u0026#39;s rw_2 \u0026lt;- sample(c(1, -1), 400, replace = T) rw_2 \u0026lt;- cumsum(rw_2) plot(rw_2)  Two-Dimensional Random Walk Now for the real fun. Here, the walk can move forward (1) or backward (-1) along either dimension 1 or 2. So, if the walk moves forward (1) in dimension 1, dimension 2 receives a value of 0 for that step. If the walk moves backward (-1) in dimension 2, dimension 1 receives a 0 for that step.\n# A matrix to store our walk # Column 1 is dimension 1, column 2 is dimension 2 rw_3 \u0026lt;- matrix(0, ncol = 2, nrow = 400) index \u0026lt;- cbind( 1:400, sample(c(1, 2), 400, replace = T) ) The “index” merits some explaining. The walk will randomly choose to move in dimension 1 (column 1 in “rw_3”) or 2 (column 2 in “rw_3”). This index establishes a way of assigning which choice the walk makes. Here is what “index” looks like:\nhead(index) ## [,1] [,2] ## [1,] 1 2 ## [2,] 2 1 ## [3,] 3 1 ## [4,] 4 2 ## [5,] 5 2 ## [6,] 6 2 The first column values tell the random walk which step its on (i.e., which row in “rw_3”), and the second column values tell the random walk which dimension it will step through (i.e., which column in “rw_3”).\nSo the “index” represents a random selection of dimension 1 or 2 at each step. Now I can apply that random choice to the random choice of stepping forward or backward (1 or -1).\n# At each step, select a dimension (specified by the index; column 1 or 2 of rw_3) # Then randomly select forward or backward rw_3[index] \u0026lt;- sample(c(-1, 1), 400, replace = T) # Now sum each column (dimension) just like our 1-dimensional walks rw_3[,1] \u0026lt;- cumsum(rw_3[,1]) rw_3[,2] \u0026lt;- cumsum(rw_3[,2]) Here is a visualization of the walk:\nlibrary(plotly) rw_3 \u0026lt;- data.frame(rw_3) rw_3$step \u0026lt;- c(1:400) names(rw_3)[1:2] \u0026lt;- c(\u0026quot;Dim_1\u0026quot;, \u0026quot;Dim_2\u0026quot;) plot_ly(rw_3, x = ~step, y = ~Dim_1, z = ~Dim_2, type = \u0026#39;scatter3d\u0026#39;, mode = \u0026#39;lines\u0026#39;, line = list(color = \u0026#39;#1f77b4\u0026#39;, width = 1))  {\"x\":{\"visdat\":{\"82dd591c3b6d\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"82dd591c3b6d\",\"attrs\":{\"82dd591c3b6d\":{\"x\":{},\"y\":{},\"z\":{},\"mode\":\"lines\",\"line\":{\"color\":\"#1f77b4\",\"width\":1},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"step\"},\"yaxis\":{\"title\":\"Dim_1\"},\"zaxis\":{\"title\":\"Dim_2\"}},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"showSendToCloud\":false},\"data\":[{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400],\"y\":[0,-1,-2,-2,-2,-2,-1,-2,-2,-2,-1,0,1,0,-1,-1,0,0,1,1,1,1,2,3,3,3,3,4,4,3,2,3,4,3,4,4,5,4,4,5,6,5,5,4,5,5,4,3,4,4,4,4,4,3,3,3,3,2,3,2,3,2,3,3,3,4,4,4,4,4,5,5,4,4,4,4,5,5,4,4,3,3,2,2,3,4,5,6,6,5,6,6,6,6,5,5,5,4,4,5,4,5,5,5,5,5,6,6,6,5,5,5,5,4,4,5,6,6,6,5,5,6,6,6,6,6,7,7,7,8,9,8,9,10,10,10,11,11,11,10,11,12,11,10,10,10,9,9,9,8,7,6,6,7,7,8,8,7,8,7,7,8,7,7,7,6,6,7,7,8,8,8,9,10,11,11,11,10,11,12,13,13,12,12,12,13,13,14,14,14,13,14,15,15,14,13,13,13,13,14,15,14,15,14,13,13,12,12,12,13,13,13,13,13,14,13,13,13,13,13,13,13,13,12,12,12,13,13,13,13,13,13,14,13,12,12,12,12,12,12,12,13,13,13,13,13,13,12,12,12,12,12,12,11,11,11,10,11,11,10,10,10,10,10,11,10,10,11,12,12,12,12,12,13,12,13,13,14,15,15,15,16,17,16,16,16,16,16,16,17,17,16,15,16,16,17,18,18,19,18,18,19,18,18,18,18,19,19,19,19,20,20,20,20,19,19,19,20,20,21,20,21,22,21,22,22,22,22,21,20,20,20,20,20,20,20,19,19,19,20,20,21,21,21,22,23,22,21,20,20,20,20,20,20,21,20,21,20,21,22,22,23,23,23,24,24,23,23,23,23,23,23,24,24,24,25,25,25,25,26,25,25,25,25,25,26,25,25,25,25,24,23,23,23,22,22,22,22,22,22],\"z\":[1,1,1,2,3,4,4,4,5,4,4,4,4,4,4,3,3,4,4,3,4,3,3,3,2,1,2,2,1,1,1,1,1,1,1,2,2,2,1,1,1,1,2,2,2,1,1,1,1,2,1,0,-1,-1,0,1,2,2,2,2,2,2,2,3,4,4,5,6,5,6,6,5,5,4,5,6,6,5,5,6,6,5,5,6,6,6,6,6,7,7,7,8,7,8,8,9,10,10,11,11,11,11,12,13,14,15,15,14,13,13,12,11,12,12,11,11,11,12,11,11,10,10,9,10,9,8,8,9,8,8,8,8,8,8,9,10,10,9,10,10,10,10,10,10,9,8,8,7,6,6,6,6,7,7,6,6,7,7,7,7,8,8,8,7,8,8,7,7,6,6,5,6,6,6,6,5,4,4,4,4,4,5,5,6,5,5,4,4,5,6,6,6,6,5,5,5,4,3,4,4,4,4,4,4,4,5,5,6,5,5,4,5,4,3,3,3,2,1,0,-1,0,-1,0,0,-1,0,0,1,0,1,0,1,1,1,1,2,1,2,3,2,3,3,2,3,2,3,2,2,1,2,3,2,1,1,0,1,1,1,2,2,1,0,-1,-2,-2,-2,-1,-1,-1,-2,-3,-4,-3,-3,-3,-3,-4,-4,-4,-3,-4,-4,-4,-4,-3,-4,-5,-4,-5,-5,-4,-4,-4,-4,-5,-5,-5,-4,-4,-4,-3,-3,-3,-2,-1,-2,-2,-1,0,-1,-1,0,1,0,0,-1,-2,-2,-1,-1,-1,-1,-1,-1,-1,-2,-1,0,0,0,1,2,3,4,3,4,4,5,4,4,5,5,4,3,3,3,3,3,3,2,3,2,3,2,2,2,2,2,2,2,1,1,2,1,1,0,0,-1,-2,-1,0,1,1,0,1,1,0,-1,-2,-2,-2,-3,-4,-5,-4,-4,-4,-3,-4,-5,-5,-5,-4,-5,-5,-6,-5,-6,-5,-6],\"mode\":\"lines\",\"line\":{\"color\":\"#1f77b4\",\"width\":1},\"type\":\"scatter3d\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} Bo\\(^2\\)m =)\n ","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"2ff5d57d5f17678d5fbed58bd0215b6f","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-01-11/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/computational_notes/2018-01-11/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Random Walks","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A couple quick pieces of code to assist any time I need to work with many CSV files.\nInto List This first code chunk loads all of the CSV files in a folder, makes each into data frame, and stores each separately in a list.\nsetwd(\u0026quot;enter path\u0026quot;) # A character vector of every file name files \u0026lt;- Sys.glob(\u0026quot;*.csv\u0026quot;) # A list of all CSV files in the respective folder as data.frames myfiles \u0026lt;- lapply(files, FUN = read.csv) # To load any single data set... data_set1 \u0026lt;- myfiles[[1]]  Into Single Data Frame The code above stores each file into a list as a separate data frame. If I want to combine every CSV file into the same data frame I can do the following:\nsetwd(\u0026quot;enter path\u0026quot;) # A character vector of every file name files \u0026lt;- list.files(pattern = \u0026quot;*.csv\u0026quot;) # Now the full command data_set \u0026lt;- do.call(cbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE))) The code shown uses “cbind” so every variable within every CSV file will receive its own column in my “data_set.” If every CSV file has the same variable names replace “cbind” with “rbind.”\nBo\\(^2\\)m =)\n ","date":1514937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514937600,"objectID":"7636f662ed68d96d1e089c8126ceeb55","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-01-03/","publishdate":"2018-01-03T00:00:00Z","relpermalink":"/computational_notes/2018-01-03/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Combining CSV Files","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Here is a quick piece of code to create numeric response scores when data are read in as strings (e.g., “Strongly Agree, Agree, Neutral”).\nlibrary(tidyverse) library(dplyr) library(plyr) df \u0026lt;- read.csv(\u0026quot;path\u0026quot;) labels_to_values1 \u0026lt;- function(x){ mapvalues(x, from = c(\u0026quot;Strongly Agree\u0026quot;, \u0026quot;Agree\u0026quot;, \u0026quot;Slightly Agree\u0026quot;, \u0026quot;Slightly Disagree\u0026quot;, \u0026quot;Disagree\u0026quot;, \u0026quot;Strongly Disagree\u0026quot;), to = c(6,5,4,3,2,1)) } recode_df \u0026lt;- df %\u0026gt;% select(column_to_modify1, column_to_modify2, column_to_modify2, etc) %\u0026gt;% apply(2, FUN = labels_to_values1) %\u0026gt;% data.frame() Note that R will throw you warnings if all of the response options are not used, but the code will still work.\nBo\\(^2\\)m =)\n","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"3e158b5d574d2cf3f6982e80b9b9d349","permalink":"https://christopherdishop.netlify.app/computational_notes/2018-01-02/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/computational_notes/2018-01-02/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Formatting Qualtrics Responses","type":"Computational_Notes"},{"authors":null,"categories":null,"content":"  A bit of practice taking the first difference when the data is not consistent with a typical time-series structure.\nThe first set of data.\nlibrary(tidyverse) library(kableExtra) dff \u0026lt;- tibble( \u0026#39;id\u0026#39; = c(\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;c\u0026#39;), \u0026#39;survey\u0026#39; = c(1, 2, 1, 2, 1, 2), \u0026#39;score\u0026#39; = c(4, 4, 2, 4, 5, 2), \u0026#39;team\u0026#39; = c(\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;) ) dff %\u0026gt;% kable() %\u0026gt;% kable_styling()   id  survey  score  team      a  1  4  a    a  2  4  a    b  1  2  a    b  2  4  a    c  1  5  a    c  2  2  a     The goal is to subtract scores on the first survey from scores on the second survey. E.g., what are the change scores across the surveys for each participant?\ndff %\u0026gt;% group_by(id) %\u0026gt;% mutate(diffscore = score - lag(score)) ## # A tibble: 6 x 5 ## # Groups: id [3] ## id survey score team diffscore ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 a 1 4 a NA ## 2 a 2 4 a 0 ## 3 b 1 2 a NA ## 4 b 2 4 a 2 ## 5 c 1 5 a NA ## 6 c 2 2 a -3 The second set of data.\nscore \u0026lt;- c(10,30,14,20,6) group \u0026lt;- c(rep(1001,2),rep(1005,3)) df \u0026lt;- data.frame(score,group) df %\u0026gt;% kable() %\u0026gt;% kable_styling()   score  group      10  1001    30  1001    14  1005    20  1005    6  1005     Group 10001 has two scores whereas group 1005 has 3. I want the change from one score to another for each group.\ndf %\u0026gt;% group_by(group) %\u0026gt;% mutate(first_diff = score - lag(score)) ## # A tibble: 5 x 3 ## # Groups: group [2] ## score group first_diff ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 10 1001 NA ## 2 30 1001 20 ## 3 14 1005 NA ## 4 20 1005 6 ## 5 6 1005 -14 Bo\\(^2\\)m =)\n","date":1513987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513987200,"objectID":"95393902585db5858e48d30d58086263","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-12-23/","publishdate":"2017-12-23T00:00:00Z","relpermalink":"/computational_notes/2017-12-23/","section":"Computational_Notes","summary":"-----","tags":null,"title":"First Differencing By Group","type":"Computational_Notes"},{"authors":["Eric Feczko","Nadir Balba","Oscar Miranda-Dominguez","Michaela Cordova","Sarah Karalunas","Lourdes Irwin","Damion Demeter","","Beth Hoover Langhorst","Julia Grieser Painter","Jan van Santen","Eric Fombonne","Joel Nigg","Damien Fair"],"categories":null,"content":"","date":1513728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513728000,"objectID":"dbc47f036e165ec68800b8c89a360f76","permalink":"https://christopherdishop.netlify.app/publication/2017-subtyping/","publishdate":"2017-12-20T00:00:00Z","relpermalink":"/publication/2017-subtyping/","section":"publication","summary":"DSM-5 Autism Spectrum Disorder (ASD) comprises a set of neurodevelopmental disorders characterized by deficits in social communication and interaction and repetitive behaviors or restricted interests, and may both affect and be affected by multiple cognitive mechanisms. This study attempts to identify and characterize cognitive subtypes within the ASD population using a random forest (RF) machine learning classification model. We trained our model on measures from seven tasks that reflect multiple levels of information processing. 47 ASD diagnosed and 58 typically developing (TD) children between the ages of 9 and 13 participated in this study. Our RF model was 72.7% accurate, with 80.7% specificity and 63.1% sensitivity. Using the RF model, we measured the proximity of each subject to every other subject, generating a distance matrix between participants. This matrix was then used in a community detection algorithm to identify subgroups within the ASD and TD groups, revealing 3 ASD and 4 TD putative subgroups with unique behavioral profiles. We then examined differences in functional brain systems between diagnostic groups and putative subgroups using resting-state functional connectivity magnetic resonance imaging (rsfcMRI). Chi-square tests revealed a significantly greater number of between group differences (p ","tags":null,"title":"Subtyping cognitive profiles in Autism Spectrum Disorder using a random forest algorithm","type":"publication"},{"authors":["Yihui Xie","","Amber Thomas"],"categories":null,"content":"","date":1513209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513209600,"objectID":"e61ec371d9b93f524d235b910de6db2a","permalink":"https://christopherdishop.netlify.app/publication/2017-blogdown/","publishdate":"2017-12-14T00:00:00Z","relpermalink":"/publication/2017-blogdown/","section":"publication","summary":"blogdown: Creating Websites with R Markdown provides a practical guide for creating websites using the blogdown package in R. In this book, we show you how to use dynamic R Markdown documents to build static websites featuring R code (or other programming languages) with automatically rendered output such as graphics, tables, analysis results, and HTML widgets. The blogdown package is also suitable for technical writing with elements such as citations, footnotes, and LaTeX math. This makes blogdown an ideal platform for any website designed to communicate information about data science, data analysis, data visualization, or R programming. Note that blogdown is not just for blogging or sites about R; it can also be used to create general-purpose websites. By default, blogdown uses Hugo, a popular open-source static website generator, which provides a fast and flexible way to build your site content to be shared online. Other website generators like Jekyll and Hexo are also supported.","tags":null,"title":"blogdown: Creating Websites with R Markdown","type":"publication"},{"authors":null,"categories":null,"content":" A fun simulation by McClelland and Judd (1993) in Psychological Bulletin that demonstrates why detecting interactions outside the lab (i.e., in field studies) is difficult. In experiments, scores on the independent variables are located at the extremes of their respective distributions because we manipulate conditions. The distribution of scores across all of the independent variables in field studies, conversely, is typically assumed to be normal. By creating “extreme groups” in experiments, therefore, it becomes easier to detect interactions.\nImagine running an experiment where we randomly assign participants to one of two groups on an independent variable, goal difficulty. In one group the goal is challening, in the other group the goal is easy to accomplish. We are then interested in which group performs better on a task. After randomly assigning to groups, the distribution of scores on “goal difficulty” would be as follows:\nwhere 50 people are assigned to each condition. In this case, the distribution of scores is aligned at the extremes (i.e., -1, or the hard goal, and 1, or the easy goal) because we manipulated that variable. In field studies, where we cannot manipulate goal difficulty, the distribution of scores would be as follows:\nwhere scores about the independent variable (goal difficulty) are dispersed because we did not manipulate. The same distributional differences occur across other independent variables that we include in our design, and they are the reason behind fewer interaction detections in field studies.\nThe cool part is that this happens even when the data generating mechanisms are exactly the same. The mechanism that causes \\(y\\), in both the experiments and field studies in this simulation, will be:\n\\[\\begin{equation} y_{i} = b_0{i} + b_1{x_i} + b_2{z_i} + b_3{zx_i} + e_{i} \\end{equation}\\]\nwhere \\(y_{i}\\) is the value of the outcome (i.e., performance) for the \\(i^\\text{th}\\) person, \\(x_i\\) is the value of one independent variable for the \\(i^\\text{th}\\) person (i.e., goal difficulty), \\(z_i\\) is the value of another independent variable for the \\(i^\\text{th}\\) person (e.g., whatever variable you please), \\(zx_i\\) represents the combination of values on \\(x\\) and \\(z\\) for the \\(i^\\text{th}\\) person (i.e., the interaction term), \\(e_i\\) is a normally distributed error term for the \\(i^\\text{th}\\) person, and \\(b_0\\), \\(b_1\\), and \\(b_2\\) represent the regression intercept and coefficients relating the predictors to the outcome.\nAgain, the data generating equation, the thing that causes \\(y\\), is the same for both field studies and experiments. We are going to find differences, however, simply because the distribution on the independent variables are different.\nThe values for \\(b_0\\), \\(b_1\\), and \\(b_2\\) will be, respectively, 0, 0.20, 0.10, and 1.0 (see McClelland \u0026amp; Judd, 1993). In other words, our interaction coefficient is gigantic.\nEach simulation will use the equation just presented to generate data across 100 individuals in the field and 100 individuals in the lab. The only difference between the two groups will be their initial distribution on \\(x\\) and \\(z\\). For the lab group, their scores will be randomly assigned to -1 or 1, and in the field group scores will be randomly dispersed (normally) between -1 and 1. After generating the data I then estimate the coefficients using multiple regression and save the significance value in a vector. The process then interates 1000 times.\nThe Experiment Data The distribution of X:  The distribution of Z:  The distribution of Y after using the equation above to generate scores on Y: y_values \u0026lt;- b_0 + 0.20*x_values + 0.10*z_values + 1.00*x_values*z_values + rnorm(100,0,4) hist(y_values)  Now estimate the parameters using regression: exp_data \u0026lt;- data.frame(\u0026quot;X\u0026quot; = c(x_values), \u0026quot;Z\u0026quot; = c(z_values), \u0026quot;Y\u0026quot; = c(y_values)) exp_model \u0026lt;- lm(Y ~ X + Z + X:Z, data = exp_data) summary(exp_model) ## ## Call: ## lm(formula = Y ~ X + Z + X:Z, data = exp_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6044 -2.2962 0.1527 2.9004 8.3609 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.4735 0.3939 1.202 0.2323 ## X -0.7021 0.3939 -1.783 0.0778 . ## Z 0.6931 0.3939 1.760 0.0817 . ## X:Z 0.9612 0.3939 2.440 0.0165 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.907 on 96 degrees of freedom ## Multiple R-squared: 0.1039, Adjusted R-squared: 0.07593 ## F-statistic: 3.712 on 3 and 96 DF, p-value: 0.01417   The Field Study Data The distribution of X:  The distribution of Z:  The distribution of Y after using the equation above to generate scores on Y: f_y_values \u0026lt;- b_0 + 0.20*f_x_values + 0.10*f_z_values + 1.00*f_x_values*f_z_values + rnorm(100,0,4) hist(f_y_values)  Now estimate the parameters using regression: field_data \u0026lt;- data.frame(\u0026quot;FX\u0026quot; = c(f_x_values), \u0026quot;FZ\u0026quot; = c(f_z_values), \u0026quot;FY\u0026quot; = c(f_y_values)) field_model \u0026lt;- lm(FY ~ FX + FZ + FX:FZ, data = field_data) summary(field_model) ## ## Call: ## lm(formula = FY ~ FX + FZ + FX:FZ, data = field_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5347 -2.5822 -0.0993 3.0451 11.0879 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.4747 0.4032 -1.177 0.242 ## FX 0.6387 0.7902 0.808 0.421 ## FZ 0.1865 0.7932 0.235 0.815 ## FX:FZ 1.9542 1.5989 1.222 0.225 ## ## Residual standard error: 4.025 on 96 degrees of freedom ## Multiple R-squared: 0.02598, Adjusted R-squared: -0.004459 ## F-statistic: 0.8535 on 3 and 96 DF, p-value: 0.4681   Putting Everything Into Monte Carlo Replicate the process above 1000 times and save the p-value each time sims \u0026lt;- 1000 exp_results \u0026lt;- numeric(1000) field_results \u0026lt;- numeric(1000) X_coefficient \u0026lt;- 0.20 Z_coefficient \u0026lt;- 0.10 XZ_coefficient \u0026lt;- 1.00 Mu \u0026lt;- 0 xy_data \u0026lt;- c(-1,1) library(MASS) for(i in 1:sims){ # Experiment Data # X x_values \u0026lt;- sample(xy_data, 100, replace = T) # Z z_values \u0026lt;- sample(xy_data, 100, replace = T) # Y y_values \u0026lt;- Mu + X_coefficient * x_values + Z_coefficient * z_values + XZ_coefficient * x_values * z_values + rnorm(100,0,4) exp_data \u0026lt;- data.frame(\u0026quot;X\u0026quot; = c(x_values), \u0026quot;Z\u0026quot; = c(z_values), \u0026quot;Y\u0026quot; = c(y_values)) # Field Data # X f_x_values \u0026lt;- rnorm(100, 0, 0.5) # Z f_z_values \u0026lt;- rnorm(100, 0, 0.5) # Y f_y_values \u0026lt;- Mu + X_coefficient * f_x_values + Z_coefficient * f_z_values + XZ_coefficient * f_x_values * f_z_values + rnorm(100,0,4) field_data \u0026lt;- data.frame(\u0026quot;FX\u0026quot; = c(f_x_values), \u0026quot;FZ\u0026quot; = c(f_z_values), \u0026quot;FY\u0026quot; = c(f_y_values)) # Modeling exp_model \u0026lt;- lm(Y ~ X + Z + X:Z, data = exp_data) exp_results[i] \u0026lt;- summary(exp_model)$coefficients[4,4] field_model \u0026lt;- lm(FY ~ FX + FZ + FX:FZ, data = field_data) field_results[i] \u0026lt;- summary(field_model)$coefficients[4,4] }   The Results What proportion of experiments find significant interaction effects? sum(exp_results \u0026lt; 0.05) / 1000 ## [1] 0.672  What proportion of field studies find significant interaction effects? sum(field_results \u0026lt; 0.05) / 1000 ## [1] 0.082 Bo\\(^2\\)m =)\n  ","date":1510704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510704000,"objectID":"50e6a334745f1dc8653418278cf1f8e5","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-11-15/","publishdate":"2017-11-15T00:00:00Z","relpermalink":"/computational_notes/2017-11-15/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Why Detecting Interactions is Easier in the Lab","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" We can model the states of a system by applying a transition matrix to values represented in an initial distribution and repeating it until we reach an equilibrium.\nSuppose we want to model how job roles in a given company change over time. Let us assume the following:\n There are three (hierarchical) positions in the company:\n Analyst\n Project Coordinator\n Manager\n  30 new workers enter the company each year, and they all begin as analysts\n The probability of moving from …\n an analyst to a project coordinator is 75%\n a project coordinator to a manager is 8%\n  The probability of staying in a position is 25%\n The initial distribution of people in each role (analyst, PC, manager) is: c(45, 15, 6)\n  The Initial States: initial \u0026lt;- c(45, 15, 6)  The Transition Matrix: Consistent with the assumptions described above…\ntransition \u0026lt;- matrix(c( 0.25, 0.00, 30, 0.75, 0.25, 0.00, 0.00, 0.08, 0.25 ), 3, 3, byrow = T)  The Company Roles Over 50 Years: df \u0026lt;- matrix(, nrow = 50, ncol = 3) count \u0026lt;- 0 for(i in 1:50){ count \u0026lt;- count + 1 if(i == 1){ df[count,] = initial } else{ df[count,] = transition%^%i %*% initial } } If job-movement in a company aligned with our initial assumptions, we would expect the distribution of jobs to follow this pattern across time:\nSome data tidying first…\ndf \u0026lt;- data.frame(df) names(df) \u0026lt;- c(\u0026quot;Analyst\u0026quot;, \u0026quot;Project_Coordinator\u0026quot;, \u0026quot;Manager\u0026quot;) df$Time \u0026lt;- rep(1:nrow(df)) data_f \u0026lt;- df %\u0026gt;% gather(Analyst, Project_Coordinator, Manager, key = \u0026quot;Position\u0026quot;, value = \u0026quot;Num_People\u0026quot;) total_value \u0026lt;- data_f %\u0026gt;% group_by(Time) %\u0026gt;% summarise( total = sum(Num_People) ) data_f \u0026lt;- left_join(data_f, total_value) data_f \u0026lt;- data_f %\u0026gt;% mutate(Proportion = Num_People / total) The proportion of people in each position:\nlibrary(ggthemes) ggplot(data_f, aes(x = Time, y = Proportion, color = Position)) + geom_point() + geom_line() The amount of people in the company overall:\nggplot(data_f, aes(x = Time, y = Num_People, color = Position)) + geom_point() + geom_line() As you can tell, this is unrealistic =)\nBo\\(^2\\)m =)\n ","date":1503360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503360000,"objectID":"795c5fbe83692c378eb217d39c649518","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-08-22/","publishdate":"2017-08-22T00:00:00Z","relpermalink":"/computational_notes/2017-08-22/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Workforce Dynamics","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" I like to think of Monte Carlo as a counting method. If a condition is satisfied we make a note (e.g., 1), and if the condition is not satisfied we make a different note (e.g., 0). We then iterate and evaluate the pattern of 1’s and 0’s to learn about our process. Art can be described in a similar way: if a condition is satisfied we use a color, and if a condition is not satisfied we use a different color. After many iterations, we have an image.\nHere is a simulation that “draws” a process, inspired by Caleb Madrigal (link here).\nThe Data Generating Process f \u0026lt;- function(x){ 2*sin(4*x) + 2*sin(5*x) + 12 }  Some Initial Values x \u0026lt;- seq(0, 10, length.out = 1000)  Using the DGP to generate values of Y y \u0026lt;- f(x) plot(x, y)  This is the process we want to “draw”  Now for the Monte Carlo We are going to evaluate 10,000 points within our process space (10 x 16).\nnum_points \u0026lt;- 10000 rect_width \u0026lt;- 10 rect_height \u0026lt;- 16 points \u0026lt;- matrix(, ncol = 2, nrow = num_points) Column 1 of our points matrix represents the width of our process space while column 2 represents its height. First we fill the matrix with random values within our process space:\nfor(i in 1:num_points){ points[i,1] = runif(1, 0, rect_width) points[i,2] = runif(1, 0, rect_height) } Now we iterate across all of those points and evaluate them with respect to our process. Think of the “width” as X values and the “height” as Y values. Given a value of X, is our random value of Y less than it would be if we created a Y value by using our function (f(x))? If so, mark it in the “points_under” vector. If not, mark it in the “points_over” vector.\npoints_under = matrix(, ncol = 2, nrow = num_points) points_above = matrix(, ncol = 2, nrow = num_points) for(i in 1:num_points){ if(points[i,2] \u0026lt; f(points[i,1])){ points_under[i,1] \u0026lt;- points[i,1] points_under[i,2] \u0026lt;- points[i,2] } else{ points_above[i,1] \u0026lt;- points[i,1] points_above[i,2] \u0026lt;- points[i,2] } } Put the results into new vectors without NA’s. Some NA’s come up because our data generating process is crazy.\npoints_under_x \u0026lt;- points_under[!is.na(points_under[,1]),1] points_under_y \u0026lt;- points_under[!is.na(points_under[,2]),2] points_over_x \u0026lt;- points_above[!is.na(points_above[,1]),1] points_over_y \u0026lt;- points_above[!is.na(points_above[,2]),2] Now we have an image…\nplot(points_under_y ~ points_under_x, pch = 20, cex = 0.3) Bo\\(^2\\)m =)\n ","date":1499817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499817600,"objectID":"5d032fc221f67a76e692157787970f23","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-07-12-art/","publishdate":"2017-07-12T00:00:00Z","relpermalink":"/computational_notes/2017-07-12-art/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Art With Monte Carlo","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Some tidyverse commands I came across and hadn’t seen before. Thought it would be useful to store them here.\nReplace \u0026amp; Recode Replace missing values with the median.\ndf \u0026lt;- df %\u0026gt;% mutate(stress = replace(stress, is.na(stress), median(stress, na.rm = T))) Change a variable’s label.\ndf \u0026lt;- df %\u0026gt;% mutate(group = replace(group, group == \u0026quot;A\u0026quot;, \u0026quot;Group-A\u0026quot;)) Recode is a simple version of case_when.\ndf %\u0026gt;% mutate(color = recode(color, \u0026quot;g\u0026quot; = \u0026quot;green\u0026quot;, \u0026quot;b\u0026quot; = \u0026quot;blue\u0026quot;, \u0026quot;y\u0026quot; = \u0026quot;y\u0026quot;, .default = \u0026quot;other\u0026quot;))  An Alternative To Quosure calc \u0026lt;- function(data, group_var) { data %\u0026gt;% group_by({{ group_var }}) %\u0026gt;% summarize(mean = mean(stress)) } calc_m_sd \u0026lt;- function(data, mean_var, sd_var) { data %\u0026gt;% summarize( \u0026quot;mean_{{mean_var}}\u0026quot; := mean({{ mean_var }}), \u0026quot;sd_{{sd_var}}\u0026quot; := mean({{ sd_var }}) ) }  Using .data in a for-loop for (variable in names(df)) { df %\u0026gt;% count(.data[[variable]]) %\u0026gt;% print() }  Select a column if it’s row values have x df %\u0026gt;% select_if(is.numeric) %\u0026gt;% select_if(~mean(., na.rm=TRUE) \u0026gt; 10) df %\u0026gt;% select_all(any_vars(str_detect(., pattern = \u0026quot;Mu\u0026quot;)))  If with “is” At with \u0026quot;vars mutate_if(is.numeric) mutate_at(vars(contains(\u0026quot;Q\u0026quot;))) Bo\\(^2\\)m =)\n ","date":1498262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498262400,"objectID":"5b590e649947b72285525cb876818012","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-06-24/","publishdate":"2017-06-24T00:00:00Z","relpermalink":"/computational_notes/2017-06-24/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Tidyverse Randoms","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" A quick piece of code that reads a text file, changes something, saves a new text file, and iterates that process for every text file in that folder.\nsetwd(\u0026quot;path to the text files\u0026quot;) library(readr) all_files = Sys.glob(\u0026quot;*.txt\u0026quot;) for(i in 1:length(all_files)){ data = all_files[i] mystring = read_file(paste(data)) new_data = gsub(\u0026quot;old piece of text\u0026quot;, \u0026quot;new piece of text\u0026quot;, mystring) write_file(new_data, path = paste(\u0026quot;something\u0026quot;, code, \u0026quot;.txt\u0026quot;, sep = \u0026quot;\u0026quot;) } Bo\\(^2\\)m =)\n","date":1491696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491696000,"objectID":"a076a06b5dd9cd2ffcaf6f3c6d73fb99","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-04-09/","publishdate":"2017-04-09T00:00:00Z","relpermalink":"/computational_notes/2017-04-09/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Convert Text File","type":"Computational_Notes"},{"authors":null,"categories":null,"content":" Great discussion of extending a facet wrap across several pages.\nBo\\(^2\\)m =)\n","date":1491523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491523200,"objectID":"74a2d56d10572ce9c36878ad1f1d528d","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-04-07/","publishdate":"2017-04-07T00:00:00Z","relpermalink":"/computational_notes/2017-04-07/","section":"Computational_Notes","summary":"-----","tags":null,"title":"Facet Wrap Across Multiple Pages","type":"Computational_Notes"},{"authors":["Heather MacFarlane","Kyle Gorman","Rosemary Ingham","","Katina Papadakis","Geza Kiss","Jan van Santen"],"categories":null,"content":"","date":1489536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489536000,"objectID":"d0febf90eefe4dcc6d7064596e6aa73e","permalink":"https://christopherdishop.netlify.app/publication/2017-mazes-asd-sli/","publishdate":"2017-03-15T00:00:00Z","relpermalink":"/publication/2017-mazes-asd-sli/","section":"publication","summary":"Deficits in social communication, particularly pragmatic language, are characteristic of individuals with autism spectrum disorder (ASD). Speech disfluencies may serve pragmatic functions such as cueing speaking problems. Previous studies have found that speakers with ASD differ from typically developing (TD) speakers in the types and patterns of disfluencies they produce, but fail to provide sufficiently detailed characterizations of the methods used to categorize and quantify disfluency, making cross-study comparison difficult. In this study we propose a simple schema for classifying major disfluency types, and use this schema in an exploratory analysis of differences in disfluency rates and patterns among children with ASD compared to TD and language impaired (SLI) groups. 115 children ages 4–8 participated in the study (ASD = 51; SLI = 20; TD = 44), completing a battery of experimental tasks and assessments. Measures of morphological and syntactic complexity, as well as word and disfluency counts, were derived from transcripts of the Autism Diagnostic Observation Schedule (ADOS). High inter-annotator agreement was obtained with the use of the proposed schema. Analyses showed ASD children produced a higher ratio of content to filler disfluencies than TD children. Relative frequencies of repetitions, revisions, and false starts did not differ significantly between groups. TD children also produced more cued disfluencies than ASD children.","tags":null,"title":"Quantitative Analysis of Disfluency in Children with Autism Spectrum Disorder or Language Impairment","type":"publication"},{"authors":null,"categories":null,"content":" Effect sizes provide information about the magnitude of an effect. Unfortunately, they can be difficult to interpret or appear “small” to anyone unfamiliar with the typical effect sizes in a given research field. Rosenthal and Rubin (1992) provide an intuitive effect size, called the Binomial Effect Size Display, that captures the change in success rate due to a treatment.\nThe calculation is simple:\n Treamtment BESD = 0.50 + (r / 2)\n Control BESD = 0.50 - (r / 2)\n  where r is the correlation coefficient between treatment and survival (however defined). Many mathematical discussions exist, below is a simulation of one specific example by Randolph and Edmondson (2005). Please keep in mind the BESD is not without its critics (e.g., Thompson 1998).\nThe Example Aziothymidine (AZT) is used to treat AIDS, and the correlation between AZT use and survival is 0.23. Using the equations above, we can calculate the BESD for the treatment and control groups.\n# Survival AZT_survive \u0026lt;- 0.50 + (0.23 / 2) Placebo_survive \u0026lt;- 0.50 - (0.23 / 2) So the survival percentages for each group are:\nAZT_survive ## [1] 0.615 Placebo_survive ## [1] 0.385 Now we can simulate that process to see if our results match.\n The Simulation Preliminary set up:\nk \u0026lt;- 1000 percent_treatment_survive \u0026lt;- numeric(k) percent_control_survive \u0026lt;- numeric(k) # The correlation between AZT and survival is 0.23 Sigma \u0026lt;- matrix(c(1.0, 0.23, 0.23, 1.0), 2, 2, byrow = T) Running the process:\nfor(i in 1:k){ # Draws from a binomial distribution with 0.50 base rate # The correlation between both vectors is 0.23 # The first vector is treatment vs control assignment. # 1 = treatment ; 0 = control # The second vector is survive vs. not survive # 1 = survive ; 0 = not survive x \u0026lt;- rmvbin(5000, margprob = c(0.5, 0.5), bincorr = Sigma) x \u0026lt;- as.data.frame(x) # \u0026quot;Survive\u0026quot; is when column 2 is equal to 1 total_survive \u0026lt;- x %\u0026gt;% filter(V2 == 1) # The amount of people in each group that survived treatment_survive \u0026lt;- sum(total_survive$V1 == 1) / nrow(total_survive) control_survive \u0026lt;- sum(total_survive$V1 == 0) / nrow(total_survive) # Save the results from each iteration percent_treatment_survive[i] \u0026lt;- treatment_survive percent_control_survive[i] \u0026lt;- control_survive }  Comparison Our original calculations were as follows:\nAZT_survive ## [1] 0.615 Placebo_survive ## [1] 0.385 and here are the simulation results:\nmean(percent_treatment_survive) ## [1] 0.6158837 mean(percent_control_survive) ## [1] 0.3841163 Keep in mind the BESD assumes a 50/50 base rate of success (however defined) with no treatment.\nBo\\(^2\\)m =)\n ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"deb7ac9d4b372951036e57b9d2f52507","permalink":"https://christopherdishop.netlify.app/computational_notes/2017-01-01/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/computational_notes/2017-01-01/","section":"Computational_Notes","summary":"-----","tags":null,"title":"The Binomial Effect Size Display","type":"Computational_Notes"},{"authors":["","Katharine Zuckerman","Eric Fombonne"],"categories":null,"content":"","date":1455840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455840000,"objectID":"0b6a054d394b57b2726a4664b37d577a","permalink":"https://christopherdishop.netlify.app/publication/2016-primer-on-asd/","publishdate":"2016-02-19T00:00:00Z","relpermalink":"/publication/2016-primer-on-asd/","section":"publication","summary":"In this chapter, we review existing prevalence estimates for ASDs since 2000 and discuss methodological factors impacting the estimation of prevalence and the interpretation of changes in prevalence estimates over time. Possible explanations for an increase in the prevalence of ASD within and across populations are considered. Increases in ASD diagnostic rates cannot currently be attributed to a true increase in the incidence of ASD due to multiple confounding factors. It remains to be seen how changes to diagnostic criteria introduced in the DSM-5 will impact estimates of ASD prevalence going forward.","tags":null,"title":"Epidemiology of autism spectrum disorders","type":"publication"},{"authors":["Kyle Gorman","Lindsay Olson","","Rebecca Lunsford","Peter A. Heeman","Jan P. H. van Santen"],"categories":null,"content":"","date":1453420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453420800,"objectID":"09b001253620028175a8ded4912e712c","permalink":"https://christopherdishop.netlify.app/publication/2016-uh-and-um-asd-sli/","publishdate":"2016-01-22T00:00:00Z","relpermalink":"/publication/2016-uh-and-um-asd-sli/","section":"publication","summary":"Atypical pragmatic language is often present in individuals with autism spectrum disorders (ASD), along with delays or deficits in structural language. This study investigated the use of the “fillers” uh and um by children ages 4–8 during the autism diagnostic observation schedule. Fillers reflect speakers’ difficulties with planning and delivering speech, but they also serve communicative purposes, such as negotiating control of the floor or conveying uncertainty. We hypothesized that children with ASD would use different patterns of fillers compared to peers with typical development or with specific language impairment (SLI), reflecting differences in social ability and communicative intent. Regression analyses revealed that children in the ASD group were much less likely to use um than children in the other two groups. Filler use is an easy-to-quantify feature of behavior that, in concert with other observations, may help to distinguish ASD from SLI.","tags":null,"title":"Uh and Um in Children With Autism Spectrum Disorders or Language Impairment","type":"publication"},{"authors":["","Katharine E. Zuckerman","Eric Fombonne"],"categories":null,"content":"METHODS: Participants were 5053 children with confirmed diagnosis of ASD in the Autism Speaks Autism Treatment Network. Measured values for weight and height were used to calculate BMI percentiles; Centers for Disease Control and Prevention criteria for BMI for gender and age were used to define overweight and obesity (85th and 95th percentiles, respectively).\nRESULTS: In children age 2 to 17 years, 33.6% were overweight and 18% were obese. Compared with a general US population sample, rates of unhealthy weight were significantly higher among children with ASDs ages 2 to 5 years and among those of non-Hispanic white origin. Multivariate analyses revealed that older age, Hispanic or Latino ethnicity, lower parent education levels, and sleep and affective problems were all significant predictors of obesity.\nCONCLUSIONS: Our results indicate that the prevalence of unhealthy weight is significantly greater among children with ASD compared with the general population, with differences present as early as ages 2 to 5 years. Because obesity is more prevalent among older children in the general population, these findings raise the question of whether there are different trajectories of weight gain among children with ASDs, possibly beginning in early childhood.\n","date":1441238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441238400,"objectID":"159c707438c6bc3dd89b73a846e84007","permalink":"https://christopherdishop.netlify.app/publication/2015-obesity-in-asd-multisite/","publishdate":"2015-09-03T00:00:00Z","relpermalink":"/publication/2015-obesity-in-asd-multisite/","section":"publication","summary":"Overweight and obesity are increasingly prevalent in the general pediatric population. Evidence suggests that children with autism spectrum disorders (ASDs) may be at elevated risk for unhealthy weight. We identify the prevalence of overweight and obesity in a multisite clinical sample of children with ASDs and explore concurrent associations with variables identified as risk factors for unhealthy weight in the general population. ","tags":null,"title":"Obesity and Autism","type":"publication"},{"authors":["","Jan van Santen","Kyle Gorman","Beth Hoover Langhorst","Eric Fombonne"],"categories":null,"content":"","date":1434240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434240000,"objectID":"9b8a3141e9c30d4d6f012c4b05642832","permalink":"https://christopherdishop.netlify.app/publication/2015-memory-in-li-asd/","publishdate":"2015-06-14T00:00:00Z","relpermalink":"/publication/2015-memory-in-li-asd/","section":"publication","summary":"Background: A subgroup of young children with autism spectrum disorders (ASD) have significant language impairments (phonology, grammar, vocabulary), although such impairments are not considered to be core symptoms of and are not unique to ASD. Children with specific language impairment (SLI) display similar impairments in language. Given evidence for phenotypic and possibly etiologic overlap between SLI and ASD, it has been suggested that language-impaired children with ASD (ASD + language impairment, ALI) may be characterized as having both ASD and SLI. However, the extent to which the language phenotypes in SLI and ALI can be viewed as similar or different depends in part upon the age of the individuals studied. The purpose of the current study is to examine differences in memory abilities, specifically those that are key “markers” of heritable SLI, among young school-age children with SLI, ALI, and ALN (ASD + language normal). Methods: In this cross-sectional study, three groups of children between ages 5 and 8 years participated: SLI (n = 18), ALI (n = 22), and ALN (n = 20). A battery of cognitive, language, and ASD assessments was administered as well as a nonword repetition (NWR) test and measures of verbal memory, visual memory, and processing speed. Results: NWR difficulties were more severe in SLI than in ALI, with the largest effect sizes in response to nonwords with the shortest syllable lengths. Among children with ASD, NWR difficulties were not associated with the presence of impairments in multiple ASD domains, as reported previously. Verbal memory difficulties were present in both SLI and ALI groups relative to children with ALN. Performance on measures related to verbal but not visual memory or processing speed were significantly associated with the relative degree of language impairment in children with ASD, supporting the role of verbal memory difficulties in language impairments among early school-age children with ASD. Conclusions: The primary difference between children with SLI and ALI was in NWR performance, particularly in repeating two- and three-syllable nonwords, suggesting that shared difficulties in early language learning found in previous studies do not necessarily reflect the same underlying mechanisms.","tags":null,"title":"Memory in language-impaired children with and without autism","type":"publication"},{"authors":["","Katharine Zuckerman","Eric Fombonne"],"categories":null,"content":"","date":1433894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433894400,"objectID":"a74fffb3b5bb1b85c72e901b096bac30","permalink":"https://christopherdishop.netlify.app/publication/2015-translational/","publishdate":"2015-06-10T00:00:00Z","relpermalink":"/publication/2015-translational/","section":"publication","summary":"In this chapter, we review existing prevalence estimates for autism spectrum disorders (ASDs) since 2000 and discuss methodological factors impacting the estimation of prevalence and the interpretation of changes in prevalence estimates over time. Possible explanations for an increase in the prevalence of ASD within and across populations are considered. Increases in ASD diagnostic rates cannot currently be attributed to a true increase in the incidence of ASD due to multiple confounding factors. It remains to be seen how changes to diagnostic criteria introduced in the DSM-5 will impact estimates of ASD prevalence going forward.","tags":null,"title":"Epidemiology of autism spectrum disorders","type":"publication"},{"authors":["","Katherine E. Zuckerman","Arlene D. Hagen","Daniel J. Kriz","Susanne W. Duvall","Jan van Santen","Joel Nigg","Damien Fair","Eric Fombonne"],"categories":null,"content":"","date":1403395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1403395200,"objectID":"ba3062d9d35a8a913fc48e7637ea70fe","permalink":"https://christopherdishop.netlify.app/publication/2014-aggression-in-asd/","publishdate":"2014-06-22T00:00:00Z","relpermalink":"/publication/2014-aggression-in-asd/","section":"publication","summary":"Aggressive behavior problems (ABP) are frequent yet poorly understood in children with autism spectrum disorders (ASD) and are likely to co-vary significantly with comorbid problems. We examined the prevalence and sociodemographic correlates of ABP in a clinical sample of children with ASD (N = 400; 2–16.9 years). We also investigated whether children with ABP experience more intensive medical interventions, greater impairments in behavioral functioning, and more severe comorbid problems than children with ASD who do not have ABP. One in four children with ASD had Child Behavior Checklist scores on the Aggressive Behavior scale in the clinical range (T-scores = 70). Sociodemographic factors (age, gender, parent education, race, ethnicity) were unrelated to ABP status. The presence of ABP was significantly associated with increased use of psychotropic drugs and melatonin, lower cognitive functioning, lower ASD severity, and greater comorbid sleep, internalizing, and attention problems. In multivariate models, sleep, internalizing, and attention problems were most strongly associated with ABP. These comorbid problems may hold promise as targets for treatment to decrease aggressive behavior and proactively identify high-risk profiles for prevention.","tags":null,"title":"Aggressive Behavior Problems in Children with Autism Spectrum Disorders: Prevalence and Correlates in a Large Clinical Sample","type":"publication"},{"authors":["","Katharine Zuckerman","Eric Fombonne"],"categories":null,"content":"","date":1393804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393804800,"objectID":"27f62fe82863f321b72f90826a502f9e","permalink":"https://christopherdishop.netlify.app/publication/2014-handbook/","publishdate":"2014-03-03T00:00:00Z","relpermalink":"/publication/2014-handbook/","section":"publication","summary":"Since 1966, over 80 epidemiological surveys of autism spectrum disorders (ASDs) have been conducted in more than 20 countries. In this chapter, we review existing prevalence estimates for ASDs and discuss methodological factors impacting the estimation of prevalence and the interpretation of changes in prevalence estimates over time. Possible explanations for an increase in the prevalence of ASD within and across populations are considered. Increases in ASD diagnostic rates cannot currently be attributed to a true increase in the incidence of ASD due to multiple confounding factors. It remains to be seen how changes to diagnostic criteria introduced in the DSM-5 will impact estimates of ASD prevalence going forward.","tags":null,"title":"Epidemiology of autism spectrum disorders","type":"publication"},{"authors":["Katharine E. Zuckerman","","Kimberly Guion","Lisa Voltolina","Eric Fombonne"],"categories":null,"content":"","date":1391299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391299200,"objectID":"ef04d4e41554e20028cbe353dc2912ab","permalink":"https://christopherdishop.netlify.app/publication/2014-obesity-in-asd-oregon/","publishdate":"2014-02-02T00:00:00Z","relpermalink":"/publication/2014-obesity-in-asd-oregon/","section":"publication","summary":"Autism Spectrum Disorders (ASDs) and childhood obesity (OBY) are rising public health concerns. This study aimed to evaluate the prevalence of overweight (OWT) and OBY in a sample of 376 Oregon children with ASD, and to assess correlates of OWT and OBY in this sample. We used descriptive statistics, bivariate, and focused multivariate analyses to determine whether socio-demographic characteristics, ASD symptoms, ASD cognitive and adaptive functioning, behavioral problems, and treatments for ASD were associated with OWT and OBY in ASD. Overall 18.1 % of children met criteria for OWT and 17.0 % met criteria for OBY. OBY was associated with sleep difficulties, melatonin use, and affective problems. Interventions that consider unique needs of children with ASD may hold promise for improving weight status among children with ASD.","tags":null,"title":"Overweight and Obesity: Prevalence and Correlates in a Large Clinical Sample of Children with Autism Spectrum Disorder","type":"publication"},{"authors":["","Eric Fombonne"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"8a79d2607d7d3103cc12100012e71247","permalink":"https://christopherdishop.netlify.app/publication/2014-asd-epidemiology-in-practice/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/2014-asd-epidemiology-in-practice/","section":"publication","summary":"In this selective review of the literature, we present the most recent prevalence estimates for autism spectrum disorder (ASD) and discuss the limitations and challenges in interpreting changes in prevalence estimates over time. Increases in ASD prevalence estimates cannot currently be attributed to a true increase in the incidence of ASD due to multiple confounding factors. These include broader diagnostic criteria and a greater awareness of ASD. The current average prevalence of ASD is approximately 66/10,000, which translates to approximately 1 in 152 children a ected, with males consistently outnumbering females by about 5:1. Several recent studies have reported higher estimates ranging from 147 (one in 68) to 264 (one in 38) per 10,000. This is in sharp contrast to the  gures of about 1-5/10,000 quoted in earlier studies that used a narrow de nition of autistic disorder and were not inclusive of all disorders falling onto the autism spectrum. It remains to be seen how changes to diagnostic criteria introduced in the DSM-5 will impact estimates of ASD prevalence.","tags":null,"title":"Epidemiology of Autism Spectrum Disorder","type":"publication"},{"authors":["Jan P. H. van Santen","Richard W. Sproat",""],"categories":null,"content":"","date":1365724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1365724800,"objectID":"053861617f3b4a9dad6296b5cbe8ea21","permalink":"https://christopherdishop.netlify.app/publication/2013-repetitive-speech-in-asd-sli/","publishdate":"2013-04-12T00:00:00Z","relpermalink":"/publication/2013-repetitive-speech-in-asd-sli/","section":"publication","summary":"We report on an automatic technique for quantifying two types of repetitive speech: repetitions of what the child says him/herself (self-repeats) and of what is uttered by an interlocutor (echolalia). We apply this technique to a sample of 111 children between the ages of four and eight: 42 typically developing children (TD), 19 children with specific language impairment (SLI), 25 children with autism spectrum disorders (ASD) plus language impairment (ALI), and 25 children with ASD with normal, non-impaired language (ALN). The results indicate robust differences in echolalia between the TD and ASD groups as a whole (ALN + ALI), and between TD and ALN children. There were no significant differences between ALI and SLI children for echolalia or self-repetitions. The results confirm previous findings that children with ASD repeat the language of others more than other populations of children. On the other hand, self-repetition does not appear to be significantly more frequent in ASD, nor does it matter whether the child’s echolalia occurred within one (immediate) or two turns (near-immediate) of the adult’s original utterance. Furthermore, non-significant differences between ALN and SLI, between TD and SLI, and between ALI and TD are suggestive that echolalia may not be specific to ALN or to ASD in general. One important innovation of this work is an objective fully automatic technique for assessing the amount of repetition in a transcript of a child’s utterances.","tags":null,"title":"Quantifying Repetitive Speech in Autism Spectrum Disorders and Language Impairment","type":"publication"},{"authors":["Kristina M. Zosuls","Carol Lynn Martin","Diane N. Ruble","Cindy F. Miller","Bridget M. Gaertner","Dawn E. England",""],"categories":null,"content":"","date":1297123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1297123200,"objectID":"4fc09d6b6c9a5931644746dad6d8415a","permalink":"https://christopherdishop.netlify.app/publication/2011-gender-attitudes/","publishdate":"2011-02-08T00:00:00Z","relpermalink":"/publication/2011-gender-attitudes/","section":"publication","summary":"Widespread gender segregation, evident throughout elementary school, seems to imply that girls and boys have negative feelings and thoughts about one another, and classic theories of inter-group processes support this idea. However, research has generally overlooked children’s feelings and perceptions about gender-related interpersonal interactions. This paper investigates the nature of children’s attitudes about same- and other-gender peers, and explores how those attitudes relate to the expectancies and beliefs children hold about same- and other-gender peer interactions. Children (N = 98 fifth graders) completed questionnaires assessing their global liking of own- and other-gender peers (Yee \u0026 Brown, 1994), positive and negative attitudes about own- and other-gender peers, and outcome expectancies related to interacting with own- and other-gender peers. Results indicated that rather than being characterized by out-group negativity, children’s inter-group gender attitudes are best characterized by an in-group positivity bias. Children’s positive and negative affective attitudes were also significantly associated with outcome expectancies. In contrast, global liking of own- and other- gender peers was less predictive of outcome expectancies. Thus, the greater specificity of the affective attitude measures appeared to be a more predictive and potentially fruitful gauge of children’s feelings about own- and other-gender peers. Results are discussed in terms of the need for finer grained and more extensive studies of children’s gender-related feelings and cognitions about own- and other-gender peers.","tags":null,"title":"‘It’s Not That We Hate You’: Understanding Children’s Gender Attitudes and Expectancies About Peer Relationships","type":"publication"},{"authors":["","Tedra A. Walden","Wendy L. Stone","Paul J. Yoder"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"10ead64306e34e2ab8a84ae656109223","permalink":"https://christopherdishop.netlify.app/publication/2007-rja-in-sibs/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/2007-rja-in-sibs/","section":"publication","summary":"We compared responding to joint attention (RJA) in younger siblings of children with ASD (SIBS- ASD; n = 46) and younger siblings of children developing typically (SIBS-TD; n = 35). Children were tested between 12 and 23 months of age in a situation in which an experimenter directed the child’s attention to one of 8 targets. Each child responded to 10 different combinations of verbal and nonverbal cues containing varying levels of attention-specifying information. SIBS-ASD had significantly lower overall RJA scores than SIBS-TD. Moderately redundant cues were most difficult for SIBS-ASD relative to SIBS-TD; adding a point to moderately redundant cues improved RJA for SIBS-ASD, bringing them to a level of RJA commensurate with SIBS-TD.","tags":null,"title":"Effects of Different Attentional Cues on Responding to Joint Attention in Younger Siblings of Children with Autism Spectrum Disorders","type":"publication"},{"authors":["David P. McCabe","","Chuck L. Robertson","Anderson D. Smith"],"categories":null,"content":"","date":1101859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1101859200,"objectID":"50e433a91dedcaad69f93848c53ab186","permalink":"https://christopherdishop.netlify.app/publication/2004-false-memories/","publishdate":"2004-12-01T00:00:00Z","relpermalink":"/publication/2004-false-memories/","section":"publication","summary":"We examined the effect of item-specific and relational encoding instructions on false recognition in two experiments in which the DRM paradigm was used (Deese, 1959; Roediger \u0026 McDermott, 1995). Type of encoding (item-specific or relational) was manipulated between subjects in Experiment 1 and within subjects in Experiment 2. Decision-based explanations (e.g., the distinctiveness heuristic) predict reductions in false recognition in between-subjects designs, but not in within-subjects designs, because they are conceptualized as global shifts in decision criteria. Memory-based explanations predict reductions in false recognition in both designs, resulting from enhanced recollection of item-specific details. False recognition was reduced following item-specific encoding instructions in both experiments, favoring a memory-based explanation. These results suggest that providing unique cues for the retrieval of individual studied items results in enhanced discrimination between those studied items and critical lures. Conversely, enhancing the similarity of studied items results in poor discrimination among items within a particular list theme. These results are discussed in terms of the item-specific/relational framework (Hunt \u0026 McDaniel, 1993).","tags":null,"title":"Item-specific Processing Reduces False Memories","type":"publication"},{"authors":null,"categories":null,"content":"Websites with many R resources  All R books written with Bookdown\n All R cheatsheets\n APS list of R resources\n Show Us Your R\n Econometrics Simulations\n UVA Data Science Resources\nPeople with helpful individual websites  Emily C. Zabor\n Jenny Bryan\n Kristoffer Magnusson\n Sacha Epskamp\n Lisa DeBruine\n FOM academic\n Suzan Baert\n Hadley Wickham\n David Robinson\n B Rodriguez\n Allie Choate\n Nathaniel D. Phillips\n Simon Ejdemyr\n The R Graph Gallery\n Christian Burkhart\n Malcolm Barrett\n John Flournoy\n UC R Guide\n Crump Lab\n Danielle Navarro\n Richard N. Landers\n Gaston Sanchez\n Mark Lai\n Joshua Loftus\n Francis L. Huang\nCourses  Applied Statistical Computing Course\n BioStates by Jarrett Byrnes\n Introduction to Bayes in DataCamp (free course)\n Data Analysis and Statistical Inference on Data Camp\n Probability and Statistics with Simulations\n Penn State Quant Developmental Systems\nRandom  R Markdown Themes\n R club\n Various Linear Model Tests in R\n ANOVA and MANOVA\n Using R to extract data from web APIs\n Handling date-times in R\n Lesser Known Dplyr tricks\n Rmarkdown\n Packages for extremely messy data\n Power Simulations\n Functions with columns as parameters 1\n Functions with columns as parameters 2\n Kable Extra\n Data.Table\n Growth models with lme and lmer\n Cross-lagged Panel Models\n Simulate Experiments\n Maximum Likelihood\n citr\n Purrr tutorial\n Psych Networks Winter School Resources\n Rmarkdown Themes 1\n Rmarkdown Themes 2\n Rmarkdown Themes 3\n Top 50 ggplot2 Visualizations\nDAGS  Dags using LaTex\n QuickDag\n DiagrammeR\n GGdag\n All DAGS from Hernan and Robins - Sam Finlayson\nStructural Equations Modeling  lavaan homepage\n Measurement Equivalence\n sims package\n Draw SEMs\n onyxR for SEM diagrams with lavaan\nProgressing Through Stats with R  Basics of R and R Markdown\n Basics of Git and Github with R\n Beginner Concepts for Reproducible Science\n Learning Statistics with R\n R for Data Science\n Beginning Computer Science with R\n Statistical Inference\n Intro to Quant Methods\n Introduction to Data Science\n Introduction to Computational Data for Biology\n Regression\n Econometrics\n Time series and forecasting\n Applied time series\n Advanced time series\n Data Visualization\n Statistical ReThinking with Tidyverse\n Text Analysis\nScraping Packages  Scrape Glass Door\nCommands and help for non-R programming  Compress pdf in terminal\n Mac denying pushing/pulling due to git keychain\n Saving username and password through keychain with Git\n Using word with Git\n Line numbering in LaTex\n Exporting .tex to word with pandoc\n Word Count on PDF From Terminal\nSo helpful they deserve their own category  Math notation with Rmarkdown\n BibTex Examples\n Latex equations basics\n Compile Rmarkdown and tex files from terminal\n Include LaTeX packages in YAML header\n Pimp Your Rmarkdown\n Rebus Package\n Newline breaks in Rmarkdown tables\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"afc60fade2d3c70880ffc2df20eb13e5","permalink":"https://christopherdishop.netlify.app/r_resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/r_resources/","section":"","summary":"Websites with many R resources  All R books written with Bookdown","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":" \u0026ldquo;It\u0026rsquo;s what you read when you don\u0026rsquo;t have to that determines who you will be when you can\u0026rsquo;t help it.\u0026rdquo; - Oscar Wilde\n    Quantifying Life: A Symbiosis of Computation, Mathematics, and Biology\n Dmitry A. Kondrashov\n Click here for link\n   On Writing Well: The Classic Guide to Writing Nonfiction\n William Zinsser\n Click here for link\n   How We Know What Isn\u0026rsquo;t So: The Fallibility of Human Reason in Everyday Life\n Thomas Gilovich\n Click here for link\n   Complexity: A Guided Tour\n Melanie Mitchell\n Click here for link\n   The Drunkard\u0026rsquo;s Walk: How Randomness Rules Our Lives\n Leonard Mlodinow\n Click here for link\n   The Order Of Time\n Carlo Rovelli\n Click here for link\n   Style: Lessons in Clarity and Grace\n John M Williams\nJoseph Bizup\n Click here for link\n   Ethics in the Real World: 82 Brief Essays on Things That Matter  Peter Singer\n Click here for link\n   The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century  David Salsburg\n Click here for link\n   Why Don't Students Like School?  Daniel T. Willingham\n Click here for link\n   A Crude Look at the Whole: The Science of Complex Systems in Business, Life, and Society  John H. Miller\n Click here for link\n   Reclaiming Conversation  Sherry Turkle\n Click here for link\n   Humble Pie: A Comedy of Maths Errors  Matt Parker\n Click here for link\n   Scale: The Universal Laws of Life, Growth, and Death in Organisms, Cities, and Companies  Geoffrey West\n Click here for link\n   The Most Good You Can Do  Peter Singer\n Click here for link\n   Reproducible Research with R and R Studio  Christopher Gandrud\n Click here for link\n   Stiff: The Curious Lives of Human Cadavers  Mary Roach\n Click here for link\n   Vehicles: Experiments in Synthetic Psychology  Valentino Braitenberg\n Click here for link\n   The Lessons of History  Will Durant\nAriel Durant\n Click here for link\n   The Death of Expertise: The Campaign against Established Knowledge and Why it Matters  Thomas M Nichols\n Click here for link\n   The Book of Why  Judea Pearl\nDana Mackenzie\n Click here for link\n   Naked Statistics: Stripping The Dread From The Data  Charles Wheelan\n Click here for link\n   The Ball: Discovering the Object of the Game  John Fox\n Click here for link\n   The Life You Can Save  Peter Singer\n Click here for link\n   The Nature of Code: Simulating Natural Systems with Processing  Daniel Shiffman\n Click here for link\n   SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed  Martin A. Nowak\nRoger Highfield\n Click here for link\n   Mathematics for the Life Sciences  Erin N. Bodine\nSuzanne Lenhart\nLouis J. Gross\n Click here for link\n   Nature's Nether Regions: What the Sex Lives of Bugs, Birds, and Beasts Tell Us About Evolution, Biodiversity, and Ourselves  Menno Schilthuizen\n Click here for link\n   Algorithms To Live By: The Computer Science Of Human Decisions  Brian Christian\n Click here for link\n   Infinite Powers  Steve Strogatz\n Click here for link\n   The Invisible Gorilla: How Our Intuitions Deceive Us  Christopher Chabris\nDaniel Simons\n Click here for link\n   Mindwise: How We Understand What Others Think, Believe, Feel, and Want  Nicholas Epley\n Click here for link\n   This Will Change Everything: Ideas That Will Shape The Future  Edited By John Brockman\n Click here for link\n   Sex At Dawn: How We Mate, Why We Stay, and What It Means for Modern Relationships  Christopher Ryan\nCacilda Jetha\n Click here for link\n   Can Medicine Be Cured? The Corruption of a Profession  Seamus O\u0026rsquo;Mahony\n Click here for link\n   Math with Bad Drawings: Illuminating the Ideas That Shape Our Reality  Ben Orlin\n Click here for link\n   How We Got to Now: Six Innovations That Made the Modern World  Steven Johnson\n Click here for link\n   Made to Stick: Why Some Ideas Survive and Others Die  Chip Heath\nDan Heath\n Click here for link\n   The Ravenous Brain: How The New Science of Consciousness Explains Our Insatiable Search For Meaning  Daniel Bor\n Click here for link\n   Matrices and Society: Matrix Algebra and Its Applications in the Social Sciences  Ian Bradley\nRonald L. Meek\n Click here for link\n   The Beginning Of Infinity: Explanations That Transform The World  David Deutsch\n Click here for link\n   The Visual Display of Quantitative Information  Edward R. Tufte\n Click here for link\n   Complex Adaptive Systems: An Introduction to Computational Models of Social Life  John H. Miller\nScott E. Page\n Click here for link\n   Homo Deus: A Brief History of Tomorrow  Yuval Noah Harari\n Click here for link\n   The Moral Landscape: How Science Can Determine Human Values  Sam Harris\n Click here for link\n   Sick Societies: Challenging the Myth of Primitive Harmony  Robert B. Edgerton\n Click here for link\n   The Expanding Circle: Ethics, Evolution, and Moral Progress  Peter Singer\n Click here for link\n   The Checklist Manifesto: How to Get Things Right  Atul Gawande\n Click here for link\n   The Joy of x: A Guided Tour of Math, from One to Infinity  Steven Strogatz\n Click here for link\n   10 1/2 Things No Commencement Speaker Has Ever Said  Charles Wheelan\n Click here for link\n   Statistical Rethinking: A Bayesian Course with Examples in R and Stan  Richard McElreath\n Click here for link\n   Reflections on the Human Condition  Eric Hoffer\n Click here for link\n   Thinking with Data: How to Turn Information into Insights  Max Shron\n Click here for link\n   Switch: How To Change Things When Change Is Hard  Chip Heath\nDan Heath\n Click here for link\n   Mathematical Modeling of Social Relationships  Urszula Strawinska-Zanko\nLarry S. Liebovitch\n Click here for link\n   The Seven Pillars of Statistical Wisdom  Stephen M. Stigler\n Click here for link\n   Life's Other Secret: The New Mathematics of the Living World  Ian Stewart\n Click here for link\n   What Should We Be Worried About? Real Scenarios That Keep Scientists Up at Night  Edited by John Brockman\n Click here for link\n   Introduction To Scientific Programming and Simulation Using R  Owen Jones\nRobert Maillardet\nAndrew Robinson\n Click here for link\n   Stumbling on Happiness  Daniel Gilbert\n Click here for link\n   Brain Bugs: How The Brain's Flaws Shape Our Lives  Dean Buonomano\n Click here for link\n   Sync  Steven H. Strogatz\n Click here for link\n   This Idea Is Brilliant  Edited by John Brockman\n Click here for link\n   How to Do Nothing: Resisting the Attention Economy  Jenny Odell\n Click here for link\n   How Not to Be Wrong: The Power of Mathematical Thinking  Jordan Ellenberg\n Click here for link\n   Introduction to Computation and Programming Using Python  John V. Guttag\n Click here for link\n   Cooperation Under Anarchy  Kenneth A. Oye\n Click here for link\n   Predictably Irrational: The Hidden Forces That Shape Our Decisions  Dan Ariely\n Click here for link\n   The Passionate State of Mind  Eric Hoffer\n Click here for link\n   Big Data: A Revolution That Will Transform How We Live, Work, and Think  Viktor Mayer-Schonberger\nKenneth Cukier\n Click here for link\n   An Introduction to Models in the Social Sciences  Charles A. Lave\nJames G. March\n Click here for link\n   In Pursuit of the Unknown: 17 Equations That Changed the World  Ian Stewart\n Click here for link\n   Sapiens: A Brief History of Humankind  Yuval Noah Harari\n Click here for link\n   The Greatest Minds and Ideas of All Time  Will Durant\n Click here for link\n   This Blinding Absence of Light  Tahar Ben Jelloun\n Click here for link\n   Working and Thinking on the Waterfront  Eric Hoffer\n Click here for link\n   The Art of Thinking Clearly  Rolf Dobelli\n Click here for link\n   Retire Before Mom and Dad: The Simple Numbers Behind A Lifetime of Financial Freedom  Rob Berger\n Click here for link\n   7 Tipping Points That Saved the World  Chris Stewart\nTed Stewart\n Click here for link\n   Digital Minimalism  Cal Newport\n Click here for link\n   The Tyranny of Metrics  Jerry Muller\n Click here for link\n   The Model Thinker: What You Need to Know to Make Data Work for You  Scott E. Page\n Click here for link\n   The Briefest History of Time  Arieh Ben_Naim\n Click here for link\n   Too Soon Old, Too Late Smart  Gordon Livingston\n Click here for link\n   System Dynamics Modeling with R  Jim Duggan\n Click here for link\n   Ten Great Ideas About Chance  Persi Diaconis\nBrian Skyrms\n Click here for link\n   Risk Savvy: How To Make Good Decisions  Gerd Gigerenzer\n Click here for link\n   Thinking in Systems: A Primer  Donella H. Meadows\n Click here for link\n   This Idea Must Die: Scientific Theories That Are Blocking Progress  Edited by John Brockman\n Click here for link\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f923bf68db3f15ecf28726ee0a138a27","permalink":"https://christopherdishop.netlify.app/rec_reading/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/rec_reading/","section":"","summary":"\u0026ldquo;It\u0026rsquo;s what you read when you don\u0026rsquo;t have to that determines who you will be when you can\u0026rsquo;t help it.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Download this file here.\nThis browser does not support PDFs. Please download the PDF to view it: Download PDF.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://christopherdishop.netlify.app/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Download this file here.","tags":null,"title":"Curriculum Vitae","type":"page"}]