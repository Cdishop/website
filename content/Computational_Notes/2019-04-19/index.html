---
title: "Everything Partialled From Everything in Regression"
date: '2019-04-19'
summary: "-----"
---



<p>In regression, everything is partialled from everything. Let’s work through that notion with images and code. Imagine that emotion and ability cause an outcome, <span class="math inline">\(Y\)</span>.</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_variance.png" width="500" /></p>
<p>What this image represents is that <span class="math inline">\(Y\)</span> has variability (across people or time), and its variability is associated with variability in emotion and variability in ability. Notice that there is variability overlap between ability and <span class="math inline">\(Y\)</span>,</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here1.png" width="500" /></p>
<p>emotion and <span class="math inline">\(Y\)</span>,</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here2.png" width="500" /></p>
<p>emotion and ability,</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here3.png" width="500" /></p>
<p>and all three variables.</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here4.png" width="500" /></p>
<p>Once we regress <span class="math inline">\(Y\)</span> on emotion and ability, the regression coefficients represent the unique variance components of each predictor</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_coefficients.png" width="500" /></p>
<p>but the technique also removes outcome-relevant variance</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_no_middle.png" width="500" /></p>
<p>and overlapping variance in emotion and ability not related to the outcome.</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_full_partial.png" width="500" /></p>
<p>So, in regression we get coefficients that represent the unique variance contribution of each predictor while partialling overlapping, outcome-relevant variance and overlapping, non-relevant variance. Emotion and ability get to account for their own causal effects of <span class="math inline">\(Y\)</span>, but neither predictor gets the overlapping variance in <span class="math inline">\(Y\)</span>, and the emotion and ability coefficients are adjusted for the emotion-ability overlap situated outside <span class="math inline">\(Y\)</span>.</p>
<p>Let’s do it with code.</p>
<p>Our sample contains 500 people with correlated emotion and ability (<span class="math inline">\(r\)</span> = 0.4).</p>
<pre class="r"><code>people &lt;- 500
emotion &lt;- rnorm(people, 0, 10)
ability &lt;- 0.4*emotion + rnorm(people, 0, 1) # could also do it with MASS</code></pre>
<p>Ability and emotion cause <span class="math inline">\(Y\)</span>.</p>
<pre class="r"><code>error &lt;- rnorm(people, 0, 1)
Y &lt;- 2 + 0.5*ability + 0.38*emotion + error</code></pre>
<p>Regression will recover the parameters.</p>
<pre class="r"><code>df &lt;- data.frame(
  &#39;emotion&#39; = c(emotion),
  &#39;ability&#39; = c(ability),
  &#39;y&#39; = c(Y)
)

summary(lm(y ~ ability + emotion,
           data = df))$coefficients[,1]</code></pre>
<pre><code>## (Intercept)     ability     emotion 
##   1.9531540   0.4584228   0.4002376</code></pre>
<p>Remember, each coefficient is consistent with the “lightning bolt” variance components above. Outcome-relevant overlap is removed and overlap between emotion and ability is removed. Since emotion and ability are partialled from each other, we won’t recover the 0.38 parameter relating emotion to <span class="math inline">\(Y\)</span> if we remove ability from the equation.</p>
<pre class="r"><code>summary(lm(y ~ emotion,
           data = df))$coefficients[,1]</code></pre>
<pre><code>## (Intercept)     emotion 
##   1.9648929   0.5861932</code></pre>
<p>How can we modify our variables to represent the “partialled multiple regression coefficient” for emotion? Naively, it seems that if we remove ability from <span class="math inline">\(Y\)</span> and then regress <span class="math inline">\(Y\)</span> on emotion we will recover the appropriate 0.38 parameter. Let’s try.</p>
<p>Regress <span class="math inline">\(Y\)</span> on just ability</p>
<pre class="r"><code>just_ability &lt;- lm(y ~ ability,
               data = df)</code></pre>
<p>and take the residuals, meaning that in our next regression we will examine the effect of emotion on “leftover <span class="math inline">\(Y\)</span>” – <span class="math inline">\(Y\)</span> with no influence from ability.</p>
<pre class="r"><code>y_with_ability_removed &lt;- resid(just_ability)
df$y_with_ability_removed &lt;- y_with_ability_removed

summary(lm(y_with_ability_removed ~ emotion,
           data = df))$coefficients[,1]</code></pre>
<pre><code>## (Intercept)     emotion 
##  0.01927374  0.02051948</code></pre>
<p>Nope. Why not? Think back to the diagrams, what we just assessed was</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/remove_ability.png" width="500" /></p>
<p>where the estimate accounts for the <span class="math inline">\(Y\)</span>-relevant overlap of emotion and ability, but it is wrong because it doesn’t account for the overlap between emotion and ability situated outside of <span class="math inline">\(Y\)</span>. In regression, everything is partialled from everything…we have not yet accounted for the overlap between emotion and ability in the space not in the <span class="math inline">\(Y\)</span> variance sphere. Now we will.</p>
<p>Partial ability from emotion</p>
<pre class="r"><code>emotion_with_ability_removed &lt;- resid(lm(emotion ~ ability,
                                         data = df))

df$emotion_with_ability_removed &lt;- emotion_with_ability_removed</code></pre>
<p>and now when we regress “Y with ability removed” on “emotion with ability removed” we will recover the 0.38 parameter.</p>
<pre class="r"><code>summary(lm(y_with_ability_removed ~ emotion_with_ability_removed,
           data = df))$coefficients[,1]</code></pre>
<pre><code>##                  (Intercept) emotion_with_ability_removed 
##                -7.545488e-17                 4.002376e-01</code></pre>
<p>In regression, everything is partialled from everything.</p>
<p><img src="https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_full_partial.png" width="500" /></p>
<p>The technique partials overlapping predictor variance both within and outside of the <span class="math inline">\(Y\)</span> space. Neither predictor accounts for overlapping variance within <span class="math inline">\(Y\)</span>, and if an important predictor is excluded then it will artificially account for variance it shouldn’t be capturing.</p>
<p>Note that all of this is relevant for III sums of squares…there are other approaches but III is by far the most common.</p>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
