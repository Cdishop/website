---
title: "Regression Creates Weighted Linear Composites"
date: "2019-12-01"
summary: "-----"
---



<p>One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.</p>
<p>Here’s a simulation to punch that point home.</p>
<p>500 people.</p>
<pre class="r"><code>N &lt;- 500</code></pre>
<p>The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.</p>
<pre class="r"><code>sigma &lt;- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)</code></pre>
<p>The mean for each variable is 0.</p>
<pre class="r"><code>mu &lt;- c(0,0,0)</code></pre>
<p>Use the correlation matrix and mean specifications to generate data.</p>
<pre class="r"><code>library(MASS)

df &lt;- mvrnorm(N, mu, sigma)</code></pre>
<p>Turn it into a data frame and label it.</p>
<pre class="r"><code>df &lt;- data.frame(df)
names(df) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;y&#39;)
df$id &lt;- c(1:N)</code></pre>
<p>Run regression and print the output.</p>
<pre class="r"><code>summary(lm(y ~ x1 + x2,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.64213 -0.58061 -0.00736  0.60250  3.13318 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.01971    0.03914  -0.504    0.615    
## x1           0.35847    0.03997   8.968   &lt;2e-16 ***
## x2           0.38284    0.04060   9.429   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8742 on 497 degrees of freedom
## Multiple R-squared:  0.2691, Adjusted R-squared:  0.2661 
## F-statistic: 91.47 on 2 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.</p>
<p>Create a composite using the regression weights.</p>
<pre class="r"><code>library(tidyverse)
df &lt;- df %&gt;%
  mutate(composite_x = 0.33*x1 + 0.4*x2)</code></pre>
<p>Those weights provide the maximum correlation between our composite and the outcome.</p>
<pre class="r"><code>cor(df$y, df$composite_x)</code></pre>
<pre><code>## [1] 0.517818</code></pre>
<p>In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.</p>
<pre class="r"><code>summary(lm(y ~ composite_x,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ composite_x, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.66643 -0.58001  0.00031  0.60042  3.11062 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.02091    0.03909  -0.535    0.593    
## composite_x  1.01116    0.07486  13.508   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8738 on 498 degrees of freedom
## Multiple R-squared:  0.2681, Adjusted R-squared:  0.2667 
## F-statistic: 182.5 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
