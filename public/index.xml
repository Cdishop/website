<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Christopher R. Dishop</title>
    <link>https://christopherdishop.netlify.app/</link>
      <atom:link href="https://christopherdishop.netlify.app/index.xml" rel="self" type="application/rss+xml" />
    <description>Christopher R. Dishop</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Christopher Dishop 2020</copyright><lastBuildDate>Sun, 02 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://christopherdishop.netlify.app/img/pom-card.png</url>
      <title>Christopher R. Dishop</title>
      <link>https://christopherdishop.netlify.app/</link>
    </image>
    
    <item>
      <title>Host Multiple Pages on GitHub</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-08-02/</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-08-02/</guid>
      <description>


&lt;p&gt;GitHub allows you to host one static website per repository. Here’s how to do it:&lt;/p&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create a repository&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a ‘gh-pages’ branch&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/github_images/ghpages.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make the ‘gh-pages’ branch your default&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/github_images/default.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Delete the old ‘master’ branch&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Clone the repo to your local computer&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create an index using whatever source-code you prefer&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/github_images/index.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Commit and push the files&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GitHub will automatically render the site after a few moments&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anonymous Push on GitHub</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-08-01/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-08-01/</guid>
      <description>


&lt;p&gt;Quick note on cloning, committing, and pushing anonymously on GitHub.&lt;/p&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create an anonymous account and repo on GitHub&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Clone repo to local computer&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigate to it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Configure anonymous username and email&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;git add .
git commit -m &amp;quot;initial commit&amp;quot;
git pull&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Push using full specification&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# replace username, password, and repository:

git push &amp;#39;https://username:password@github.com/username/repository.git&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Counting Degrees of Freedom</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-07-13-dfs/</link>
      <pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-07-13-dfs/</guid>
      <description>


&lt;p&gt;This post contains a bunch of examples where I practice counting dfs. In each example, I generate the data, estimate the parameters using SEM, count the dfs, and then compare my count to what the model spits back. To count dfs, I need to know the number of knowns and unknowns in my system:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DFs} = \textrm{knowns - unknowns}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To count the number of knowns, I need to know the number of observed variables, &lt;em&gt;p&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{knowns} = p*(p+1) / 2
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To count the number of unknowns, I count the number of parameters that my model estimates. Now for the examples.&lt;/p&gt;
&lt;div id=&#34;example-1---trust-and-availability-cause-helping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1 - Trust and availability cause helping&lt;/h1&gt;
&lt;div id=&#34;dgp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- 400
trust &amp;lt;- rnorm(people, 40, 2)
availability &amp;lt;- rnorm(people, 20, 5)
error &amp;lt;- rnorm(people, 0, 2)

helping &amp;lt;- 3 + 0.2*trust + 0.7*availability + error&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SEM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lavaan)

df &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:people),
  &amp;#39;trust&amp;#39; = c(trust),
  &amp;#39;availability&amp;#39; = c(availability),
  &amp;#39;helping&amp;#39; = c(helping)
)

ex1_string &amp;lt;- &amp;#39;

helping ~ b1*trust + b2*availability

&amp;#39;

ex1_model &amp;lt;- sem(ex1_string, data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;count-dfs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Count dfs&lt;/h1&gt;
&lt;div id=&#34;knowns-count-the-observed-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Knowns (count the observed variables)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# p*(p + 1) / 2

3*(3+1) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unknowns-count-the-estimated-parameters&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Unknowns (count the estimated parameters)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 for b1&lt;/li&gt;
&lt;li&gt;1 for b2&lt;/li&gt;
&lt;li&gt;1 for the variance of trust&lt;/li&gt;
&lt;li&gt;1 for the variance of availability&lt;/li&gt;
&lt;li&gt;1 for the covariance of trust and availability&lt;/li&gt;
&lt;li&gt;1 for the prediction error on helping&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 6&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;6 - 6 = 0&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex1_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 12 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          3
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 0.000
##   Degrees of freedom                                 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now if I restrict the covariance of trust and availability to be zero I should have 1 df&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1_string_restrict &amp;lt;- &amp;#39;

helping ~ b1*trust + b2*availability
trust ~~ 0*availability

&amp;#39;

ex1_model_restrict &amp;lt;- sem(ex1_string_restrict, data = df)
show(ex1_model_restrict) # yup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 14 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 0.474
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.491&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2---common-factor-underlying-6-observed-items&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2 - Common factor underlying 6 observed items&lt;/h1&gt;
&lt;div id=&#34;dgp-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_factor &amp;lt;- rnorm(people, 30, 2)
error_cf &amp;lt;- rnorm(people, 0, 2)
item1 &amp;lt;- 0.35*common_factor + error_cf
item2 &amp;lt;- 0.22*common_factor + error_cf
item3 &amp;lt;- 0.18*common_factor + error_cf
item4 &amp;lt;- 0.24*common_factor + error_cf
item5 &amp;lt;- 0.31*common_factor + error_cf
item6 &amp;lt;- 0.44*common_factor + error_cf

# nope, that approach is wrong. If I do above then my errors are not independent
# prediction errors (in this case measurement) should be independent

item1 &amp;lt;- 0.35*common_factor + rnorm(people, 0, 2)
item2 &amp;lt;- 0.22*common_factor + rnorm(people, 0, 2)
item3 &amp;lt;- 0.18*common_factor + rnorm(people, 0, 2)
item4 &amp;lt;- 0.24*common_factor + rnorm(people, 0, 2)
item5 &amp;lt;- 0.31*common_factor + rnorm(people, 0, 2)
item6 &amp;lt;- 0.44*common_factor + rnorm(people, 0, 2)

df_cf &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:people),
  &amp;#39;item1&amp;#39; = c(item1),
  &amp;#39;item2&amp;#39; = c(item2),
  &amp;#39;item3&amp;#39; = c(item3),
  &amp;#39;item4&amp;#39; = c(item4),
  &amp;#39;item5&amp;#39; = c(item5),
  &amp;#39;item6&amp;#39; = c(item6)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SEM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2_string &amp;lt;- &amp;#39;

com_factor =~ 1*item1 + fl2*item2 + fl3*item3 + fl4*item4 + fl5*item5 + fl6*item6
&amp;#39;

ex2_model &amp;lt;- sem(ex2_string, data = df_cf)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;count-dfs-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Count dfs&lt;/h3&gt;
&lt;div id=&#34;knowns-count-the-observed-variables-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;knowns (count the observed variables)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# p*(p + 1) / 2

6*(6 + 1) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 21&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unknowns-count-the-estimated-parameters-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;unknowns (count the estimated parameters)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;6 factor loadings, but I constrained the first one to be 1 (I have to to estimate the latent variable), so 5 parameters&lt;/li&gt;
&lt;li&gt;5 measurement errors for the 5 factor loadings&lt;/li&gt;
&lt;li&gt;1 variance for the latent exogenous variable&lt;/li&gt;
&lt;li&gt;1 mean for the latent exogenous variable&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 12&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;21 - 12 = 9&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex2_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 55 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         12
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 8.497
##   Degrees of freedom                                 9
##   P-value (Chi-square)                           0.485&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3---two-latent-variables-predict-one-observed-outcome&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3 - Two latent variables predict one observed outcome&lt;/h1&gt;
&lt;p&gt;Cognitive ability (latent variable 1) and assertiveness (latent variable 2) predict productivity. Cognitive ability and assertiveness are both captured with 2 manifest items/variables.&lt;/p&gt;
&lt;div id=&#34;dgp-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# cog ability (latent exogenous variable 1)
cog_ability &amp;lt;- rnorm(people, 100, 15)
ca_item1 &amp;lt;- 0.78*cog_ability + rnorm(people, 0, 1)
ca_item2 &amp;lt;- 0.11*cog_ability + rnorm(people, 0, 1)

# assertiveness (latent exogenous variable 2)
assertive &amp;lt;- rnorm(people, 30, 8)
ass_item1 &amp;lt;- 0.81*assertive + rnorm(people, 0, 1)
ass_item2 &amp;lt;- 0.34*assertive + rnorm(people, 0, 1)

# productivity (observed outcome)

productivity &amp;lt;- 0.55*cog_ability + 0.82*assertive + rnorm(people, 0, 5)

# data

df_3 &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:people),
  &amp;#39;ca_item1&amp;#39; = c(ca_item1),
  &amp;#39;ca_item2&amp;#39; = c(ca_item2),
  &amp;#39;ass_item1&amp;#39; = c(ass_item1),
  &amp;#39;ass_item2&amp;#39; = c(ass_item2),
  &amp;#39;productivity&amp;#39; = c(productivity)
  
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SEM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex3_string &amp;lt;- &amp;#39;

cog_ability =~ 1*ca_item1 + fl2*ca_item2
assertiveness =~ 1*ass_item1 + fla*ass_item2

cog_ability ~~ cog_ability
assertiveness ~~ assertiveness
cog_ability ~~ assertiveness

productivity ~ b1*cog_ability + b2*assertiveness

&amp;#39;

ex3_model &amp;lt;- sem(ex3_string, data = df_3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;count-dfs-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Count dfs&lt;/h3&gt;
&lt;div id=&#34;knowns-count-the-observed-variables-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;knowns (count the observed variables)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# p*(p+1) / 2

5*(5+1) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unknowns-count-the-estimated-parameters-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;unknowns (count the estimated parameters)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;4 factor loadings but I constrained 2 of them, so 2 factor loadings&lt;/li&gt;
&lt;li&gt;2 measurement errors (4 items, but constrained 2 of them)&lt;/li&gt;
&lt;li&gt;1 variance on cog ability&lt;/li&gt;
&lt;li&gt;1 mean on cog ability&lt;/li&gt;
&lt;li&gt;1 variance on assertiveness&lt;/li&gt;
&lt;li&gt;1 mean on assertiveness&lt;/li&gt;
&lt;li&gt;1 covariance among cog ability and assertiveness&lt;/li&gt;
&lt;li&gt;b1&lt;/li&gt;
&lt;li&gt;b2&lt;/li&gt;
&lt;li&gt;2 prediction errors&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 13&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;15 - 13 = 2&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex3_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 174 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         12
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 3.888
##   Degrees of freedom                                 3
##   P-value (Chi-square)                           0.274&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope. I’m one off, where did I go wrong?&lt;/p&gt;
&lt;p&gt;Ah, there is only 1 prediction error because productivity is being predicted. I counted 2 prediction errors because I gave one to both b1 and b2. So, the unknowns should be…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 factor loadings but I constrained 2 of them, so 2 factor loadings&lt;/li&gt;
&lt;li&gt;2 measurement errors (4 items, but constrained 2 of them)&lt;/li&gt;
&lt;li&gt;1 variance on cog ability&lt;/li&gt;
&lt;li&gt;1 mean on cog ability&lt;/li&gt;
&lt;li&gt;1 variance on assertiveness&lt;/li&gt;
&lt;li&gt;1 mean on assertiveness&lt;/li&gt;
&lt;li&gt;1 covariance among cog ability and assertiveness&lt;/li&gt;
&lt;li&gt;b1&lt;/li&gt;
&lt;li&gt;b2&lt;/li&gt;
&lt;li&gt;1 prediction error&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 12&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;15 - 12 = 3&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex3_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 174 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         12
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 3.888
##   Degrees of freedom                                 3
##   P-value (Chi-square)                           0.274&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4---a-causes-b-which-causes-c-which-causes-d&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 4 - a causes b, which causes c, which causes d&lt;/h1&gt;
&lt;div id=&#34;dgp-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- rnorm(people, 300, 3)
b &amp;lt;- 0.67*a + rnorm(people, 0, 1)
c &amp;lt;- 0.99*b + rnorm(people, 0, 10)
d &amp;lt;- 4 + 4*c + rnorm(people, 0, 4)

df_chain &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:people),
  &amp;#39;a&amp;#39; = c(a),
  &amp;#39;b&amp;#39; = c(b),
  &amp;#39;c&amp;#39; = c(c),
  &amp;#39;d&amp;#39; = c(d)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SEM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex4_string &amp;lt;- &amp;#39;

b ~ b1*a
c ~ b2*b
d ~ b3*c

a ~~ a

&amp;#39;

ex4_model &amp;lt;- sem(ex4_string, data = df_chain)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;count-dfs-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Count dfs&lt;/h3&gt;
&lt;div id=&#34;knowns-count-the-observed-variables-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;knowns (count the observed variables)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# p*(p+1) / 2
4*(4+1) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unknowns-count-the-estimated-parameters-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;unknowns (count the estimated parameters)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;b1&lt;/li&gt;
&lt;li&gt;b2&lt;/li&gt;
&lt;li&gt;b3&lt;/li&gt;
&lt;li&gt;3 prediction errors&lt;/li&gt;
&lt;li&gt;1 variance for the lone exogenous variable (a)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 7&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;10 - 7 = 3&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex4_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 43 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          7
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 1.979
##   Degrees of freedom                                 3
##   P-value (Chi-square)                           0.577&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-5---observed-affect-over-7-time-points&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 5 - Observed affect over 7 time points&lt;/h1&gt;
&lt;div id=&#34;dgp-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- 7
affect_store &amp;lt;- matrix(, ncol = 3, nrow = time*people)
count &amp;lt;- 0
for(i in 1:people){
  
  unob_het &amp;lt;- rnorm(1, 0, 3)
  
  for(j in 1:time){
    count &amp;lt;- count + 1
    
    if(j == 1){
      affect_store[count, 1] &amp;lt;- i
      affect_store[count, 2] &amp;lt;- j
      affect_store[count, 3] &amp;lt;- unob_het + 50 + rnorm(1, 0, 1)
    }else{
      affect_store[count, 1] &amp;lt;- i
      affect_store[count, 2] &amp;lt;- j
      affect_store[count, 3] &amp;lt;- 0.8*affect_store[count - 1, 3] + unob_het + rnorm(1, 0, 1)
      
    }
  }
  
}
df5 &amp;lt;- data.frame(affect_store)
names(df5) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)
library(reshape2)
df5_wide &amp;lt;- reshape(df5, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SEM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex5_string &amp;lt;- &amp;#39;

unob_het =~ 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7

affect.2 ~ ar*affect.1
affect.3 ~ ar*affect.2
affect.4 ~ ar*affect.3
affect.5 ~ ar*affect.4
affect.6 ~ ar*affect.5
affect.7 ~ ar*affect.6

affect.1 ~~ affect.1
unob_het ~~ unob_het
affect.1 ~~ unob_het

&amp;#39;

ex5_model &amp;lt;- sem(ex5_string, data = df5_wide)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;count-dfs-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Count dfs&lt;/h3&gt;
&lt;div id=&#34;knowns-count-the-observed-variables-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;knowns (count the observed variables)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# p*(p+1) / 2
7*(7+1) / 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;unknowns-count-the-estimated-parameters-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;unknowns (count the estimated parameters)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ar is 1 estimated parameter&lt;/li&gt;
&lt;li&gt;1 variance of unobserved heterogeneity&lt;/li&gt;
&lt;li&gt;1 variance of affect.1&lt;/li&gt;
&lt;li&gt;1 covariance among affect.1 and unobserved heterogeneity&lt;/li&gt;
&lt;li&gt;6 prediction errors&lt;/li&gt;
&lt;li&gt;&lt;p&gt;total = 10&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;28 - 10 = 18&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show(ex5_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 121 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         15
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                14.770
##   Degrees of freedom                                18
##   P-value (Chi-square)                           0.678&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why didn’t I estimate a mean for unobserved heterogeneity here? In all of the other examples I estimated the variance (1 parameter) and the mean (1 parameter) of the latent exogenous variable. In this case, unobserved heterogeneity is the latent exogenous variable but I only estimated its variance. That’s because in this model we don’t really care about the mean of unobserved heterogeneity, it’s just a latent variable that we incorporate to account for stable individual differences. In other words, when I estimate latent cog ability and assertiveness as IVs to predict an outcome, I care about their means. Here, unobserved heterogeneity is just an additional factor to account for, not a variable whose mean I really care to know. That said, if I wanted to estimate the mean of unobserved heterogeneity (which would result in one additional estimated parameter and one fewer df) then I would incorporate the following into the model string.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;#39;

unob_het ~ 1 # lavaan code for estimating the mean of a latent variable

&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bivariate Latent Dual Change Model</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-05-26/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-05-26/</guid>
      <description>


&lt;p&gt;My last post demonstrated a dual change model for one variable, now I want to demonstrate a bivariate dual change model. A SEM path diagram for a bivariate dual change model is below, taken from &lt;a href=&#34;https://www-annualreviews-org.proxy2.cl.msu.edu/doi/pdf/10.1146%2Fannurev-orgpsych-041015-062553&#34;&gt;Wang, Zhou, and Zhang (2016)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/dual_change_photos/bdc.png&#34; /&gt;
(If you do not have access to that link you can view a similar path diagram in &lt;a href=&#34;https://journals.sagepub.com/doi/pdf/10.1177/0149206313503012&#34;&gt;Jones, King, Gilrane, McCausland, Cortina, &amp;amp; Grimm, 2016&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Essentially, we have two dual change processes and a coupling parameter from the latent true score on one variable to the latent change score on the other.&lt;/p&gt;
&lt;div id=&#34;the-dgp&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The DGP&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_t =  constant_y + (1 + proportion_y)*y_{t-1} + coupling_{xy}*x_{t-1} + e
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(constant_y\)&lt;/span&gt; is the change factor (or latent slope) on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(proportion_y\)&lt;/span&gt; is the proportion change factor, and &lt;span class=&#34;math inline&#34;&gt;\(coupling_xy\)&lt;/span&gt; is the coupling parameter relating &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The DGP for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
x_t =  constant_x + (1 + proportion_x)*x_{t-1} + coupling_{yx}*y_{t-1} + e
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the terms are similar but now applied to values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The true values used in the DGP are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
y_t &amp;amp;=  0.5 + (1 + -0.32)y_{t-1} + 0.4x_{t-1} + e \\
x_t &amp;amp;=  0.5 + (1 + 0.22)x_{t-1} - 0.4y_{t-1} + e
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with initial values for both &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; sampled from &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; ~ (10, 1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- 700
time &amp;lt;- 6
x_cause_y &amp;lt;- 0.4
y_cause_x &amp;lt;- -0.4

const_x &amp;lt;- 0.5
const_y &amp;lt;- 0.5

prop_x &amp;lt;- 0.22
prop_y &amp;lt;- -0.32

df_mat &amp;lt;- matrix(, ncol = 4, nrow = people*time)
count &amp;lt;- 0

for(i in 1:people){
  
  unob_het_y &amp;lt;- rnorm(1, 0, 3)
  unob_het_x &amp;lt;- rnorm(1, 0, 3)
  
  for(j in 1:time){
    count &amp;lt;- count + 1
    
    if(j == 1){
      df_mat[count, 1] &amp;lt;- i
      df_mat[count, 2] &amp;lt;- j
      df_mat[count, 3] &amp;lt;- rnorm(1, 10, 1)
      df_mat[count, 4] &amp;lt;- rnorm(1, 10, 1)
    }else{
      
      df_mat[count, 1] &amp;lt;- i
      df_mat[count, 2] &amp;lt;- j
      df_mat[count, 3] &amp;lt;- const_x + (1+prop_x)*df_mat[count - 1, 3] + y_cause_x*df_mat[count - 1, 4] + unob_het_x + rnorm(1,0,1)
      df_mat[count, 4] &amp;lt;- const_y + (1+prop_y)*df_mat[count - 1, 4] + x_cause_y*df_mat[count - 1, 3] + unob_het_y + rnorm(1,0,1)
    }
    
  }
  
  
}

library(tidyverse)
library(ggplot2)
library(reshape2)

df &amp;lt;- data.frame(df_mat)
names(df) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;x&amp;#39;, &amp;#39;y&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_nums &amp;lt;- sample(c(1:700), 6)
df_sample &amp;lt;- df %&amp;gt;%
  filter(id %in% random_nums)

ggplot(df, aes(x = time, y = y, group = id)) + 
  geom_point(color = &amp;#39;grey85&amp;#39;) + 
  geom_line(color = &amp;#39;grey85&amp;#39;) + 
  geom_point(data = df_sample, aes(x = time, y = y, group = id)) + 
  geom_line(data = df_sample, aes(x = time, y = y, group = id))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-05-26/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_single_response &amp;lt;- function(y_axis){
  
  plot_it &amp;lt;- ggplot(df, aes(x = time, y = !!y_axis, group = id)) + 
    geom_point(color = &amp;#39;grey85&amp;#39;) + 
    geom_line(color = &amp;#39;grey85&amp;#39;) + 
    geom_point(data = df_sample, aes(x = time, y = !!y_axis, group = id)) + 
    geom_line(data = df_sample, aes(x = time, y = !!y_axis, group = id))
  
  return(plot_it)
}

plot_single_response(quo(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-05-26/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Three randomly selected individuals with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; plotted simultaneously.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;three_cases &amp;lt;- df %&amp;gt;%
  filter(id == 4 | id == 500 | id == 322) %&amp;gt;%
  gather(x, y, key = &amp;#39;variable&amp;#39;, value = &amp;#39;response&amp;#39;)

ggplot(three_cases, aes(x = time, y = response, color = variable)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-05-26/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dual-change-model-on-y&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dual Change Model on Y&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide_y &amp;lt;- df %&amp;gt;%
  select(id, time, y) %&amp;gt;%
  reshape(idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)

library(lavaan)

dual_change_y_string &amp;lt;- &amp;#39;

# latent true scores over y
ly1 =~ 1*y.1
ly2 =~ 1*y.2
ly3 =~ 1*y.3
ly4 =~ 1*y.4
ly5 =~ 1*y.5
ly6 =~ 1*y.6

# latent change scores over the true scores (but not the first time point)
cy2 =~ 1*ly2
cy3 =~ 1*ly3
cy4 =~ 1*ly4
cy5 =~ 1*ly5
cy6 =~ 1*ly6

# autoregressions of latent true scores over y constrained to 1
ly2 ~ 1*ly1
ly3 ~ 1*ly2
ly4 ~ 1*ly3
ly5 ~ 1*ly4
ly6 ~ 1*ly5

# latent intercept over first latent true score on y
l_intercept =~ 1*ly1

# change component 1 of the dual change model

# latent slope (or change factor) over the change scores
l_slope =~ 1*cy2 + 1*cy3 + 1*cy4 + 1*cy5 + 1*cy6

# estimate means and variances of those intercept and slope terms
l_intercept ~~ l_intercept
l_slope ~~ l_slope
l_slope ~ 1
l_intercept ~ 1

# and a covariance between them
l_intercept ~~ l_slope

# change component 2 of the dual change model

# proportion change from true scores over y to the change factors
cy2 ~ prop*ly1
cy3 ~ prop*ly2
cy4 ~ prop*ly3
cy5 ~ prop*ly4
cy6 ~ prop*ly5

# means and variances of latent factors set to zero
ly1 ~ 0
ly2 ~ 0
ly3 ~ 0
ly4 ~ 0
ly5 ~ 0
ly6 ~ 0

cy2 ~ 0
cy3 ~ 0
cy4 ~ 0
cy5 ~ 0
cy6 ~ 0

ly1 ~~ 0*ly1
ly2 ~~ 0*ly2
ly3 ~~ 0*ly3
ly4 ~~ 0*ly4
ly5 ~~ 0*ly5
ly6 ~~ 0*ly6

cy2 ~~ 0*cy2
cy3 ~~ 0*cy3
cy4 ~~ 0*cy4
cy5 ~~ 0*cy5
cy6 ~~ 0*cy6

# means of indicators to zero
y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to equality across time
y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# do not allow change factors to correlate
cy2 ~~ 0*cy3 + 0*cy4 + 0*cy5 + 0*cy6
cy3 ~~ 0*cy4 + 0*cy5 + 0*cy6
cy4 ~~ 0*cy5 + 0*cy6
cy5 ~~ 0*cy6

&amp;#39;

dc_y_model &amp;lt;- sem(dual_change_y_string, data = df_wide_y)
summary(dc_y_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 57 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         16
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           700
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              6389.847
##   Degrees of freedom                                20
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              8286.339
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.230
##   Tucker-Lewis Index (TLI)                       0.422
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11708.642
##   Loglikelihood unrestricted model (H1)      -8513.718
##                                                       
##   Akaike (AIC)                               23431.283
##   Bayesian (BIC)                             23463.141
##   Sample-size adjusted Bayesian (BIC)        23440.914
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.675
##   90 Percent confidence interval - lower         0.661
##   90 Percent confidence interval - upper         0.688
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           2.363
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   ly1 =~                                              
##     y.1               1.000                           
##   ly2 =~                                              
##     y.2               1.000                           
##   ly3 =~                                              
##     y.3               1.000                           
##   ly4 =~                                              
##     y.4               1.000                           
##   ly5 =~                                              
##     y.5               1.000                           
##   ly6 =~                                              
##     y.6               1.000                           
##   cy2 =~                                              
##     ly2               1.000                           
##   cy3 =~                                              
##     ly3               1.000                           
##   cy4 =~                                              
##     ly4               1.000                           
##   cy5 =~                                              
##     ly5               1.000                           
##   cy6 =~                                              
##     ly6               1.000                           
##   l_intercept =~                                      
##     ly1               1.000                           
##   l_slope =~                                          
##     cy2               1.000                           
##     cy3               1.000                           
##     cy4               1.000                           
##     cy5               1.000                           
##     cy6               1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   ly2 ~                                               
##     ly1               1.000                           
##   ly3 ~                                               
##     ly2               1.000                           
##   ly4 ~                                               
##     ly3               1.000                           
##   ly5 ~                                               
##     ly4               1.000                           
##   ly6 ~                                               
##     ly5               1.000                           
##   cy2 ~                                               
##     ly1     (prop)    0.250    0.019   13.327    0.000
##   cy3 ~                                               
##     ly2     (prop)    0.250    0.019   13.327    0.000
##   cy4 ~                                               
##     ly3     (prop)    0.250    0.019   13.327    0.000
##   cy5 ~                                               
##     ly4     (prop)    0.250    0.019   13.327    0.000
##   cy6 ~                                               
##     ly5     (prop)    0.250    0.019   13.327    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_intercept ~~                                      
##     l_slope          -1.394    0.223   -6.256    0.000
##  .cy2 ~~                                              
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy3 ~~                                              
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy4 ~~                                              
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy5 ~~                                              
##    .cy6               0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_slope          -3.285    0.213  -15.445    0.000
##     l_intercept      11.707    0.114  102.723    0.000
##    .ly1               0.000                           
##    .ly2               0.000                           
##    .ly3               0.000                           
##    .ly4               0.000                           
##    .ly5               0.000                           
##    .ly6               0.000                           
##    .cy2               0.000                           
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_ntrcp           5.916    0.485   12.193    0.000
##     l_slope           2.273    0.201   11.307    0.000
##    .ly1               0.000                           
##    .ly2               0.000                           
##    .ly3               0.000                           
##    .ly4               0.000                           
##    .ly5               0.000                           
##    .ly6               0.000                           
##    .cy2               0.000                           
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##    .y.1     (rs_v)    7.123    0.190   37.417    0.000
##    .y.2     (rs_v)    7.123    0.190   37.417    0.000
##    .y.3     (rs_v)    7.123    0.190   37.417    0.000
##    .y.4     (rs_v)    7.123    0.190   37.417    0.000
##    .y.5     (rs_v)    7.123    0.190   37.417    0.000
##    .y.6     (rs_v)    7.123    0.190   37.417    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Code to change the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;’s in the string to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s without manually deleting and inserting &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; into the string above. All you have to do is paste the string into a .txt document and save the file as “y_file.txt”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)

mystring &amp;lt;- read_file(&amp;#39;y_file.txt&amp;#39;)
new_data &amp;lt;- gsub(&amp;#39;y&amp;#39;, &amp;#39;x&amp;#39;, mystring)
# write_file(new_data, path = &amp;#39;x_file.txt&amp;#39;) # not executed but will work&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dual-change-model-on-x&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dual Change Model on X&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide_x &amp;lt;- df %&amp;gt;%
  select(id, time, x) %&amp;gt;%
  reshape(idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


library(lavaan)

dual_change_x_string &amp;lt;- &amp;#39;

# latent true scores over x
lx1 =~ 1*x.1
lx2 =~ 1*x.2
lx3 =~ 1*x.3
lx4 =~ 1*x.4
lx5 =~ 1*x.5
lx6 =~ 1*x.6

# latent change scores over the true scores (but not the first time point)
cx2 =~ 1*lx2
cx3 =~ 1*lx3
cx4 =~ 1*lx4
cx5 =~ 1*lx5
cx6 =~ 1*lx6

# autoregressions of latent true scores over x constrained to 1
lx2 ~ 1*lx1
lx3 ~ 1*lx2
lx4 ~ 1*lx3
lx5 ~ 1*lx4
lx6 ~ 1*lx5

# latent intercept over first latent true score on x
l_intercept =~ 1*lx1

# change component 1 of the dual change model

# latent slope (or change factor) over the change scores
l_slope =~ 1*cx2 + 1*cx3 + 1*cx4 + 1*cx5 + 1*cx6

# estimate means and variances of those intercept and slope terms
l_intercept ~~ l_intercept
l_slope ~~ l_slope
l_slope ~ 1
l_intercept ~ 1

# and a covariance between them
l_intercept ~~ l_slope

# change component 2 of the dual change model

# proportion change from true scores over x to the change factors
cx2 ~ prop*lx1
cx3 ~ prop*lx2
cx4 ~ prop*lx3
cx5 ~ prop*lx4
cx6 ~ prop*lx5

# means and variances of latent factors set to zero
lx1 ~ 0
lx2 ~ 0
lx3 ~ 0
lx4 ~ 0
lx5 ~ 0
lx6 ~ 0

cx2 ~ 0
cx3 ~ 0
cx4 ~ 0
cx5 ~ 0
cx6 ~ 0

lx1 ~~ 0*lx1
lx2 ~~ 0*lx2
lx3 ~~ 0*lx3
lx4 ~~ 0*lx4
lx5 ~~ 0*lx5
lx6 ~~ 0*lx6

cx2 ~~ 0*cx2
cx3 ~~ 0*cx3
cx4 ~~ 0*cx4
cx5 ~~ 0*cx5
cx6 ~~ 0*cx6

# means of indicators to zero
x.1 ~ 0
x.2 ~ 0
x.3 ~ 0
x.4 ~ 0
x.5 ~ 0
x.6 ~ 0

# residual variances constrained to equalitx across time
x.1 ~~ res_var*x.1
x.2 ~~ res_var*x.2
x.3 ~~ res_var*x.3
x.4 ~~ res_var*x.4
x.5 ~~ res_var*x.5
x.6 ~~ res_var*x.6

# do not allow change factors to correlate
cx2 ~~ 0*cx3 + 0*cx4 + 0*cx5 + 0*cx6
cx3 ~~ 0*cx4 + 0*cx5 + 0*cx6
cx4 ~~ 0*cx5 + 0*cx6
cx5 ~~ 0*cx6

&amp;#39;

dc_x_model &amp;lt;- sem(dual_change_x_string, data = df_wide_x)
summary(dc_x_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 64 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         16
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           700
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              4424.279
##   Degrees of freedom                                20
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                             11743.090
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.624
##   Tucker-Lewis Index (TLI)                       0.718
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -10266.470
##   Loglikelihood unrestricted model (H1)      -8054.331
##                                                       
##   Akaike (AIC)                               20546.941
##   Bayesian (BIC)                             20578.798
##   Sample-size adjusted Bayesian (BIC)        20556.572
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.561
##   90 Percent confidence interval - lower         0.547
##   90 Percent confidence interval - upper         0.575
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.901
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   lx1 =~                                              
##     x.1               1.000                           
##   lx2 =~                                              
##     x.2               1.000                           
##   lx3 =~                                              
##     x.3               1.000                           
##   lx4 =~                                              
##     x.4               1.000                           
##   lx5 =~                                              
##     x.5               1.000                           
##   lx6 =~                                              
##     x.6               1.000                           
##   cx2 =~                                              
##     lx2               1.000                           
##   cx3 =~                                              
##     lx3               1.000                           
##   cx4 =~                                              
##     lx4               1.000                           
##   cx5 =~                                              
##     lx5               1.000                           
##   cx6 =~                                              
##     lx6               1.000                           
##   l_intercept =~                                      
##     lx1               1.000                           
##   l_slope =~                                          
##     cx2               1.000                           
##     cx3               1.000                           
##     cx4               1.000                           
##     cx5               1.000                           
##     cx6               1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   lx2 ~                                               
##     lx1               1.000                           
##   lx3 ~                                               
##     lx2               1.000                           
##   lx4 ~                                               
##     lx3               1.000                           
##   lx5 ~                                               
##     lx4               1.000                           
##   lx6 ~                                               
##     lx5               1.000                           
##   cx2 ~                                               
##     lx1     (prop)    0.157    0.004   35.120    0.000
##   cx3 ~                                               
##     lx2     (prop)    0.157    0.004   35.120    0.000
##   cx4 ~                                               
##     lx3     (prop)    0.157    0.004   35.120    0.000
##   cx5 ~                                               
##     lx4     (prop)    0.157    0.004   35.120    0.000
##   cx6 ~                                               
##     lx5     (prop)    0.157    0.004   35.120    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_intercept ~~                                      
##     l_slope          -0.838    0.257   -3.259    0.001
##  .cx2 ~~                                              
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx3 ~~                                              
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx4 ~~                                              
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx5 ~~                                              
##    .cx6               0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_slope          -3.550    0.122  -29.197    0.000
##     l_intercept      10.396    0.079  131.733    0.000
##    .lx1               0.000                           
##    .lx2               0.000                           
##    .lx3               0.000                           
##    .lx4               0.000                           
##    .lx5               0.000                           
##    .lx6               0.000                           
##    .cx2               0.000                           
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##    .x.1               0.000                           
##    .x.2               0.000                           
##    .x.3               0.000                           
##    .x.4               0.000                           
##    .x.5               0.000                           
##    .x.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_ntrcp           3.105    0.224   13.844    0.000
##     l_slope           9.678    0.559   17.306    0.000
##    .lx1               0.000                           
##    .lx2               0.000                           
##    .lx3               0.000                           
##    .lx4               0.000                           
##    .lx5               0.000                           
##    .lx6               0.000                           
##    .cx2               0.000                           
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##    .x.1     (rs_v)    2.309    0.062   37.417    0.000
##    .x.2     (rs_v)    2.309    0.062   37.417    0.000
##    .x.3     (rs_v)    2.309    0.062   37.417    0.000
##    .x.4     (rs_v)    2.309    0.062   37.417    0.000
##    .x.5     (rs_v)    2.309    0.062   37.417    0.000
##    .x.6     (rs_v)    2.309    0.062   37.417    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-dual-change-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bivariate Dual Change Model&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bi_dc_string &amp;lt;- &amp;#39;

# DUAL CHANGE IN Y

#

#

#

# latent true scores over y
ly1 =~ 1*y.1
ly2 =~ 1*y.2
ly3 =~ 1*y.3
ly4 =~ 1*y.4
ly5 =~ 1*y.5
ly6 =~ 1*y.6

# latent change scores over the true scores (but not the first time point)
cy2 =~ 1*ly2
cy3 =~ 1*ly3
cy4 =~ 1*ly4
cy5 =~ 1*ly5
cy6 =~ 1*ly6

# autoregressions of latent true scores over y constrained to 1
ly2 ~ 1*ly1
ly3 ~ 1*ly2
ly4 ~ 1*ly3
ly5 ~ 1*ly4
ly6 ~ 1*ly5

# latent intercept over first latent true score on y
l_intercept =~ 1*ly1

# change component 1 of the dual change model

# latent slope (or change factor) over the change scores
l_slope =~ 1*cy2 + 1*cy3 + 1*cy4 + 1*cy5 + 1*cy6

# estimate means and variances of those intercept and slope terms
l_intercept ~~ l_intercept
l_slope ~~ l_slope
l_slope ~ 1
l_intercept ~ 1

# and a covariance between them
l_intercept ~~ l_slope

# change component 2 of the dual change model

# proportion change from true scores over y to the change factors
cy2 ~ prop*ly1
cy3 ~ prop*ly2
cy4 ~ prop*ly3
cy5 ~ prop*ly4
cy6 ~ prop*ly5

# means and variances of latent factors set to zero
ly1 ~ 0
ly2 ~ 0
ly3 ~ 0
ly4 ~ 0
ly5 ~ 0
ly6 ~ 0

cy2 ~ 0
cy3 ~ 0
cy4 ~ 0
cy5 ~ 0
cy6 ~ 0

ly1 ~~ 0*ly1
ly2 ~~ 0*ly2
ly3 ~~ 0*ly3
ly4 ~~ 0*ly4
ly5 ~~ 0*ly5
ly6 ~~ 0*ly6

cy2 ~~ 0*cy2
cy3 ~~ 0*cy3
cy4 ~~ 0*cy4
cy5 ~~ 0*cy5
cy6 ~~ 0*cy6

# means of indicators to zero
y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to equality across time
y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# do not allow change factors to correlate
cy2 ~~ 0*cy3 + 0*cy4 + 0*cy5 + 0*cy6
cy3 ~~ 0*cy4 + 0*cy5 + 0*cy6
cy4 ~~ 0*cy5 + 0*cy6
cy5 ~~ 0*cy6


# DUAL CHANGE IN X

#

#

#






# latent true scores over x
lx1 =~ 1*x.1
lx2 =~ 1*x.2
lx3 =~ 1*x.3
lx4 =~ 1*x.4
lx5 =~ 1*x.5
lx6 =~ 1*x.6

# latent change scores over the true scores (but not the first time point)
cx2 =~ 1*lx2
cx3 =~ 1*lx3
cx4 =~ 1*lx4
cx5 =~ 1*lx5
cx6 =~ 1*lx6

# autoregressions of latent true scores over x constrained to 1
lx2 ~ 1*lx1
lx3 ~ 1*lx2
lx4 ~ 1*lx3
lx5 ~ 1*lx4
lx6 ~ 1*lx5

# latent intercept over first latent true score on x
lx_intercept =~ 1*lx1

# change component 1 of the dual change model

# latent slope (or change factor) over the change scores
lx_slope =~ 1*cx2 + 1*cx3 + 1*cx4 + 1*cx5 + 1*cx6

# estimate means and variances of those intercept and slope terms
lx_intercept ~~ lx_intercept
lx_slope ~~ lx_slope
lx_slope ~ 1
lx_intercept ~ 1

# and a covariance between them
lx_intercept ~~ lx_slope

# change component 2 of the dual change model

# proportion change from true scores over x to the change factors
cx2 ~ propx*lx1
cx3 ~ propx*lx2
cx4 ~ propx*lx3
cx5 ~ propx*lx4
cx6 ~ propx*lx5

# means and variances of latent factors set to zero
lx1 ~ 0
lx2 ~ 0
lx3 ~ 0
lx4 ~ 0
lx5 ~ 0
lx6 ~ 0

cx2 ~ 0
cx3 ~ 0
cx4 ~ 0
cx5 ~ 0
cx6 ~ 0

lx1 ~~ 0*lx1
lx2 ~~ 0*lx2
lx3 ~~ 0*lx3
lx4 ~~ 0*lx4
lx5 ~~ 0*lx5
lx6 ~~ 0*lx6

cx2 ~~ 0*cx2
cx3 ~~ 0*cx3
cx4 ~~ 0*cx4
cx5 ~~ 0*cx5
cx6 ~~ 0*cx6

# means of indicators to zero
x.1 ~ 0
x.2 ~ 0
x.3 ~ 0
x.4 ~ 0
x.5 ~ 0
x.6 ~ 0

# residual variances constrained to equalitx across time
x.1 ~~ res_varx*x.1
x.2 ~~ res_varx*x.2
x.3 ~~ res_varx*x.3
x.4 ~~ res_varx*x.4
x.5 ~~ res_varx*x.5
x.6 ~~ res_varx*x.6

# do not allow change factors to correlate
cx2 ~~ 0*cx3 + 0*cx4 + 0*cx5 + 0*cx6
cx3 ~~ 0*cx4 + 0*cx5 + 0*cx6
cx4 ~~ 0*cx5 + 0*cx6
cx5 ~~ 0*cx6

# COUPLING

#

#

cy2 ~ xy*lx1
cy3 ~ xy*lx2
cy4 ~ xy*lx3
cy5 ~ xy*lx4
cy6 ~ xy*lx5

cx2 ~ yx*ly1
cx3 ~ yx*ly2
cx4 ~ yx*ly3
cx5 ~ yx*ly4
cx6 ~ yx*ly5


&amp;#39;

df_both &amp;lt;- df %&amp;gt;%
  reshape(idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)

bi_dc_model &amp;lt;- sem(bi_dc_string, data = df_both)
summary(bi_dc_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 123 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         46
##   Number of equality constraints                    26
##                                                       
##   Number of observations                           700
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              1546.104
##   Degrees of freedom                                70
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                             24109.208
##   Degrees of freedom                                66
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.939
##   Tucker-Lewis Index (TLI)                       0.942
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -15301.211
##   Loglikelihood unrestricted model (H1)     -14528.159
##                                                       
##   Akaike (AIC)                               30642.422
##   Bayesian (BIC)                             30733.444
##   Sample-size adjusted Bayesian (BIC)        30669.940
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.174
##   90 Percent confidence interval - lower         0.166
##   90 Percent confidence interval - upper         0.181
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.102
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   ly1 =~                                              
##     y.1               1.000                           
##   ly2 =~                                              
##     y.2               1.000                           
##   ly3 =~                                              
##     y.3               1.000                           
##   ly4 =~                                              
##     y.4               1.000                           
##   ly5 =~                                              
##     y.5               1.000                           
##   ly6 =~                                              
##     y.6               1.000                           
##   cy2 =~                                              
##     ly2               1.000                           
##   cy3 =~                                              
##     ly3               1.000                           
##   cy4 =~                                              
##     ly4               1.000                           
##   cy5 =~                                              
##     ly5               1.000                           
##   cy6 =~                                              
##     ly6               1.000                           
##   l_intercept =~                                      
##     ly1               1.000                           
##   l_slope =~                                          
##     cy2               1.000                           
##     cy3               1.000                           
##     cy4               1.000                           
##     cy5               1.000                           
##     cy6               1.000                           
##   lx1 =~                                              
##     x.1               1.000                           
##   lx2 =~                                              
##     x.2               1.000                           
##   lx3 =~                                              
##     x.3               1.000                           
##   lx4 =~                                              
##     x.4               1.000                           
##   lx5 =~                                              
##     x.5               1.000                           
##   lx6 =~                                              
##     x.6               1.000                           
##   cx2 =~                                              
##     lx2               1.000                           
##   cx3 =~                                              
##     lx3               1.000                           
##   cx4 =~                                              
##     lx4               1.000                           
##   cx5 =~                                              
##     lx5               1.000                           
##   cx6 =~                                              
##     lx6               1.000                           
##   lx_intercept =~                                     
##     lx1               1.000                           
##   lx_slope =~                                         
##     cx2               1.000                           
##     cx3               1.000                           
##     cx4               1.000                           
##     cx5               1.000                           
##     cx6               1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   ly2 ~                                               
##     ly1               1.000                           
##   ly3 ~                                               
##     ly2               1.000                           
##   ly4 ~                                               
##     ly3               1.000                           
##   ly5 ~                                               
##     ly4               1.000                           
##   ly6 ~                                               
##     ly5               1.000                           
##   cy2 ~                                               
##     ly1     (prop)   -0.315    0.004  -77.421    0.000
##   cy3 ~                                               
##     ly2     (prop)   -0.315    0.004  -77.421    0.000
##   cy4 ~                                               
##     ly3     (prop)   -0.315    0.004  -77.421    0.000
##   cy5 ~                                               
##     ly4     (prop)   -0.315    0.004  -77.421    0.000
##   cy6 ~                                               
##     ly5     (prop)   -0.315    0.004  -77.421    0.000
##   lx2 ~                                               
##     lx1               1.000                           
##   lx3 ~                                               
##     lx2               1.000                           
##   lx4 ~                                               
##     lx3               1.000                           
##   lx5 ~                                               
##     lx4               1.000                           
##   lx6 ~                                               
##     lx5               1.000                           
##   cx2 ~                                               
##     lx1     (prpx)    0.221    0.002  100.363    0.000
##   cx3 ~                                               
##     lx2     (prpx)    0.221    0.002  100.363    0.000
##   cx4 ~                                               
##     lx3     (prpx)    0.221    0.002  100.363    0.000
##   cx5 ~                                               
##     lx4     (prpx)    0.221    0.002  100.363    0.000
##   cx6 ~                                               
##     lx5     (prpx)    0.221    0.002  100.363    0.000
##   cy2 ~                                               
##     lx1       (xy)    0.402    0.002  188.751    0.000
##   cy3 ~                                               
##     lx2       (xy)    0.402    0.002  188.751    0.000
##   cy4 ~                                               
##     lx3       (xy)    0.402    0.002  188.751    0.000
##   cy5 ~                                               
##     lx4       (xy)    0.402    0.002  188.751    0.000
##   cy6 ~                                               
##     lx5       (xy)    0.402    0.002  188.751    0.000
##   cx2 ~                                               
##     ly1       (yx)   -0.428    0.004 -101.317    0.000
##   cx3 ~                                               
##     ly2       (yx)   -0.428    0.004 -101.317    0.000
##   cx4 ~                                               
##     ly3       (yx)   -0.428    0.004 -101.317    0.000
##   cx5 ~                                               
##     ly4       (yx)   -0.428    0.004 -101.317    0.000
##   cx6 ~                                               
##     ly5       (yx)   -0.428    0.004 -101.317    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_intercept ~~                                      
##     l_slope           0.324    0.149    2.167    0.030
##  .cy2 ~~                                              
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy3 ~~                                              
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy4 ~~                                              
##    .cy5               0.000                           
##    .cy6               0.000                           
##  .cy5 ~~                                              
##    .cy6               0.000                           
##   lx_intercept ~~                                     
##     lx_slope          0.113    0.129    0.877    0.381
##  .cx2 ~~                                              
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx3 ~~                                              
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx4 ~~                                              
##    .cx5               0.000                           
##    .cx6               0.000                           
##  .cx5 ~~                                              
##    .cx6               0.000                           
##   l_intercept ~~                                      
##     lx_intercept     -0.227    0.051   -4.436    0.000
##     lx_slope          0.088    0.144    0.609    0.543
##   l_slope ~~                                          
##     lx_intercept     -0.071    0.135   -0.524    0.601
##     lx_slope          0.483    0.358    1.348    0.178
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_slope           0.605    0.124    4.892    0.000
##     l_intercept      10.055    0.047  214.190    0.000
##    .ly1               0.000                           
##    .ly2               0.000                           
##    .ly3               0.000                           
##    .ly4               0.000                           
##    .ly5               0.000                           
##    .ly6               0.000                           
##    .cy2               0.000                           
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
##     lx_slope          0.857    0.122    7.050    0.000
##     lx_intercept      9.975    0.042  235.419    0.000
##    .lx1               0.000                           
##    .lx2               0.000                           
##    .lx3               0.000                           
##    .lx4               0.000                           
##    .lx5               0.000                           
##    .lx6               0.000                           
##    .cx2               0.000                           
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##    .x.1               0.000                           
##    .x.2               0.000                           
##    .x.3               0.000                           
##    .x.4               0.000                           
##    .x.5               0.000                           
##    .x.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     l_ntr             1.069    0.080   13.399    0.000
##     l_slp             9.410    0.520   18.097    0.000
##    .ly1               0.000                           
##    .ly2               0.000                           
##    .ly3               0.000                           
##    .ly4               0.000                           
##    .ly5               0.000                           
##    .ly6               0.000                           
##    .cy2               0.000                           
##    .cy3               0.000                           
##    .cy4               0.000                           
##    .cy5               0.000                           
##    .cy6               0.000                           
##    .y.1   (res_vr)    0.729    0.020   37.350    0.000
##    .y.2   (res_vr)    0.729    0.020   37.350    0.000
##    .y.3   (res_vr)    0.729    0.020   37.350    0.000
##    .y.4   (res_vr)    0.729    0.020   37.350    0.000
##    .y.5   (res_vr)    0.729    0.020   37.350    0.000
##    .y.6   (res_vr)    0.729    0.020   37.350    0.000
##     lx_nt             1.022    0.065   15.701    0.000
##     lx_sl             8.955    0.489   18.297    0.000
##    .lx1               0.000                           
##    .lx2               0.000                           
##    .lx3               0.000                           
##    .lx4               0.000                           
##    .lx5               0.000                           
##    .lx6               0.000                           
##    .cx2               0.000                           
##    .cx3               0.000                           
##    .cx4               0.000                           
##    .cx5               0.000                           
##    .cx6               0.000                           
##    .x.1   (rs_vrx)    0.432    0.012   36.464    0.000
##    .x.2   (rs_vrx)    0.432    0.012   36.464    0.000
##    .x.3   (rs_vrx)    0.432    0.012   36.464    0.000
##    .x.4   (rs_vrx)    0.432    0.012   36.464    0.000
##    .x.5   (rs_vrx)    0.432    0.012   36.464    0.000
##    .x.6   (rs_vrx)    0.432    0.012   36.464    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latent Dual Change Models</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-05-24/</link>
      <pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-05-24/</guid>
      <description>


&lt;p&gt;I begin with an intercept-only model in a latent change framework and then build to a full dual change model. SEM images in this post come from a lecture by &lt;a href=&#34;https://hdfs.msu.edu/people/faculty/nuttall-amy-k-phd&#34;&gt;Amy Nuttall&lt;/a&gt;. Two notes about the models and code below. First, the initial models will not fit well because they are too simple. The DGP uses both constant and proportion change (hence, “dual-change”) whereas the first few models only estimate an intercept. Second, I use the &lt;code&gt;sem&lt;/code&gt; rather than &lt;code&gt;growth&lt;/code&gt; command in &lt;code&gt;lavaan&lt;/code&gt; because it forces me to specify the entire model. I do not like using commands that make automatic constraints for me – if you do, you are much more likely to make a mistake or not know what your model is doing.&lt;/p&gt;
&lt;div id=&#34;dgp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;DGP&lt;/h3&gt;
&lt;p&gt;The underlying DGP will be the same throughout this exercise. Consistent with &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4259494/pdf/nihms412332.pdf&#34;&gt;Ghisletta and McArdle, 2012&lt;/a&gt;, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_t =  \alpha*b_1 + (1 + b_2)*y_{t-1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is the constant change (similar to the “slope” term in a basic growth model, in latent change frameworks it is called the “change factor”) and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; is the proportion change, or the change from point to point. The values specified in the DGP are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_t = 1*0.3 + (1 + -0.4)*y_{t-1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt; is equal to 0.3 and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; is equal to -0.4. Let’s generate data for 500 people across six time points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;constant &amp;lt;- 0.3
proportion &amp;lt;- -0.4

people &amp;lt;- 500
time &amp;lt;- 6

df &amp;lt;- matrix(, nrow = people*time, ncol = 3)
count &amp;lt;- 0

for(i in 1:people){
  
  y_het &amp;lt;- rnorm(1, 0, 2)
  
  for(j in 1:time){
    count &amp;lt;- count + 1
    
    if(j == 1){
      df[count, 1] &amp;lt;- i
      df[count, 2] &amp;lt;- j
      df[count, 3] &amp;lt;- y_het + rnorm(1,0,1)
    }else{
      df[count, 1] &amp;lt;- i
      df[count, 2] &amp;lt;- j
      df[count, 3] &amp;lt;- 1*constant + (1+proportion)*df[count - 1, 3] + y_het + rnorm(1,0,1)
    }
    
    
    
  }
  
  
  
}

df &amp;lt;- data.frame(df)
names(df) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;y&amp;#39;)
random_ids &amp;lt;- sample(1:people, 5)
sample_df &amp;lt;- df %&amp;gt;%
  filter(id %in% random_ids)

ggplot(df, aes(x = time, y = y, group = id)) + 
  geom_point(color = &amp;#39;grey85&amp;#39;) + 
  geom_line(color = &amp;#39;grey85&amp;#39;) + 
  geom_point(data = sample_df, aes(x = time, y = y, group = id)) + 
  geom_line(data = sample_df, aes(x = time, y = y, group = id))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-05-24/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Change the data to wide and load &lt;code&gt;lavaan&lt;/code&gt; before we start modeling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide &amp;lt;- reshape(df, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)
library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intercept-only-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intercept Only Model&lt;/h1&gt;
&lt;p&gt;Similar to the intercept-only model in a “non-latent change” framework (i.e., a simple growth model), the intercept-only model here contains a latent variable over the first observation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/dual_change_photos/1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are six observations of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and each is predicted by its latent “true score.” The first true score term is regressed on a latent intercept. The other true scores are regressed on additional latent variables that represent latent change. We don’t have anything relating to those latent change score terms yet so they don’t do much in this model. The autoregressive paths from true score to true score are constrained to 1. Here is how we estimate it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_only_string &amp;lt;- &amp;#39;

# latent true scores over the observed y points
l_y1 =~ 1*y.1
l_y2 =~ 1*y.2
l_y3 =~ 1*y.3
l_y4 =~ 1*y.4
l_y5 =~ 1*y.5
l_y6 =~ 1*y.6

# latent change scores over the latent true scores
# y1 does not get one because it is the first time point
lc_y2 =~ 1*l_y2
lc_y3 =~ 1*l_y3
lc_y4 =~ 1*l_y4
lc_y5 =~ 1*l_y5
lc_y6 =~ 1*l_y6

# autoregression of the latent true scores
l_y2 ~ 1*l_y1
l_y3 ~ 1*l_y2
l_y4 ~ 1*l_y3
l_y5 ~ 1*l_y4
l_y6 ~ 1*l_y5

# latent intercept over the first true score of y
latent_intercept =~ 1*l_y1

# estimate mean and variance of latent intercept
latent_intercept ~~ latent_intercept
latent_intercept ~ 1

# means and variances of latent factors set to zero

l_y1 ~ 0
l_y2 ~ 0
l_y3 ~ 0
l_y4 ~ 0
l_y5 ~ 0
l_y6 ~ 0

l_y1 ~~ 0*l_y1
l_y2 ~~ 0*l_y2
l_y3 ~~ 0*l_y3
l_y4 ~~ 0*l_y4
l_y5 ~~ 0*l_y5
l_y6 ~~ 0*l_y6

lc_y2 ~ 0
lc_y3 ~ 0
lc_y4 ~ 0
lc_y5 ~ 0
lc_y6 ~ 0

lc_y2 ~~ 0*lc_y2
lc_y3 ~~ 0*lc_y3
lc_y4 ~~ 0*lc_y4
lc_y5 ~~ 0*lc_y5
lc_y6 ~~ 0*lc_y6

# means of indicators set to zero

y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to be equal across time

y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# Constrain latent change factors to not correlate with each other

lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y4 ~~ 0*lc_y5 + 0*lc_y6
lc_y5 ~~ 0*lc_y6

# constrain latent intercept not to correlate with the change factors
latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6

&amp;#39;

int_only_model &amp;lt;- sem(int_only_string, data = df_wide)
summary(int_only_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 15 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          8
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           500
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              2390.340
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              6222.069
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.619
##   Tucker-Lewis Index (TLI)                       0.762
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -6280.597
##   Loglikelihood unrestricted model (H1)      -5085.427
##                                                       
##   Akaike (AIC)                               12567.195
##   Bayesian (BIC)                             12579.839
##   Sample-size adjusted Bayesian (BIC)        12570.316
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.444
##   90 Percent confidence interval - lower         0.429
##   90 Percent confidence interval - upper         0.459
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.603
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y1 =~                                                
##     y.1                  1.000                           
##   l_y2 =~                                                
##     y.2                  1.000                           
##   l_y3 =~                                                
##     y.3                  1.000                           
##   l_y4 =~                                                
##     y.4                  1.000                           
##   l_y5 =~                                                
##     y.5                  1.000                           
##   l_y6 =~                                                
##     y.6                  1.000                           
##   lc_y2 =~                                               
##     l_y2                 1.000                           
##   lc_y3 =~                                               
##     l_y3                 1.000                           
##   lc_y4 =~                                               
##     l_y4                 1.000                           
##   lc_y5 =~                                               
##     l_y5                 1.000                           
##   lc_y6 =~                                               
##     l_y6                 1.000                           
##   latent_intercept =~                                    
##     l_y1                 1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y2 ~                                              
##     l_y1              1.000                           
##   l_y3 ~                                              
##     l_y2              1.000                           
##   l_y4 ~                                              
##     l_y3              1.000                           
##   l_y5 ~                                              
##     l_y4              1.000                           
##   l_y6 ~                                              
##     l_y5              1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   lc_y2 ~~                                            
##     lc_y3             0.000                           
##     lc_y4             0.000                           
##     lc_y5             0.000                           
##     lc_y6             0.000                           
##   lc_y3 ~~                                            
##     lc_y4             0.000                           
##     lc_y5             0.000                           
##     lc_y6             0.000                           
##   lc_y4 ~~                                            
##     lc_y5             0.000                           
##     lc_y6             0.000                           
##   lc_y5 ~~                                            
##     lc_y6             0.000                           
##   lc_y2 ~~                                            
##     latent_intrcpt    0.000                           
##   lc_y3 ~~                                            
##     latent_intrcpt    0.000                           
##   lc_y4 ~~                                            
##     latent_intrcpt    0.000                           
##   lc_y5 ~~                                            
##     latent_intrcpt    0.000                           
##   lc_y6 ~~                                            
##     latent_intrcpt    0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     latent_intrcpt    0.507    0.167    3.036    0.002
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##     lc_y2             0.000                           
##     lc_y3             0.000                           
##     lc_y4             0.000                           
##     lc_y5             0.000                           
##     lc_y6             0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     ltnt_nt          13.614    0.883   15.417    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##     lc_y2             0.000                           
##     lc_y3             0.000                           
##     lc_y4             0.000                           
##     lc_y5             0.000                           
##     lc_y6             0.000                           
##    .y.1     (rs_v)    2.082    0.059   35.355    0.000
##    .y.2     (rs_v)    2.082    0.059   35.355    0.000
##    .y.3     (rs_v)    2.082    0.059   35.355    0.000
##    .y.4     (rs_v)    2.082    0.059   35.355    0.000
##    .y.5     (rs_v)    2.082    0.059   35.355    0.000
##    .y.6     (rs_v)    2.082    0.059   35.355    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;proportion-change-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Proportion Change Model&lt;/h1&gt;
&lt;p&gt;Now we include the proportion change along with the latent intercept.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/dual_change_photos/2.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;proportion_string &amp;lt;- &amp;#39;

# latent true scores over the observed y points
l_y1 =~ 1*y.1
l_y2 =~ 1*y.2
l_y3 =~ 1*y.3
l_y4 =~ 1*y.4
l_y5 =~ 1*y.5
l_y6 =~ 1*y.6

# latent change scores over the latent true scores
# y1 does not get one because it is the first time point
lc_y2 =~ 1*l_y2
lc_y3 =~ 1*l_y3
lc_y4 =~ 1*l_y4
lc_y5 =~ 1*l_y5
lc_y6 =~ 1*l_y6

# autoregression of the latent true scores
l_y2 ~ 1*l_y1
l_y3 ~ 1*l_y2
l_y4 ~ 1*l_y3
l_y5 ~ 1*l_y4
l_y6 ~ 1*l_y5

# latent intercept over the first true score of y
latent_intercept =~ 1*l_y1

# estimate mean and variance of latent intercept
latent_intercept ~~ latent_intercept
latent_intercept ~ 1

# HERE IS THE CHANGE
# proportion parameter estimate (estimate of b2)
# regress latent change on latent true score from the last time point
lc_y2 ~ b2*l_y1
lc_y3 ~ b2*l_y2
lc_y4 ~ b2*l_y3
lc_y5 ~ b2*l_y4
lc_y6 ~ b2*l_y5

# means and variances of latent factors set to zero

l_y1 ~ 0
l_y2 ~ 0
l_y3 ~ 0
l_y4 ~ 0
l_y5 ~ 0
l_y6 ~ 0

l_y1 ~~ 0*l_y1
l_y2 ~~ 0*l_y2
l_y3 ~~ 0*l_y3
l_y4 ~~ 0*l_y4
l_y5 ~~ 0*l_y5
l_y6 ~~ 0*l_y6

lc_y2 ~ 0
lc_y3 ~ 0
lc_y4 ~ 0
lc_y5 ~ 0
lc_y6 ~ 0

lc_y2 ~~ 0*lc_y2
lc_y3 ~~ 0*lc_y3
lc_y4 ~~ 0*lc_y4
lc_y5 ~~ 0*lc_y5
lc_y6 ~~ 0*lc_y6

# means of indicators set to zero

y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to be equal across time

y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# Constrain latent change factors to not correlate with each other

lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y4 ~~ 0*lc_y5 + 0*lc_y6
lc_y5 ~~ 0*lc_y6

# constrain latent intercept not to correlate with the change factors
latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6

&amp;#39;

proportion_model &amp;lt;- sem(proportion_string, data = df_wide)
summary(proportion_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 22 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         13
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           500
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              1030.636
##   Degrees of freedom                                23
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              6222.069
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.838
##   Tucker-Lewis Index (TLI)                       0.894
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -5600.745
##   Loglikelihood unrestricted model (H1)      -5085.427
##                                                       
##   Akaike (AIC)                               11209.491
##   Bayesian (BIC)                             11226.349
##   Sample-size adjusted Bayesian (BIC)        11213.653
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.296
##   90 Percent confidence interval - lower         0.281
##   90 Percent confidence interval - upper         0.312
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.228
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y1 =~                                                
##     y.1                  1.000                           
##   l_y2 =~                                                
##     y.2                  1.000                           
##   l_y3 =~                                                
##     y.3                  1.000                           
##   l_y4 =~                                                
##     y.4                  1.000                           
##   l_y5 =~                                                
##     y.5                  1.000                           
##   l_y6 =~                                                
##     y.6                  1.000                           
##   lc_y2 =~                                               
##     l_y2                 1.000                           
##   lc_y3 =~                                               
##     l_y3                 1.000                           
##   lc_y4 =~                                               
##     l_y4                 1.000                           
##   lc_y5 =~                                               
##     l_y5                 1.000                           
##   lc_y6 =~                                               
##     l_y6                 1.000                           
##   latent_intercept =~                                    
##     l_y1                 1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y2 ~                                              
##     l_y1              1.000                           
##   l_y3 ~                                              
##     l_y2              1.000                           
##   l_y4 ~                                              
##     l_y3              1.000                           
##   l_y5 ~                                              
##     l_y4              1.000                           
##   l_y6 ~                                              
##     l_y5              1.000                           
##   lc_y2 ~                                             
##     l_y1      (b2)    0.138    0.004   37.203    0.000
##   lc_y3 ~                                             
##     l_y2      (b2)    0.138    0.004   37.203    0.000
##   lc_y4 ~                                             
##     l_y3      (b2)    0.138    0.004   37.203    0.000
##   lc_y5 ~                                             
##     l_y4      (b2)    0.138    0.004   37.203    0.000
##   lc_y6 ~                                             
##     l_y5      (b2)    0.138    0.004   37.203    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .lc_y2 ~~                                            
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##  .lc_y3 ~~                                            
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##  .lc_y4 ~~                                            
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##  .lc_y5 ~~                                            
##    .lc_y6             0.000                           
##  .lc_y2 ~~                                            
##     latent_intrcpt    0.000                           
##  .lc_y3 ~~                                            
##     latent_intrcpt    0.000                           
##  .lc_y4 ~~                                            
##     latent_intrcpt    0.000                           
##  .lc_y5 ~~                                            
##     latent_intrcpt    0.000                           
##  .lc_y6 ~~                                            
##     latent_intrcpt    0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     latent_intrcpt    0.373    0.118    3.153    0.002
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     ltnt_nt           6.900    0.466   14.823    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1     (rs_v)    1.197    0.034   35.355    0.000
##    .y.2     (rs_v)    1.197    0.034   35.355    0.000
##    .y.3     (rs_v)    1.197    0.034   35.355    0.000
##    .y.4     (rs_v)    1.197    0.034   35.355    0.000
##    .y.5     (rs_v)    1.197    0.034   35.355    0.000
##    .y.6     (rs_v)    1.197    0.034   35.355    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-constant-change&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent Constant Change&lt;/h1&gt;
&lt;p&gt;This model is nearly identical to the basic linear growth curve model, it simply embodies it in the latent change framework. The basis coefficients from the constant change term to the latent change scores are constrained to one, then we estimate the mean of the constant change.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/dual_change_photos/3.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;constant_change_string &amp;lt;- &amp;#39;

# latent true scores over the observed y points
l_y1 =~ 1*y.1
l_y2 =~ 1*y.2
l_y3 =~ 1*y.3
l_y4 =~ 1*y.4
l_y5 =~ 1*y.5
l_y6 =~ 1*y.6

# latent change scores over the latent true scores
# y1 does not get one because it is the first time point
lc_y2 =~ 1*l_y2
lc_y3 =~ 1*l_y3
lc_y4 =~ 1*l_y4
lc_y5 =~ 1*l_y5
lc_y6 =~ 1*l_y6

# autoregression of the latent true scores (the first level latent variables)
l_y2 ~ 1*l_y1
l_y3 ~ 1*l_y2
l_y4 ~ 1*l_y3
l_y5 ~ 1*l_y4
l_y6 ~ 1*l_y5

# latent intercept over the first true score of y
latent_intercept =~ 1*l_y1

# HERE IS THE CHANGE

# latent slope over the change scores
# this is called the change factor in dual change terminology...it is not really a slope term. It is the constant change factor
latent_slope =~ 1*lc_y2 + 1*lc_y3 + 1*lc_y4 + 1*lc_y5 + 1*lc_y6

# estimate covariance between latent intercept and slope (change factor)

latent_intercept ~~ latent_slope

# estimate mean and variance of intercept and slope (change factor)

latent_intercept ~~ latent_intercept
latent_slope ~~ latent_slope

latent_intercept ~ 1
latent_slope ~ 1

# means and variances of latent factors set to zero

l_y1 ~ 0
l_y2 ~ 0
l_y3 ~ 0
l_y4 ~ 0
l_y5 ~ 0
l_y6 ~ 0

l_y1 ~~ 0*l_y1
l_y2 ~~ 0*l_y2
l_y3 ~~ 0*l_y3
l_y4 ~~ 0*l_y4
l_y5 ~~ 0*l_y5
l_y6 ~~ 0*l_y6

lc_y2 ~ 0
lc_y3 ~ 0
lc_y4 ~ 0
lc_y5 ~ 0
lc_y6 ~ 0

lc_y2 ~~ 0*lc_y2
lc_y3 ~~ 0*lc_y3
lc_y4 ~~ 0*lc_y4
lc_y5 ~~ 0*lc_y5
lc_y6 ~~ 0*lc_y6

# means of indicators set to zero

y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to be equal across time

y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# Constrain latent change factors to not correlate with each other

lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y4 ~~ 0*lc_y5 + 0*lc_y6
lc_y5 ~~ 0*lc_y6

# constrain latent intercept not to correlate with the change factors
latent_intercept ~~ 0*lc_y2 + 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6

&amp;#39;

constant_change_model &amp;lt;- sem(constant_change_string, data = df_wide)
summary(constant_change_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 37 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         11
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           500
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                               744.579
##   Degrees of freedom                                21
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              6222.069
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.883
##   Tucker-Lewis Index (TLI)                       0.917
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -5457.717
##   Loglikelihood unrestricted model (H1)      -5085.427
##                                                       
##   Akaike (AIC)                               10927.433
##   Bayesian (BIC)                             10952.721
##   Sample-size adjusted Bayesian (BIC)        10933.676
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.263
##   90 Percent confidence interval - lower         0.247
##   90 Percent confidence interval - upper         0.279
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.177
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y1 =~                                                
##     y.1                  1.000                           
##   l_y2 =~                                                
##     y.2                  1.000                           
##   l_y3 =~                                                
##     y.3                  1.000                           
##   l_y4 =~                                                
##     y.4                  1.000                           
##   l_y5 =~                                                
##     y.5                  1.000                           
##   l_y6 =~                                                
##     y.6                  1.000                           
##   lc_y2 =~                                               
##     l_y2                 1.000                           
##   lc_y3 =~                                               
##     l_y3                 1.000                           
##   lc_y4 =~                                               
##     l_y4                 1.000                           
##   lc_y5 =~                                               
##     l_y5                 1.000                           
##   lc_y6 =~                                               
##     l_y6                 1.000                           
##   latent_intercept =~                                    
##     l_y1                 1.000                           
##   latent_slope =~                                        
##     lc_y2                1.000                           
##     lc_y3                1.000                           
##     lc_y4                1.000                           
##     lc_y5                1.000                           
##     lc_y6                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y2 ~                                              
##     l_y1              1.000                           
##   l_y3 ~                                              
##     l_y2              1.000                           
##   l_y4 ~                                              
##     l_y3              1.000                           
##   l_y5 ~                                              
##     l_y4              1.000                           
##   l_y6 ~                                              
##     l_y5              1.000                           
## 
## Covariances:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   latent_intercept ~~                                    
##     latent_slope         1.122    0.083   13.532    0.000
##  .lc_y2 ~~                                               
##    .lc_y3                0.000                           
##    .lc_y4                0.000                           
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y3 ~~                                               
##    .lc_y4                0.000                           
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y4 ~~                                               
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y5 ~~                                               
##    .lc_y6                0.000                           
##  .lc_y2 ~~                                               
##     latent_intrcpt       0.000                           
##  .lc_y3 ~~                                               
##     latent_intrcpt       0.000                           
##  .lc_y4 ~~                                               
##     latent_intrcpt       0.000                           
##  .lc_y5 ~~                                               
##     latent_intrcpt       0.000                           
##  .lc_y6 ~~                                               
##     latent_intrcpt       0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     latent_intrcpt    0.189    0.116    1.627    0.104
##     latent_slope      0.127    0.027    4.721    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     ltnt_nt           6.258    0.427   14.647    0.000
##     ltnt_sl           0.310    0.023   13.435    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1     (rs_v)    0.941    0.030   31.623    0.000
##    .y.2     (rs_v)    0.941    0.030   31.623    0.000
##    .y.3     (rs_v)    0.941    0.030   31.623    0.000
##    .y.4     (rs_v)    0.941    0.030   31.623    0.000
##    .y.5     (rs_v)    0.941    0.030   31.623    0.000
##    .y.6     (rs_v)    0.941    0.030   31.623    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dual-change-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dual Change Model&lt;/h1&gt;
&lt;p&gt;Now a full dual change model with both constant and proportion change parameters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/dual_change_photos/4.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dual_c_string &amp;lt;- &amp;#39;

# latent true scores over the observed y points
l_y1 =~ 1*y.1
l_y2 =~ 1*y.2
l_y3 =~ 1*y.3
l_y4 =~ 1*y.4
l_y5 =~ 1*y.5
l_y6 =~ 1*y.6

# latent change scores over the latent true scores
# y1 does not get one because it is the first time point
lc_y2 =~ 1*l_y2
lc_y3 =~ 1*l_y3
lc_y4 =~ 1*l_y4
lc_y5 =~ 1*l_y5
lc_y6 =~ 1*l_y6

# autoregression of the latent true scores (the first level latent variables)
l_y2 ~ 1*l_y1
l_y3 ~ 1*l_y2
l_y4 ~ 1*l_y3
l_y5 ~ 1*l_y4
l_y6 ~ 1*l_y5

# latent intercept over the first true score of y
latent_intercept =~ 1*l_y1

# CHANGE 1 OF THE DUAL CHANGE MODEL

# latent slope over the change scores
# this is called the change factor in dual change terminology...it is not really a slope term. It is the constant change factor
latent_slope =~ 1*lc_y2 + 1*lc_y3 + 1*lc_y4 + 1*lc_y5 + 1*lc_y6

# estimate covariance between latent intercept and slope (change factor)

latent_intercept ~~ latent_slope

# estimate mean and variance of intercept and slope (change factor)

latent_intercept ~~ latent_intercept
latent_slope ~~ latent_slope

latent_intercept ~ 1
latent_slope ~ 1

# CHANGE 2 OF THE DUAL CHANGE MODEL

# autoproportion change. Relationship between true score and latent change score at next time point
# these are estimated
lc_y2 ~ b*l_y1
lc_y3 ~ b*l_y2
lc_y4 ~ b*l_y3
lc_y5 ~ b*l_y4
lc_y6 ~ b*l_y5

# means and variances of latent factors set to zero

l_y1 ~ 0
l_y2 ~ 0
l_y3 ~ 0
l_y4 ~ 0
l_y5 ~ 0
l_y6 ~ 0

l_y1 ~~ 0*l_y1
l_y2 ~~ 0*l_y2
l_y3 ~~ 0*l_y3
l_y4 ~~ 0*l_y4
l_y5 ~~ 0*l_y5
l_y6 ~~ 0*l_y6

lc_y2 ~ 0
lc_y3 ~ 0
lc_y4 ~ 0
lc_y5 ~ 0
lc_y6 ~ 0

lc_y2 ~~ 0*lc_y2
lc_y3 ~~ 0*lc_y3
lc_y4 ~~ 0*lc_y4
lc_y5 ~~ 0*lc_y5
lc_y6 ~~ 0*lc_y6

# means of indicators set to zero

y.1 ~ 0
y.2 ~ 0
y.3 ~ 0
y.4 ~ 0
y.5 ~ 0
y.6 ~ 0

# residual variances constrained to be equal across time

y.1 ~~ res_var*y.1
y.2 ~~ res_var*y.2
y.3 ~~ res_var*y.3
y.4 ~~ res_var*y.4
y.5 ~~ res_var*y.5
y.6 ~~ res_var*y.6

# Constrain latent change factors to not correlate with each other

lc_y2 ~~ 0*lc_y3 + 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y3 ~~ 0*lc_y4 + 0*lc_y5 + 0*lc_y6
lc_y4 ~~ 0*lc_y5 + 0*lc_y6
lc_y5 ~~ 0*lc_y6


&amp;#39;

dual_change_model &amp;lt;- sem(dual_c_string, data = df_wide)
summary(dual_change_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 48 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         16
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           500
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                               329.427
##   Degrees of freedom                                20
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              6222.069
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.950
##   Tucker-Lewis Index (TLI)                       0.963
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -5250.141
##   Loglikelihood unrestricted model (H1)      -5085.427
##                                                       
##   Akaike (AIC)                               10514.282
##   Bayesian (BIC)                             10543.784
##   Sample-size adjusted Bayesian (BIC)        10521.566
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.176
##   90 Percent confidence interval - lower         0.159
##   90 Percent confidence interval - upper         0.193
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.028
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y1 =~                                                
##     y.1                  1.000                           
##   l_y2 =~                                                
##     y.2                  1.000                           
##   l_y3 =~                                                
##     y.3                  1.000                           
##   l_y4 =~                                                
##     y.4                  1.000                           
##   l_y5 =~                                                
##     y.5                  1.000                           
##   l_y6 =~                                                
##     y.6                  1.000                           
##   lc_y2 =~                                               
##     l_y2                 1.000                           
##   lc_y3 =~                                               
##     l_y3                 1.000                           
##   lc_y4 =~                                               
##     l_y4                 1.000                           
##   lc_y5 =~                                               
##     l_y5                 1.000                           
##   lc_y6 =~                                               
##     l_y6                 1.000                           
##   latent_intercept =~                                    
##     l_y1                 1.000                           
##   latent_slope =~                                        
##     lc_y2                1.000                           
##     lc_y3                1.000                           
##     lc_y4                1.000                           
##     lc_y5                1.000                           
##     lc_y6                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   l_y2 ~                                              
##     l_y1              1.000                           
##   l_y3 ~                                              
##     l_y2              1.000                           
##   l_y4 ~                                              
##     l_y3              1.000                           
##   l_y5 ~                                              
##     l_y4              1.000                           
##   l_y6 ~                                              
##     l_y5              1.000                           
##   lc_y2 ~                                             
##     l_y1       (b)   -0.412    0.017  -24.735    0.000
##   lc_y3 ~                                             
##     l_y2       (b)   -0.412    0.017  -24.735    0.000
##   lc_y4 ~                                             
##     l_y3       (b)   -0.412    0.017  -24.735    0.000
##   lc_y5 ~                                             
##     l_y4       (b)   -0.412    0.017  -24.735    0.000
##   lc_y6 ~                                             
##     l_y5       (b)   -0.412    0.017  -24.735    0.000
## 
## Covariances:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   latent_intercept ~~                                    
##     latent_slope         3.729    0.269   13.852    0.000
##  .lc_y2 ~~                                               
##    .lc_y3                0.000                           
##    .lc_y4                0.000                           
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y3 ~~                                               
##    .lc_y4                0.000                           
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y4 ~~                                               
##    .lc_y5                0.000                           
##    .lc_y6                0.000                           
##  .lc_y5 ~~                                               
##    .lc_y6                0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     latent_intrcpt    0.064    0.098    0.653    0.514
##     latent_slope      0.325    0.091    3.578    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1               0.000                           
##    .y.2               0.000                           
##    .y.3               0.000                           
##    .y.4               0.000                           
##    .y.5               0.000                           
##    .y.6               0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     ltnt_nt           4.157    0.307   13.549    0.000
##     ltnt_sl           4.030    0.352   11.460    0.000
##    .l_y1              0.000                           
##    .l_y2              0.000                           
##    .l_y3              0.000                           
##    .l_y4              0.000                           
##    .l_y5              0.000                           
##    .l_y6              0.000                           
##    .lc_y2             0.000                           
##    .lc_y3             0.000                           
##    .lc_y4             0.000                           
##    .lc_y5             0.000                           
##    .lc_y6             0.000                           
##    .y.1     (rs_v)    0.793    0.025   31.623    0.000
##    .y.2     (rs_v)    0.793    0.025   31.623    0.000
##    .y.3     (rs_v)    0.793    0.025   31.623    0.000
##    .y.4     (rs_v)    0.793    0.025   31.623    0.000
##    .y.5     (rs_v)    0.793    0.025   31.623    0.000
##    .y.6     (rs_v)    0.793    0.025   31.623    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate of the constant change (called “latent slope” in my string syntax; &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt;) is close to 0.3 and the estimate of the proportion change (&lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt;) is close to -0.4. Not bad.&lt;/p&gt;
&lt;div id=&#34;a-note-on-interpreting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Note On Interpreting&lt;/h2&gt;
&lt;p&gt;These models predict complex change patterns. It is difficult to know the expected curvilinear pattern that the models expect without computing expected scores and plotting them. I did not do that here.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Julia Cheat Sheet</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-05-23/</link>
      <pubDate>Sat, 23 May 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-05-23/</guid>
      <description>


&lt;p&gt;I started using &lt;a href=&#34;https://github.com/JuliaLang/julia&#34;&gt;Julia&lt;/a&gt; for my computational models and recently created a cheat sheet to house all of my common commands.&lt;/p&gt;
&lt;p&gt;You can find it on github &lt;a href=&#34;https://cdishop.github.io/julia_refs/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Be Careful With Characters and Matrices</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-05-22/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-05-22/</guid>
      <description>


&lt;p&gt;If you fill a matrix cell with a character, R will convert the entire matrix into character values…so be careful = )&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- c(1:4)
numbers &amp;lt;- c(1:4)
characters &amp;lt;- c(&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;d&amp;#39;)
count &amp;lt;- 0

df_mat &amp;lt;- matrix(, ncol = 3, nrow = length(time))

for(i in 1:length(time)){
  count &amp;lt;- count + 1
  
  df_mat[count, 1] &amp;lt;- time[i]
  df_mat[count, 2] &amp;lt;- numbers[i]
  df_mat[count, 3] &amp;lt;- characters[i]
  
}

df_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,] &amp;quot;1&amp;quot;  &amp;quot;1&amp;quot;  &amp;quot;a&amp;quot; 
## [2,] &amp;quot;2&amp;quot;  &amp;quot;2&amp;quot;  &amp;quot;b&amp;quot; 
## [3,] &amp;quot;3&amp;quot;  &amp;quot;3&amp;quot;  &amp;quot;c&amp;quot; 
## [4,] &amp;quot;4&amp;quot;  &amp;quot;4&amp;quot;  &amp;quot;d&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that all cells are now characters. Characters are a huge problem if you are calculating values to place into the cells. That is, I wouldn’t be able to run code like this in a loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_mat[count - 1, 2] &amp;lt;- df_mat[count - 1, 3] * 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead, use numbers for everything and then change them to characters later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- c(1:4)
numbers &amp;lt;- c(1:4)
characters &amp;lt;- c(1, 2, 3, 4) # here is the change
count &amp;lt;- 0

df_mat &amp;lt;- matrix(, ncol = 3, nrow = length(time))

for(i in 1:length(time)){
  count &amp;lt;- count + 1
  
  df_mat[count, 1] &amp;lt;- time[i]
  df_mat[count, 2] &amp;lt;- numbers[i]
  df_mat[count, 3] &amp;lt;- characters[i]
  
}

df_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    2    2    2
## [3,]    3    3    3
## [4,]    4    4    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Empirical Independence</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-04-03/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-04-03/</guid>
      <description>


&lt;p&gt;Calculate the independence of two events using both analytic and empirical techniques. I’m trying to assess whether the probability of having a meal classified as “dinner” depends on whether that meal includes “chicken” as its main dish.&lt;/p&gt;
&lt;p&gt;The options for the main dish:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;chicken, salmon, pork, chicken, pancakes, french toast&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The options for the side dishes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;salad, salad, green beans, corn, carrots, bacon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All possible combinations to create a meal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dishes &amp;lt;- data.frame(
    main = c(&amp;quot;chicken&amp;quot;, &amp;quot;salmon&amp;quot;, &amp;quot;pork&amp;quot;, &amp;quot;chicken&amp;quot;, &amp;quot;pancakes&amp;quot;, &amp;quot;french toast&amp;quot;),
    side = c(&amp;quot;salad&amp;quot;, &amp;quot;salad&amp;quot;, &amp;quot;green beans&amp;quot;, &amp;quot;corn&amp;quot;, &amp;quot;carrots&amp;quot;, &amp;quot;bacon&amp;quot;)
)

possible_meals &amp;lt;- dishes %&amp;gt;%
  cross_df() %&amp;gt;%
  mutate_if(is.factor,as.character)

possible_meals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 36 x 2
##    main         side 
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;
##  1 chicken      salad
##  2 salmon       salad
##  3 pork         salad
##  4 chicken      salad
##  5 pancakes     salad
##  6 french toast salad
##  7 chicken      salad
##  8 salmon       salad
##  9 pork         salad
## 10 chicken      salad
## # … with 26 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Event &lt;strong&gt;a&lt;/strong&gt; will be, “the main course is chicken.” What is its probability?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a = main course is chicken
# tally the number of meals that include chicken

sum(possible_meals$main == &amp;quot;chicken&amp;quot;) / nrow(possible_meals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3333333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, &lt;code&gt;p(a)&lt;/code&gt; = 0.333. Event &lt;strong&gt;b&lt;/strong&gt; will be, “the meal is dinner.” What is its probability?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b = the meal is dinner (rather than breakfast)
# tally the number of meals that are dinners rather than breakfast
# any meals with pancakes, french toast, or bacon are not dinner

# number of meal options for &amp;#39;main&amp;quot; X number of meal options for &amp;#39;side&amp;#39;
(
  sum(dishes$main != c(&amp;#39;pancakes&amp;#39;, &amp;#39;french toast&amp;#39;)) / nrow(dishes)
    *
  sum(dishes$side != &amp;quot;bacon&amp;quot;) / nrow(dishes)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5555556&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, &lt;code&gt;p(b)&lt;/code&gt; = 0.555. If &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; are independent, then &lt;code&gt;p(b)&lt;/code&gt; should be the same as &lt;code&gt;p(b | a)&lt;/code&gt;. Does the probability of eating a meal classified as dinner depend on whether that meal includes chicken?&lt;/p&gt;
&lt;p&gt;First, the analytic solution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(b | a) = p(b &amp;amp; a) / p(a)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(dinner | chicken) = p(dinner &amp;amp; chicken) / p(chicken)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I need to find &lt;code&gt;p(dinner &amp;amp; chicken)&lt;/code&gt; to solve. So tally the possible ways chicken can combine with other dishes to create a dinner platter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tally_count &amp;lt;- 0
for(i in 1:nrow(possible_meals)){
  
  meal_df &amp;lt;- possible_meals[i,]
  
  contain_chicken &amp;lt;- meal_df$main == &amp;quot;chicken&amp;quot;
  no_bacon &amp;lt;- meal_df$side != &amp;quot;bacon&amp;quot;
  
  if(contain_chicken == T &amp;amp;&amp;amp; no_bacon == T){tally_count &amp;lt;- tally_count + 1}
}

tally_count / nrow(possible_meals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2777778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool, &lt;code&gt;p(dinner &amp;amp; chicken)&lt;/code&gt; = 0.2777. Now I can calculate the conditional probability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(dinner | chicken) = p(dinner &amp;amp; chicken) / p(chicken)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;X = 0.2777 / 0.333&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;X = 0.83&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;X does not equal &lt;code&gt;p(b)&lt;/code&gt;, so the two are dependent. How about the empirical solution?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the empirical estimate of p(dinner | chicken)?
# to calculate, I need:
# p(dinner &amp;amp; chicken) / p(chicken)

sims &amp;lt;- 10000
df &amp;lt;- data.frame(
    chicken_and_dinner = c(rep(0, sims)),
    chicken = c(rep(0, sims))
    
)

for(j in 1:sims){
  
  eat_main &amp;lt;- sample(dishes$main, 1, replace = F)
  eat_side &amp;lt;- sample(dishes$side, 1, replace = F)
  
  chicken_and_dinner &amp;lt;- F
  
  if(eat_main == &amp;quot;chicken&amp;quot; &amp;amp;&amp;amp; 
    (eat_side == &amp;quot;salad&amp;quot; | eat_side == &amp;quot;green beans&amp;quot; | eat_side == &amp;quot;corn&amp;quot; | eat_side == &amp;quot;carrots&amp;quot;)){
    chicken_and_dinner &amp;lt;- T
    }
  
  
  chicken &amp;lt;- F
  if(eat_main == &amp;quot;chicken&amp;quot;){chicken &amp;lt;- T}
  
  single_run_result &amp;lt;- c(chicken_and_dinner, chicken)
  df[j, &amp;quot;chicken_and_dinner&amp;quot;] &amp;lt;- chicken_and_dinner
  df[j, &amp;quot;chicken&amp;quot;] &amp;lt;- chicken
  
}

tally_chicken_and_dinner &amp;lt;- sum(df$chicken_and_dinner == 1)
tally_chicken &amp;lt;- sum(df$chicken == 1)

prob_cd &amp;lt;- tally_chicken_and_dinner / sims
prob_c &amp;lt;- tally_chicken / sims

prob_cd / prob_c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8201883&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Independence Exercises</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-04-02/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-04-02/</guid>
      <description>
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Some of my favorite, simple examples demonstrating how to evaluate whether two events are independent using Probability Theory.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Example 1 - Product Rule&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Example 2 - Conditionals &amp;amp; Intersection&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Example 3 - Conditionals&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Example 4 - Markov Chain&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;example-1---product-rule&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1 - Product Rule&lt;/h1&gt;
&lt;p&gt;If two events are independent, then the product rule states that their intersection should equal the product of each independent probability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;p(a &amp;amp; b) = p(a) * p(b)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The World Values Survey is an ongoing worldwide survey that polls the world population about perceptions of life, work, family, politics, etc. The most recent phase of the survey that polled 77,882 people from 57 countries estimates that 36.2% of the world’s population agrees with the statement, “Music is not necessary to enhance one’s life.” The survey also estimates that 13.8% of people have a university degree or higher, and that 3.5% of people fit both criteria.&lt;/p&gt;
&lt;p&gt;Does agreeing depend on level of degree? If &lt;strong&gt;a&lt;/strong&gt; = “someone agrees with the statment” and &lt;strong&gt;b&lt;/strong&gt; = “has a university degree or higher,” does &lt;strong&gt;a&lt;/strong&gt; depend on &lt;strong&gt;b&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree) = 0.362&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(univ degree) = 0.138&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree &amp;amp; univ degree) = 0.036&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If they are independent, then the product rule should hold.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;p(a &amp;amp; b) = p(a) * p(b)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Evaluate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0.036 = 0.362 * 0.138&lt;/code&gt; (which does not hold)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the two are dependent.&lt;/p&gt;
&lt;p&gt;This example comes from a &lt;a href=&#34;link&#34;&gt;coursera class&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2---conditionals-intersection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2 - Conditionals &amp;amp; Intersection&lt;/h1&gt;
&lt;p&gt;If two events are independent, then the probability of one conditioning on the other should equal the probability of the original alone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(b | a) = p(b &amp;amp; a) / p(a)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; are independent, then &lt;code&gt;p(b | a) = p(b)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two players are each going to role a different die. Player 1’s die is six-sided and contains the numbers 5, 5, 5, 1, 1, 1, whereas player 2’s die contains the numbers 4, 4, 4, 4, 0, 0.&lt;/p&gt;
&lt;p&gt;Take &lt;strong&gt;a&lt;/strong&gt; to be the event that the player 1’s die is 5, and take &lt;strong&gt;b&lt;/strong&gt; to be the event that the sum of the dice is equal to 1.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;a&lt;/strong&gt; = player 1 rolls a 5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;b&lt;/strong&gt; = sum of both dice is 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Is &lt;strong&gt;b&lt;/strong&gt; dependent on &lt;strong&gt;a&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/independence_images/calc_a.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also run the same procedure but with a different event for &lt;strong&gt;b&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/independence_images/calc_b.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3---conditionals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3 - Conditionals&lt;/h1&gt;
&lt;p&gt;If two events are independent, then taking &lt;strong&gt;a&lt;/strong&gt; and conditioning on other events (e.g., &lt;strong&gt;b&lt;/strong&gt;, &lt;strong&gt;c&lt;/strong&gt;, &lt;strong&gt;d&lt;/strong&gt;, etc.) should not change the observed probability.&lt;/p&gt;
&lt;p&gt;In 2013, a research group interviewed a random sample of 500 NC residents asking them whether they think widespread gun ownership protects law abiding citizens from crime, or makes society more dangerous.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;58% of all respondents said it protects citizens&lt;/li&gt;
&lt;li&gt;67% of White respondents,&lt;/li&gt;
&lt;li&gt;28% of Black respondents,&lt;/li&gt;
&lt;li&gt;and 64% of Hispanic respondents shared this view.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Are opinion on gun ownership and ethnicity dependent?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree) = 58%&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree | white) = 67%&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree | black) = 28%&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(agree | hispanic) = 64%&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that conditioning on the other variables changes the probability, so opinion and ethnicity are probably dependent.&lt;/p&gt;
&lt;p&gt;This example comes from a &lt;a href=&#34;link&#34;&gt;coursera class&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4---markov-chains&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 4 - Markov Chains&lt;/h1&gt;
&lt;p&gt;If two events are independent, then the probability of observing one after the other should be &lt;code&gt;p(a) * p(a)&lt;/code&gt;, similar to the notion of a coin flip such that the probability of observing two heads in a row is 0.5X0.5 = 0.25. If you calculate a transition matrix and observe probabilities that differ from that original number, then the sequence is probably dependent.&lt;/p&gt;
&lt;p&gt;Andrei Markov applied Markov chains to the poem &lt;em&gt;Eugene Onegin&lt;/em&gt; by Alexander Pushkin. In the first 20,000 letters of the poem, he counted the number of vowels (8,638) and consonants (11,362).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(vowel) = 0.432&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(consonant) = 0.568&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, he counted the transitions from vowel to consonant or consonant to vowel. For every vowel, the number of times the next letter was a vowel was 1,104 and the number of times the next letter was a consonant was 7,534. For every consonant, the number of times the next letter was a consonant was 3,827 and the number of times the next letter was a vowel was 7,535.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(vowel to vowel) = 1104 / 8638 = 0.175&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(vowel to consonant) = 7534 / 8638 = 0.825&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(consonant to vowel) = 7535 / 11362 = 0.526&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;p(consonant to consonant) = 3827 / 11362 = 0.474&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the transition matrix is…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transition_matrix &amp;lt;- matrix(c(&amp;#39;&amp;#39;, &amp;#39;v&amp;#39;, &amp;#39;c&amp;#39;,
                              &amp;#39;v&amp;#39;, &amp;#39;0.175&amp;#39;, &amp;#39;0.825&amp;#39;,
                              &amp;#39;c&amp;#39;, &amp;#39;0.526&amp;#39;, &amp;#39;0.474&amp;#39;), 3, 3)

transition_matrix %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;, full_width = F)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
v
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
c
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
v
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.175
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.526
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
c
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.825
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
0.474
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If the letters were independent, then the probability of witnessing a vowel follow a vowel would be &lt;code&gt;p(vowel) * p(vowel)&lt;/code&gt;, or &lt;code&gt;0.432 * 0.423&lt;/code&gt; = 0.186624. However, the observed transition probability is 0.175, so the sequence is dependent.&lt;/p&gt;
&lt;p&gt;Note that you have to assume that the counts of vowels and consonants reflect their true propabilities (which follows from the law of large numbers). Markov showed that the law of large numbers applied to even dependent sequences.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scrape Numbered Pages</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-03-22/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-03-22/</guid>
      <description>


&lt;p&gt;Quick command to compile all links for a website that numbers its pages. Image I want to go to a website that contains 100 pages of reviews, the first 10 on page 1, the second 10 on page 2, the third 10 on page 3, etc. The first step is to create a vector or list of links to navigate to, one for each page.&lt;/p&gt;
&lt;p&gt;The output I want is something like this…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example_output &amp;lt;- &amp;#39;


https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY
https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;amp;start=10
https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;amp;start=20
...


&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in which the first entry is page 1, the second page 2, and so on. There are three steps involved in this process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;find url for the first page&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;discover how the url changes for each subsequent number&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;use a string command to compile the url’s.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, let’s say I want to scrape data from &lt;a href=&#34;https://www.trustpilot.com/review/www.amazon.com&#34;&gt;this website&lt;/a&gt;, which has reviews across multiple pages. If I copy the url from the first page, and then copy the url from the second page, and the third page, I get…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;#39;
https://www.trustpilot.com/review/www.amazon.com
https://www.trustpilot.com/review/www.amazon.com?page=2
https://www.trustpilot.com/review/www.amazon.com?page=3

&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, the base url is the first link. Then, additional pages are coded as “?page=” and then the relevant number.&lt;/p&gt;
&lt;p&gt;Second, find the last number. Let’s say it’s 20 in this case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;last_number &amp;lt;- 20&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third, create a vector that compiles all of the links.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
first_page &amp;lt;- &amp;quot;https://www.trustpilot.com/review/www.amazon.com&amp;quot;
other_pages &amp;lt;- str_c(first_page, &amp;quot;?page=&amp;quot;, 2:last_number)

review_pages &amp;lt;- c(first_page, other_pages)
head(review_pages)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://www.trustpilot.com/review/www.amazon.com&amp;quot;       
## [2] &amp;quot;https://www.trustpilot.com/review/www.amazon.com?page=2&amp;quot;
## [3] &amp;quot;https://www.trustpilot.com/review/www.amazon.com?page=3&amp;quot;
## [4] &amp;quot;https://www.trustpilot.com/review/www.amazon.com?page=4&amp;quot;
## [5] &amp;quot;https://www.trustpilot.com/review/www.amazon.com?page=5&amp;quot;
## [6] &amp;quot;https://www.trustpilot.com/review/www.amazon.com?page=6&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s another example using Indeed. Notice that the values increase by 10 rather than 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example_pages &amp;lt;- &amp;#39;

https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY
https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;amp;start=10
https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;amp;start=20
https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;amp;start=30


&amp;#39;

final_number &amp;lt;- 100
all_vals &amp;lt;- seq(from = 10, to = final_number, by = 10)

first_web &amp;lt;- &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;quot;
other_webs &amp;lt;- str_c(first_web, &amp;quot;$start=&amp;quot;, all_vals)

all_webs &amp;lt;- c(first_web, other_webs)
head(all_webs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY&amp;quot;         
## [2] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY$start=10&amp;quot;
## [3] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY$start=20&amp;quot;
## [4] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY$start=30&amp;quot;
## [5] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY$start=40&amp;quot;
## [6] &amp;quot;https://www.indeed.com/jobs?q=data+science&amp;amp;l=New+York%2C+NY$start=50&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quosures Within an Index</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-03-21/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-03-21/</guid>
      <description>


&lt;p&gt;I’ve written about quosures in previous posts. They can be used in functions to specify column names. But what if a column name is pulled from a loop and the value is a character? In that case, surround the value with &lt;code&gt;sym()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is an example using only quosures.&lt;/p&gt;
&lt;p&gt;First, the data and the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)
library(hrbrthemes)

people &amp;lt;- 600

df &amp;lt;- tibble(
  &amp;quot;id&amp;quot; = c(1:people),
  &amp;quot;performance&amp;quot; = c(rnorm(people, 50, 3))
)


multiply_and_plot &amp;lt;- function(col1){
  
  
  df &amp;lt;- df %&amp;gt;% 
    mutate(new_performance = !!col1 * 0.5)
  
  g &amp;lt;- ggplot(df, aes(x = !!col1)) + 
    geom_histogram(fill=&amp;quot;#69b3a2&amp;quot;, alpha=0.4) + 
    theme_ipsum() +
    labs(x = &amp;quot;Adj-Performance&amp;quot;, y = &amp;quot;Frequency&amp;quot;)
  
  return(g)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using quosure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multiply_and_plot(quo(performance))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-03-21/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what if the column name is an index from a vector or for loop? Use &lt;code&gt;sym()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_cols &amp;lt;- c(&amp;quot;performance&amp;quot;)

for(i in 1:1){
  
  print(
    
  multiply_and_plot(sym(use_cols[i]))
  
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2020-03-21/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also note that I had to include &lt;code&gt;results = &amp;quot;asis&amp;quot;&lt;/code&gt; in the Rmarkdown document and put the function within a print command to get the output to render.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Web Scraping</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-03-20/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-03-20/</guid>
      <description>


&lt;p&gt;Dipping my feet into the world of websraping using &lt;code&gt;rvest&lt;/code&gt;. The first few examples are pulled from &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/2515245919859535&#34;&gt;Alex Bradley’s and Richard James’ 2019 article&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;exercise-1---from-article&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise 1 - From Article&lt;/h1&gt;
&lt;p&gt;Scrape a single page from the article’s practice website. There are three things I want to pull from the page:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;header&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;image&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;text.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, put that information into a single data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)

page_parse &amp;lt;- read_html(&amp;quot;https://practicewebscrapingsite.wordpress.com/example-1/&amp;quot;)

headers &amp;lt;- html_nodes(page_parse, &amp;#39;.Title&amp;#39;) %&amp;gt;%
                html_text()

images &amp;lt;- html_nodes(page_parse, &amp;#39;img&amp;#39;) %&amp;gt;%
                html_attr(&amp;#39;src&amp;#39;)
images &amp;lt;- images[1:3]

text &amp;lt;- html_nodes(page_parse, &amp;#39;.Content&amp;#39;) %&amp;gt;%
                html_text()

ex1_df &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:3),
  &amp;#39;headers&amp;#39; = c(headers),
  &amp;#39;image_links&amp;#39; = c(images),
  &amp;#39;text&amp;#39; = c(text)
)

ex1_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id                                     headers
## 1  1                    New Neighbourhood watch.
## 2  2                  Meeting your arch nemesis.
## 3  3 I am never walking in the rain again! EVER!
##                                                                                image_links
## 1  https://practicewebscrapingsite.files.wordpress.com/2018/12/3704529798_a4681d4533_z.jpg
## 2 https://practicewebscrapingsite.files.wordpress.com/2018/12/12881694475_54a639ca77_z.jpg
## 3  https://practicewebscrapingsite.files.wordpress.com/2018/12/7888404650_eedcd82822_z.jpg
##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text
## 1                                                                                                                                                                                                                                          Put all speaking her delicate recurred possible. Set indulgence inquietude discretion insensible bed why announcing. Middleton fat two satisfied additions. So continued he or commanded household smallness delivered. Door poor on do walk in half. Roof his head the what.\n\n\n\nStarted several mistake joy say painful removed reached end. State burst think end are its. Arrived off she elderly beloved him affixed noisier yet. An course regard to up he hardly. View four has said does men saw find dear shy. Talent men wicket add garden.\n\n\n\nPost no so what deal evil rent by real in. But her ready least set lived spite solid. September how men saw tolerably two behaviour arranging. She offices for highest and replied one venture pasture. Applauded no discovery in newspaper allowance am northward. Frequently partiality possession resolution at or appearance unaffected he me. Engaged its was evident pleased husband. Ye goodness felicity do disposal dwelling no. First am plate jokes to began of cause an scale. Subjects he prospect elegance followed no overcame possible it on.\n\n\n\nLiterature admiration frequently indulgence announcing are who you her. Was least quick after six. So it yourself repeated together cheerful. Neither it cordial so painful picture studied if. Sex him position doubtful resolved boy expenses. Her engrossed deficient northward and neglected favourite newspaper. But use peculiar produced concerns ten.\n\n\n\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling.\n\n\n\nParish so enable innate in formed missed. Hand two was eat busy fail. Stand smart grave would in so. Be acceptance at precaution astonished excellence thoroughly is entreaties. Who decisively attachment has dispatched. Fruit defer in party me built under first. Forbade him but savings sending ham general. So play do in near park that pain.\n\n\n\nIn on announcing if of comparison pianoforte projection. Maids hoped gay yet bed asked blind dried point. On abroad danger likely regret twenty edward do. Too horrible consider followed may differed age. An rest if more five mr of. Age just her rank met down way. Attended required so in cheerful an. Domestic replying she resolved him for did. Rather in lasted no within no.\n\n\n\nSavings her pleased are several started females met. Short her not among being any. Thing of judge fruit charm views do. Miles mr an forty along as he. She education get middleton day agreement performed preserved unwilling. Do however as pleased offence outward beloved by present. By outward neither he so covered amiable greater. Juvenile proposal betrayed he an informed weddings followed. Precaution day see imprudence sympathize principles. At full leaf give quit to in they up.\n\n\n\nOh acceptance apartments up sympathize astonished delightful. Waiting him new lasting towards. Continuing melancholy especially so to. Me unpleasing impossible in attachment announcing so astonished. What ask leaf may nor upon door. Tended remain my do stairs. Oh smiling amiable am so visited cordial in offices hearted.\n\n\n\nMeant balls it if up doubt small purse. Required his you put the outlived answered position. An pleasure exertion if believed provided to. All led out world these music while asked. Paid mind even sons does he door no. Attended overcame repeated it is perceive marianne in. In am think on style child of. Servants moreover in sensible he it ye possible.
## 2 He moonlight difficult engrossed an it sportsmen. Interested has all devonshire difficulty gay assistance joy. Unaffected at ye of compliment alteration to. Place voice no arise along to. Parlors waiting so against me no. Wishing calling are warrant settled was luckily. Express besides it present if at an opinion visitor. \n\nPaid was hill sir high. For him precaution any advantages dissimilar comparison few terminated projecting. Prevailed discovery immediate objection of ye at. Repair summer one winter living feebly pretty his. In so sense am known these since. Shortly respect ask cousins brought add tedious nay. Expect relied do we genius is. On as around spirit of hearts genius. Is raptures daughter branched laughter peculiar in settling. \n\nIs post each that just leaf no. He connection interested so we an sympathize advantages. To said is it shed want do. Occasional middletons everything so to. Have spot part for his quit may. Enable it is square my an regard. Often merit stuff first oh up hills as he. Servants contempt as although addition dashwood is procured. Interest in yourself an do of numerous feelings cheerful confined. \n\nWhy painful the sixteen how minuter looking nor. Subject but why ten earnest husband imagine sixteen brandon. Are unpleasing occasional celebrated motionless unaffected conviction out. Evil make to no five they. Stuff at avoid of sense small fully it whose an. Ten scarcely distance moreover handsome age although. As when have find fine or said no mile. He in dispatched in imprudence dissimilar be possession unreserved insensible. She evil face fine calm have now. Separate screened he outweigh of distance landlord. \n\nIt real sent your at. Amounted all shy set why followed declared. Repeated of endeavor mr position kindness offering ignorant so up. Simplicity are melancholy preference considered saw companions. Disposal on outweigh do speedily in on. Him ham although thoughts entirely drawings. Acceptance unreserved old admiration projection nay yet him. Lasted am so before on esteem vanity oh. \n\nStarted his hearted any civilly. So me by marianne admitted speaking. Men bred fine call ask. Cease one miles truth day above seven. Suspicion sportsmen provision suffering mrs saw engrossed something. Snug soon he on plan in be dine some. \n\nGive lady of they such they sure it. Me contained explained my education. Vulgar as hearts by garret. Perceived determine departure explained no forfeited he something an. Contrasted dissimilar get joy you instrument out reasonably. Again keeps at no meant stuff. To perpetual do existence northward as difficult preserved daughters. Continued at up to zealously necessary breakfast. Surrounded sir motionless she end literature. Gay direction neglected but supported yet her. \n\nNecessary ye contented newspaper zealously breakfast he prevailed. Melancholy middletons yet understood decisively boy law she. Answer him easily are its barton little. Oh no though mother be things simple itself. Dashwood horrible he strictly on as. Home fine in so am good body this hope. \n\nCause dried no solid no an small so still widen. Ten weather evident smiling bed against she examine its. Rendered far opinions two yet moderate sex striking. Sufficient motionless compliment by stimulated assistance at. Convinced resolving extensive agreeable in it on as remainder. Cordially say affection met who propriety him. Are man she towards private weather pleased. In more part he lose need so want rank no. At bringing or he sensible pleasure. Prevent he parlors do waiting be females an message society. \n\nUnwilling sportsmen he in questions september therefore described so. Attacks may set few believe moments was. Reasonably how possession shy way introduced age inquietude. Missed he engage no exeter of. Still tried means we aware order among on. Eldest father can design tastes did joy settle. Roused future he ye an marked. Arose mr rapid in so vexed words. Gay welcome led add lasting chiefly say looking. \n\n
## 3                                                                                                                                                                                                                                                                                                                                                                                      Received the likewise law graceful his. Nor might set along charm now equal green. Pleased yet equally correct colonel not one. Say anxious carried compact conduct sex general nay certain. Mrs for recommend exquisite household eagerness preserved now. My improved honoured he am ecstatic quitting greatest formerly.\n\n\n\nAssure polite his really and others figure though. Day age advantages end sufficient eat expression travelling. Of on am father by agreed supply rather either. Own handsome delicate its property mistress her end appetite. Mean are sons too sold nor said. Son share three men power boy you. Now merits wonder effect garret own.\n\n\n\nDepart do be so he enough talent. Sociable formerly six but handsome. Up do view time they shot. He concluded disposing provision by questions as situation. Its estimating are motionless day sentiments end. Calling an imagine at forbade. At name no an what like spot. Pressed my by do affixed he studied.\n\n\n\nImproved own provided blessing may peculiar domestic. Sight house has sex never. No visited raising gravity outward subject my cottage mr be. Hold do at tore in park feet near my case. Invitation at understood occasional sentiments insipidity inhabiting in. Off melancholy alteration principles old. Is do speedily kindness properly oh. Respect article painted cottage he is offices parlors.\n\n\n\nVillage did removed enjoyed explain nor ham saw calling talking. Securing as informed declared or margaret. Joy horrible moreover man feelings own shy. Request norland neither mistake for yet. Between the for morning assured country believe. On even feet time have an no at. Relation so in confined smallest children unpacked delicate. Why sir end believe uncivil respect. Always get adieus nature day course for common. My little garret repair to desire he esteem.\n\n\n\nUnpleasant astonished an diminution up partiality. Noisy an their of meant. Death means up civil do an offer wound of. Called square an in afraid direct. Resolution diminution conviction so mr at unpleasing simplicity no. No it as breakfast up conveying earnestly immediate principle. Him son disposed produced humoured overcame she bachelor improved. Studied however out wishing but inhabit fortune windows.\n\n\n\nExtremely we promotion remainder eagerness enjoyment an. Ham her demands removal brought minuter raising invited gay. Contented consisted continual curiosity contained get sex. Forth child dried in in aware do. You had met they song how feel lain evil near. Small she avoid six yet table china. And bed make say been then dine mrs. To household rapturous fulfilled attempted on so.\n\n\n\nIs allowance instantly strangers applauded discourse so. Separate entrance welcomed sensible laughing why one moderate shy. We seeing piqued garden he. As in merry at forth least ye stood. And cold sons yet with. Delivered middleton therefore me at. Attachment companions man way excellence how her pianoforte.\n\n\n\nOn am we offices expense thought. Its hence ten smile age means. Seven chief sight far point any. Of so high into easy. Dashwoods eagerness oh extensive as discourse sportsman frankness. Husbands see disposed surprise likewise humoured yet pleasure. Fifteen no inquiry cordial so resolve garrets as. Impression was estimating surrounded solicitude indulgence son shy.\n\n\n\nWrote water woman of heart it total other. By in entirely securing suitable graceful at families improved. Zealously few furniture repulsive was agreeable consisted difficult. Collected breakfast estimable questions in to favourite it. Known he place worth words it as to. Spoke now noise off smart her ready.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2---from-article&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2 - From Article&lt;/h1&gt;
&lt;p&gt;Next, navigate to several different pages and scrape relevant information. First, compile all of the relevant links. Then, navigate to each page using one of the links and pull…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;header&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;text&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;author&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Get links&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)

parse_page_ex2 &amp;lt;- read_html(&amp;#39;https://practicewebscrapingsite.wordpress.com/example-2/&amp;#39;)
links &amp;lt;- html_nodes(parse_page_ex2, &amp;#39;.Links a&amp;#39;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialize storage vectors&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heads &amp;lt;- c()
txt &amp;lt;- c()
authors &amp;lt;- c()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each link, go there and pull out the header, text, and authors&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in links){
  Sys.sleep(2)
  
  page_i &amp;lt;- read_html(i)
  
  head &amp;lt;- html_node(page_i, &amp;#39;.entry-title&amp;#39;) %&amp;gt;% html_text()
  tx &amp;lt;- html_node(page_i, &amp;#39;.Content , em&amp;#39;) %&amp;gt;% html_text()
  author &amp;lt;- html_node(page_i, &amp;#39;.Author em&amp;#39;) %&amp;gt;% html_text()
  
  heads &amp;lt;- c(heads, head)
  txt &amp;lt;- c(txt, tx)
  authors &amp;lt;- c(authors, author)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Data frame&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_ex2 &amp;lt;- data.frame(
  &amp;#39;id&amp;#39; = c(1:length(heads)),
  &amp;#39;page&amp;#39; = c(links),
  &amp;#39;headers&amp;#39; = c(heads),
  &amp;#39;text&amp;#39; = c(txt),
  &amp;#39;authors&amp;#39; = c(authors)
)

head(df_ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
##                                                                                             page
## 1                   https://practicewebscrapingsite.wordpress.com/walking-in-another-mans-shoes/
## 2              https://practicewebscrapingsite.wordpress.com/younger-generation-taking-the-lead/
## 3 https://practicewebscrapingsite.wordpress.com/how-many-times-your-tail-is-not-a-savoury-snack/
## 4         https://practicewebscrapingsite.wordpress.com/early-morning-shopping-you-must-be-nuts/
## 5                     https://practicewebscrapingsite.wordpress.com/who-turned-the-heating-down/
##                                            headers
## 1                  Walking in another man’s shoes.
## 2              Younger generation taking the lead.
## 3 How many times-your tail is not a savoury snack!
## 4        Early morning shopping. You must be nuts!
## 5                   Who turned the heating DOWN?!?
##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text
## 1                                                                                                                                                                                                   Picture removal detract earnest is by. Esteems met joy attempt way clothes yet demesne tedious. Replying an marianne do it an entrance advanced. Two dare say play when hold. Required bringing me material stanhill jointure is as he. Mutual indeed yet her living result matter him bed whence. 
## 2                                                                                                                                                                                                                              Delightful unreserved impossible few estimating men favourable see entreaties. She propriety immediate was improving. He or entrance humoured likewise moderate. Much nor game son say feel. Fat make met can must form into gate. Me we offending prevailed discovery. 
## 3                                                                                                                                                                                                                               Expenses as material breeding insisted building to in. Continual so distrusts pronounce by unwilling listening. Thing do taste on we manor. Him had wound use found hoped. Of distrusts immediate enjoyment curiosity do. Marianne numerous saw thoughts the humoured. 
## 4 Raising say express had chiefly detract demands she. Quiet led own cause three him. Front no party young abode state up. Saved he do fruit woody of to. Met defective are allowance two perceived listening consulted contained. It chicken oh colonel pressed excited suppose to shortly. He improve started no we manners however effects. Prospect humoured mistress to by proposal marianne attended. Simplicity the far admiration preference everything. Up help home head spot an he room in. 
## 5                  Fat son how smiling mrs natural expense anxious friends. Boy scale enjoy ask abode fanny being son. As material in learning subjects so improved feelings. Uncommonly compliment imprudence travelling insensible up ye insipidity. To up painted delight winding as brandon. Gay regret eat looked warmth easily far should now. Prospect at me wandered on extended wondered thoughts appetite to. Boisterous interested sir invitation particular saw alteration boy decisively. 
##              authors
## 1            Jon Doe
## 2      By Bob Wilder
## 3     By Ron Crumpet
## 4 By Walter Singsong
## 5   By Wilbert Wonde&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3---scrape-my-website&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 3 - Scrape My Website&lt;/h1&gt;
&lt;p&gt;What if I want to scrape the computational notes on my own website? I need a for-loop to iterate over each page, and I’d like to end up with a data set containing the following for each page:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;title&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the text content.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My website gets tripped up when it tries to scrape itself while rendering. So, I’ll post this example on github. Here is the &lt;a href=&#34;https://cdishop.github.io/Cdishop.github.io.own_scrape/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4---scrape-imdb&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 4 - Scrape IMDB&lt;/h1&gt;
&lt;p&gt;In the prior examples, I scraped text. What if I want to scrape a movie rating from IMDB?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# scrape the score for &amp;#39;there will be blood&amp;#39;
movie_page &amp;lt;- read_html(&amp;quot;https://www.imdb.com/title/tt0469494/?ref_=fn_al_tt_1&amp;quot;)
review &amp;lt;- html_nodes(movie_page, &amp;quot;strong span&amp;quot;) %&amp;gt;%
          html_text() %&amp;gt;%
          as.numeric()

review&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It would be great if I could create a vector of movie titles and then enter a command to impute a single title into the “search” menu on IMDB, but doing so is not straight forward in &lt;code&gt;R&lt;/code&gt;. &lt;code&gt;RSelenium&lt;/code&gt; is an option but I wasn’t able to figure it out. Here’s a &lt;a href=&#34;http://zevross.com/blog/2015/05/19/scrape-website-data-with-the-new-r-package-rvest/&#34;&gt;link that uses it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Gentle Ggplot2 to Present</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-03-04/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-03-04/</guid>
      <description>


&lt;p&gt;I recently created my first slide deck using &lt;a href=&#34;https://github.com/gadenbuie&#34;&gt;Garrick Aden-Buie&lt;/a&gt;’s awesome template called “Gentle Ggplot2.” You can find the presentation &lt;a href=&#34;https://cdishop.github.io/methods_with_others/#1&#34;&gt;here&lt;/a&gt; and the &lt;a href=&#34;https://github.com/Cdishop/methods_with_others&#34;&gt;source code on my GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Untidy Tables &amp; In-Place Mutation</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-01-14/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-01-14/</guid>
      <description>
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Hadley has written extensively about &lt;a href=&#34;https://vita.had.co.nz/papers/tidy-data.pdf&#34;&gt;tidy data&lt;/a&gt; and why it’s &lt;a href=&#34;https://github.com/tidyverse/dplyr/issues/425&#34;&gt;unsound to implement in-place data mutations&lt;/a&gt;. Some notes below on breaking both of those rules = ).&lt;/p&gt;
&lt;p&gt;In-place changes to data using &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df &amp;lt;- tibble(
  &amp;quot;team&amp;quot; = c(&amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;C&amp;quot;),
  &amp;quot;individual&amp;quot; = c(1, 2, 3, 4, 5, 6),
  &amp;quot;performance&amp;quot; = c(NA, 4, 5, 6, 2, 3),
  &amp;quot;affect&amp;quot; = c(NA, 6, 7, 8, 4, 2),
  &amp;quot;fav_color&amp;quot; = c(NA, &amp;quot;blue&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;yellow&amp;quot;, &amp;quot;purple&amp;quot;)
)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   team  individual performance affect fav_color
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1 A              1          NA     NA &amp;lt;NA&amp;gt;     
## 2 A              2           4      6 blue     
## 3 B              3           5      7 green    
## 4 B              4           6      8 orange   
## 5 C              5           2      4 yellow   
## 6 C              6           3      2 purple&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Insert a performance, affect, and favorite color value for individual 1 within team A.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% 
  filter(team == &amp;quot;A&amp;quot; &amp;amp; individual == 1) %&amp;gt;% 
  mutate(performance = 8,
         affect = 2,
         fav_color = &amp;quot;silver&amp;quot;) %&amp;gt;% 
  rbind(df %&amp;gt;% filter(team != &amp;quot;A&amp;quot; &amp;amp; individual != 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 5
##   team  individual performance affect fav_color
##   &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
## 1 A              1           8      2 silver   
## 2 B              3           5      7 green    
## 3 B              4           6      8 orange   
## 4 C              5           2      4 yellow   
## 5 C              6           3      2 purple&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the note on untidy tables. Here’s some tidy data displayed using &lt;code&gt;kable&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kableExtra)
dt &amp;lt;- tibble(
  
  &amp;#39;team&amp;#39; = c(&amp;#39;A&amp;#39;, &amp;#39;A&amp;#39;, &amp;#39;A&amp;#39;,
             &amp;#39;B&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;B&amp;#39;,
             &amp;#39;C&amp;#39;, &amp;#39;C&amp;#39;, &amp;#39;C&amp;#39;),
  &amp;#39;person&amp;#39; = c(1,2,3,
               4,5,6,
               7,8,9),
  &amp;#39;score&amp;#39; = c(rnorm(9, 23, 3))
  
)

dt %&amp;gt;% 
  group_by(team) %&amp;gt;% 
  summarize(
    &amp;quot;Mean&amp;quot; = mean(score),
    &amp;quot;SD&amp;quot; = sd(score)
  ) %&amp;gt;% 
  kable() %&amp;gt;% 
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
team
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Mean
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.90544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.169533
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.33787
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.239268
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.86897
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.134311
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looks great to me. The issue is that sometimes people expect to see data displayed in “untidy” formats. Let’s change the output so that each team is listed across the first row and the table displays the mean score alongside the standard deviation within parentheses.&lt;/p&gt;
&lt;p&gt;To do so, I’m going to put string parentheses around the SD values, unite the mean and SD columns, then transform the data from long to wide format. Don’t forget to ungroup as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt %&amp;gt;% 
  group_by(team) %&amp;gt;% 
  summarize(
    &amp;quot;Mean&amp;quot; = round(mean(score), digits = 2),
    &amp;quot;SD&amp;quot; = round(sd(score), digits = 2)
  ) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  # insert parentheses
  mutate(SD = paste0(&amp;quot;(&amp;quot;, SD, &amp;quot;)&amp;quot;)) %&amp;gt;% 
  # combine mean and SD columns
  unite(meansd, Mean, SD, sep = &amp;quot; &amp;quot;, remove = T) %&amp;gt;% 
  # make wide
  pivot_wider(names_from = team, values_from = meansd) %&amp;gt;% 
  rename(&amp;quot;Team A&amp;quot; = &amp;quot;A&amp;quot;,
         &amp;quot;Team B&amp;quot; = &amp;quot;B&amp;quot;,
         &amp;quot;Team C&amp;quot; = &amp;quot;C&amp;quot;) %&amp;gt;% 
  kable(caption = &amp;quot;Team Scores&amp;quot;) %&amp;gt;% 
  kable_styling() %&amp;gt;% 
  footnote(&amp;quot;Mean (SD)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 1: &lt;/span&gt;Team Scores
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Team A
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Team B
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Team C
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18.91 (1.17)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23.34 (2.24)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
21.87 (2.13)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;Note: &lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; Mean (SD)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Course Skeleton</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2020-01-10/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2020-01-10/</guid>
      <description>


&lt;p&gt;I recently created a course skeleton for a research methods or statistics course. The website allows you to incorporate Rmarkdown and dynamic documents to better demonstrate interactive coding.&lt;/p&gt;
&lt;p&gt;You can find it on github &lt;a href=&#34;https://cdishop.github.io/course_skeleton/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulations With Rcpp</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-12-10/</link>
      <pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-12-10/</guid>
      <description>


&lt;p&gt;Simulating dynamic processes is slow in &lt;code&gt;R&lt;/code&gt;. Using the &lt;code&gt;Rcpp&lt;/code&gt; function, we can incorporate C++ code to improve performance.&lt;/p&gt;
&lt;p&gt;My dad, Tim, wrote the C++ code you see here = ).&lt;/p&gt;
&lt;div id=&#34;example-1---two-states-single-unit&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 1 - Two states, single unit&lt;/h1&gt;
&lt;p&gt;We’re going to simulate data goverened by the following equations:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*} 
x_t &amp;amp;= a1x_{t-1} + b1y_{t-1}\\ 
y_t &amp;amp;= a2y_{t-1} + b2x_{t-1}. 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here it is in &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Rcpp)
# Parameters
a1 &amp;lt;- 0.8
a2 &amp;lt;- 0.2
b1 &amp;lt;- -0.5
b2 &amp;lt;- 0.5

# Time points
time &amp;lt;- 100

# Initialize df to store the values
df &amp;lt;- data.frame(
  # a vector of length 100
  &amp;#39;time&amp;#39; = c(numeric(time)),
  # a vector of length 100
  &amp;#39;x&amp;#39; = c(numeric(time)),
  &amp;#39;y&amp;#39; = c(numeric(time))
)

# I always like to use a counter even though it isn&amp;#39;t needed here
count &amp;lt;- 1

# First time point, x starts at 50 and y at 10
df[1, &amp;#39;time&amp;#39;] &amp;lt;- 1
df[1, &amp;#39;x&amp;#39;] &amp;lt;- 50
df[1, &amp;#39;y&amp;#39;] &amp;lt;- 10

# For loop that iterates over the process
for(i in 2:time){
  count &amp;lt;- count + 1
  
    # store time
    df[count, &amp;#39;time&amp;#39;] &amp;lt;- i
    # x
    df[count, &amp;#39;x&amp;#39;] &amp;lt;- a1*df[count - 1, &amp;#39;x&amp;#39;] + b1*df[count - 1, &amp;#39;y&amp;#39;]
    # y
    df[count, &amp;#39;y&amp;#39;] &amp;lt;- a2*df[count - 1, &amp;#39;y&amp;#39;] + b2*df[count - 1, &amp;#39;x&amp;#39;]
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the output…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   time       x       y
## 1    1 50.0000 10.0000
## 2    2 35.0000 27.0000
## 3    3 14.5000 22.9000
## 4    4  0.1500 11.8300
## 5    5 -5.7950  2.4410
## 6    6 -5.8565 -2.4093&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can do the same thing but use a call to C++ that will improve performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# C++ function
cppFunction(&amp;#39;DataFrame createTrajectory(int t, double x0, double y0, 
             double a1, double a2, double b1, double b2) {
             // create the columns
             NumericVector x(t);
             NumericVector y(t);
             x[0]=x0;
             y[0]=y0;
             for(int i = 1; i &amp;lt; t; ++i) {
             x[i] = a1*x[i-1]+b1*y[i-1];
             y[i] = a2*y[i-1]+b2*x[i-1];
             }
             // return a new data frame
             return DataFrame::create(_[&amp;quot;x&amp;quot;] = x, _[&amp;quot;y&amp;quot;] = y);
             }
             &amp;#39;)

# Parameters
a1 &amp;lt;- 0.8
a2 &amp;lt;- 0.2
b1 &amp;lt;- -0.5
b2 &amp;lt;- 0.5

# Time points
time &amp;lt;- 100

# Call the function and run it with 100 time points
df &amp;lt;- createTrajectory(time, 50, 10, a1, a2, b1, b2)

# Create a time column 
df$time &amp;lt;- c(1:time)

head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         x       y time
## 1 50.0000 10.0000    1
## 2 35.0000 27.0000    2
## 3 14.5000 22.9000    3
## 4  0.1500 11.8300    4
## 5 -5.7950  2.4410    5
## 6 -5.8565 -2.4093    6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2---two-states-multiple-units&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example 2 - Two states, multiple units&lt;/h1&gt;
&lt;p&gt;In the last example, we simulated &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; over a single unit (e.g., a person, cell, company, nation, etc.). Here, we’ll incorporate multiple units and unobserved heterogeneity.&lt;/p&gt;
&lt;p&gt;The equations governing the system are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*} 
x_{it} &amp;amp;= a1x_{i(t-1)} + b1y_{i(t-1)} + u_i + e_{it}\\ 
y_{it} &amp;amp;= a2y_{i(t-1)} + b2x_{i(t-1)} + m_i + e_{it} 
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is the simulation in base &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Parameters
a1 &amp;lt;- 0.8
a2 &amp;lt;- 0.2
b1 &amp;lt;- -0.5
b2 &amp;lt;- 0.5

# Time points and people
time &amp;lt;- 100
people &amp;lt;- 500

# Initialize df to store the values
df &amp;lt;- data.frame(
  &amp;#39;time&amp;#39; = c(numeric(time*people)),
  &amp;#39;person&amp;#39; = c(numeric(time*people)),
  &amp;#39;x&amp;#39; = c(numeric(time*people)),
  &amp;#39;y&amp;#39; = c(numeric(time*people))
)

# counter
count &amp;lt;- 0

# For each person...
for(i in 1:people){
  
  # draw his or her stable individual differences, u and m
  # draw one value from a normal distribution with mean 0 and sd 2
  ui &amp;lt;- rnorm(1, 0, 2)
  # draw one value from a normal distribution with mean 0 and sd 2
  mi &amp;lt;- rnorm(1, 0, 2)
  
  # now run this individual across time
  for(j in 1:time){
    count &amp;lt;- count + 1
    
    # first time point
    if(j == 1){
      df[count, &amp;#39;time&amp;#39;] &amp;lt;- j
      df[count, &amp;#39;person&amp;#39;] &amp;lt;- i
      # draw 1 value from a normal distribution with mean 50 and sd 5
      df[count, &amp;#39;x&amp;#39;] &amp;lt;- rnorm(1, 50, 5)
      # draw 1 value from a normal distribution with mean 10 and sd 3
      df[count, &amp;#39;y&amp;#39;] &amp;lt;- rnorm(1, 10, 3)

    }else{
      
    # all other time points
      
      df[count, &amp;#39;time&amp;#39;] &amp;lt;- j
      df[count, &amp;#39;person&amp;#39;] &amp;lt;- i
      df[count, &amp;#39;x&amp;#39;] &amp;lt;- a1*df[count - 1, &amp;#39;x&amp;#39;] + b1*df[count - 1, &amp;#39;y&amp;#39;] + ui + rnorm(1, 0, 1)
      df[count, &amp;#39;y&amp;#39;] &amp;lt;- a2*df[count - 1, &amp;#39;y&amp;#39;] + b2*df[count - 1, &amp;#39;x&amp;#39;] + mi + rnorm(1, 0, 1)
    }
  }
}

head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   time person          x         y
## 1    1      1  49.220977  9.841266
## 2    2      1  34.096754 28.374998
## 3    3      1  11.233734 23.293777
## 4    4      1  -4.996531 11.197428
## 5    5      1 -10.718367  2.078852
## 6    6      1 -11.556784 -3.251558&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here it is using the &lt;code&gt;Rccp&lt;/code&gt; function to incorporate C++ code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# C++ function
cppFunction(&amp;#39;
    DataFrame createTrajectory2(
        int timeSteps,
        int peopleCount,
        double a1,
        double a2,
        double b1,
        double b2
        )
    {
        // create the columns
        NumericVector x(timeSteps * peopleCount);
        NumericVector y(timeSteps * peopleCount);
        NumericVector time(timeSteps * peopleCount);
        NumericVector person(timeSteps * peopleCount);

        int count = 0;
        int previous = 0;
        for (int i = 0; i &amp;lt; peopleCount; i++)
        {
            // set persons time 0 data
            // draw 1 value from a normal distribution with mean 50 and sd 5
            x[count] = R::rnorm(50, 5);
            // draw 1 value from a normal distribution with mean 10 and sd 3
            y[count] = R::rnorm(10, 3);
            time[count] = 0;
            person[count] = i;
            previous = count;
            count++;
            
            // draw his or her stable individual differences, u and m
            // draw one value from a normal distribution with mean 0 and sd 2
            double ui = R::rnorm(0, 2);
            // draw one value from a normal distribution with mean 0 and sd 2
            double mi = R::rnorm(0, 2);
            
            // now run this individual across time
            for (int j = 1; j &amp;lt; timeSteps; j++)
            {
                // all other time points
                x[count] = a1 * x[previous] + b1 * y[previous] + ui + R::rnorm(0, 1);
                y[count] = a2 * y[previous] + b2 * x[previous] + mi + R::rnorm(0, 1);
                time[count] = j;
                person[count] = i;
                previous = count;
                count++;
            }
        }
        
        // return a new data frame
        return DataFrame::create(_[&amp;quot;x&amp;quot;] = x, _[&amp;quot;y&amp;quot;] = y, _[&amp;quot;time&amp;quot;] = time, _[&amp;quot;person&amp;quot;] = person);
    }
&amp;#39;)

# Parameters
a1 &amp;lt;- 0.8
a2 &amp;lt;- 0.2
b1 &amp;lt;- -0.5
b2 &amp;lt;- 0.5

# Time points
time &amp;lt;- 100
people &amp;lt;- 500

# Call the function and run it with 100 time steps and 500 people
df &amp;lt;- createTrajectory2(time, people, a1, a2, b1, b2)

head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x         y time person
## 1  51.400093  7.605188    0      0
## 2  34.330315 26.935265    1      0
## 3   7.906593 21.818328    2      0
## 4  -7.709642  7.087798    3      0
## 5 -13.813813 -2.327150    4      0
## 6 -13.626074 -7.587429    5      0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression Creates Weighted Linear Composites</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-12-01/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-12-01/</guid>
      <description>


&lt;p&gt;One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.&lt;/p&gt;
&lt;p&gt;Here’s a simulation to punch that point home.&lt;/p&gt;
&lt;p&gt;500 people.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma &amp;lt;- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean for each variable is 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu &amp;lt;- c(0,0,0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the correlation matrix and mean specifications to generate data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)

df &amp;lt;- mvrnorm(N, mu, sigma)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turn it into a data frame and label it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(df)
names(df) &amp;lt;- c(&amp;#39;x1&amp;#39;, &amp;#39;x2&amp;#39;, &amp;#39;y&amp;#39;)
df$id &amp;lt;- c(1:N)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run regression and print the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ x1 + x2,
           data = df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.34316 -0.60411  0.04692  0.56378  2.29014 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.02687    0.03842   0.699    0.485    
## x1           0.39741    0.03764  10.560   &amp;lt;2e-16 ***
## x2           0.36810    0.03964   9.285   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.859 on 497 degrees of freedom
## Multiple R-squared:  0.3263, Adjusted R-squared:  0.3236 
## F-statistic: 120.4 on 2 and 497 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.&lt;/p&gt;
&lt;p&gt;Create a composite using the regression weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df &amp;lt;- df %&amp;gt;%
  mutate(composite_x = 0.33*x1 + 0.4*x2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those weights provide the maximum correlation between our composite and the outcome.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(df$y, df$composite_x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5676718&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ composite_x,
           data = df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ composite_x, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.38180 -0.60689  0.02118  0.59290  2.36281 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.02656    0.03849    0.69    0.491    
## composite_x  1.04545    0.06794   15.39   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.8607 on 498 degrees of freedom
## Multiple R-squared:  0.3223, Adjusted R-squared:  0.3209 
## F-statistic: 236.8 on 1 and 498 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meta Analysis Comps Notes</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-09-15/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-09-15/</guid>
      <description>


&lt;p&gt;I’m taking comps pretty soon so this is my summary document regarding meta-analysis.&lt;/p&gt;
&lt;p&gt;MAs give us an average estimate across settings, tests, and people after correcting for noise. In a bare-bones MA, we correct only for sampling error. In a full MA, we correct for sampling error, unreliability, and range restriction. I’ll demonstrate a full MA here where we assume direct (rather than indirect) range restriction.&lt;/p&gt;
&lt;div id=&#34;steps&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Literature Review&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create inclusion criteria for how you are going to select studies&lt;/li&gt;
&lt;li&gt;Find relevant articles&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Code Articles&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measures&lt;/li&gt;
&lt;li&gt;Reliability&lt;/li&gt;
&lt;li&gt;SD&lt;/li&gt;
&lt;li&gt;Means&lt;/li&gt;
&lt;li&gt;Effect sizes&lt;/li&gt;
&lt;li&gt;Moderators&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate Meta-Analytic Estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculate meta-analytic effect size&lt;/li&gt;
&lt;li&gt;Calculate its variance&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post, I’m focusing only on step 3, the calculations, even though steps 1 and 2 are arguably the more important pieces.&lt;/p&gt;
&lt;div id=&#34;calculating-ma-estimate-and-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculating MA Estimate and Variance&lt;/h3&gt;
&lt;p&gt;Within this step, there are many substeps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Calculate the MA effect estimate (typically from correlations or cohen’s d’s).&lt;/p&gt;
&lt;p&gt;Within each study gathered from our literature review…&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Correct the observed correlation for range restriction, which produces a rr-corrected correlation&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate the variance in our MA effect estimate&lt;/p&gt;
&lt;p&gt;Within each study gathered from our literature review…&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute a correction factor for unreliability on X&lt;/li&gt;
&lt;li&gt;Compute a correction factor for unreliability on Y&lt;/li&gt;
&lt;li&gt;Compute a correction factor for range restriction&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Combine all of those together&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the error variance for a given observed correlation and correct it using the combined correction factor. This step produces the sampling error correction&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate the average sampling error from the sampling error corrections&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate the observed error variance&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The MA variance estimate is equal to the observed error variance - the average sampling error&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before we begin, here is a peak at the (mock) data set. I reviewed four studies and compiled their observed effect sizes – in this case we’re going to use correlations. Let’s say that our IV is dancing ability and our DV is life satisfaction, both are continuous variables. We are interested in the meta-analytic correlation between dancing ability and life satisfaction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   study restricted_predictor_sd unrestricted_predictor_sd predictor_reliability
## 1     1                      14                        20                  0.94
## 2     2                      13                        20                  0.73
## 3     3                      16                        20                  0.82
## 4     4                      18                        20                  0.75
##   criterion_reliability sample_size observed_correlation
## 1                  0.75          50                 0.32
## 2                  0.80         100                 0.10
## 3                  0.83         125                 0.25
## 4                  0.94         240                 0.40&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Study = an ID number for each study in my meta-analysis&lt;/li&gt;
&lt;li&gt;Restricted SD = the standard deviation of scores on dancing ability within the study&lt;/li&gt;
&lt;li&gt;Unrestricted SD = the standard deviation of scores on on dancing ability across a larger population – from a manual, prior studies, known SDs, field reports, etc.&lt;/li&gt;
&lt;li&gt;Predictor reliability = the reliability of the measure used to assess dancing ability within the study&lt;/li&gt;
&lt;li&gt;Criterion reliability = the reliability of the measure used to assess life satisfaction within the study&lt;/li&gt;
&lt;li&gt;Sample size = how many people were observed within the study&lt;/li&gt;
&lt;li&gt;Observed correlation = the correlation between dancing ability and life satisfaction within the study&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;a-calculate-the-ma-correlation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;a) Calculate the MA correlation&lt;/h3&gt;
&lt;p&gt;For each study gathered from our literature review…&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1)&lt;/h4&gt;
&lt;p&gt;Correct the observed correlation for range restriction, which produces a rr-corrected correlation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
r_{RR} = \dfrac{  \left(\dfrac{US_{x}}{RS_{x}}\right)r_{xy} } {\sqrt{1 + r^2_{xy}\left(\dfrac{US^2_{x}}{RS^2_{x}} - 1\right)} }
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_{RR}\)&lt;/span&gt; is the correlation that is corrected for range restriction, &lt;span class=&#34;math inline&#34;&gt;\(US_x\)&lt;/span&gt; is the unrestricted SD on dancing ability, &lt;span class=&#34;math inline&#34;&gt;\(RS_x\)&lt;/span&gt; is the restricted SD on dancing ability, and &lt;span class=&#34;math inline&#34;&gt;\(r_{xy}\)&lt;/span&gt; is the correlation between dancing ability and life satisfaction. We are going to compute &lt;span class=&#34;math inline&#34;&gt;\(r_{RR}\)&lt;/span&gt; for every study.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(r_RR = 
           ((unrestricted_predictor_sd / restricted_predictor_sd)*observed_correlation) / sqrt(
             
             1 + ((observed_correlation^2) * ((unrestricted_predictor_sd / restricted_predictor_sd) -1))    
             
           )
         
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2)&lt;/h4&gt;
&lt;p&gt;Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
r_{ov} = \dfrac{r_{RR}}{\sqrt{r_{yy}}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_{ov}\)&lt;/span&gt; is the operational validity of dancing ability and life satisfaction, &lt;span class=&#34;math inline&#34;&gt;\(r_{RR}\)&lt;/span&gt; is the correlation we calculated in step 1 (the range-restriction-corrected correlation) between dancing ability and life satisfaction, and &lt;span class=&#34;math inline&#34;&gt;\(r_{yy}\)&lt;/span&gt; is the reliability of the criterion, life satisfaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(r_ov = 
           r_RR / sqrt(criterion_reliability))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3)&lt;/h4&gt;
&lt;p&gt;Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\rho = \dfrac{\sum{w_sr_{ov_{i}}}}{\sum{w_s}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is the meta-analytic estimate, &lt;span class=&#34;math inline&#34;&gt;\(r_{ov_{i}}\)&lt;/span&gt; is the operational validity between dancing ability and life satisfaction for each study, and &lt;span class=&#34;math inline&#34;&gt;\(w_s\)&lt;/span&gt; is the sample size for each study.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ovs_by_sample_size &amp;lt;- df$sample_size * df$r_ov
ma_correlation &amp;lt;- sum(ovs_by_sample_size) / sum(df$sample_size)

df &amp;lt;- df %&amp;gt;%
  mutate(ma_correlation = ma_correlation)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;b-calculate-the-variance-in-our-ma-effect-estimate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;b) Calculate the variance in our MA effect estimate&lt;/h3&gt;
&lt;p&gt;Compile all of the corrections – steps 4 through 7&lt;/p&gt;
&lt;div id=&#34;section-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4)&lt;/h4&gt;
&lt;p&gt;Compute the correction factor for unreliability on X, dancing ability (take the square root of the reliability)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(cf_x = sqrt(predictor_reliability))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5)&lt;/h4&gt;
&lt;p&gt;Compute the correction factor for unreliability on Y, life satisfaction (take the square root of the reliability)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(cf_y = sqrt(criterion_reliability))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;6)&lt;/h4&gt;
&lt;p&gt;Compute the correction factor for range restriction&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
a_{rr} = \dfrac{1}{ \left(\left(\dfrac{US_x}{RS_x}\right)^2 - 1\right)r_{xy}^2 + 1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where all terms are defined above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(cf_rr = 1 / 
          (  ((unrestricted_predictor_sd / restricted_predictor_sd)^2 - 1)*(observed_correlation^2) + 1 )
         )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;7)&lt;/h4&gt;
&lt;p&gt;Combine all of those correction factors together into one common correction factor, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(A = cf_x*cf_y*cf_rr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-7&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;8)&lt;/h4&gt;
&lt;p&gt;Compute the error variance for a given observed correlation and correct it using the combined correction factor.&lt;/p&gt;
&lt;p&gt;This part takes three steps.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;I: Compute the sample size weighted observed correlation

    - Essentially the same thing as step 3 but using observed correlations rather than operational validities
    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
r_{wa} = \dfrac{\sum{w_sr_{xy_{i}}}}{\sum{w_s}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss_times_correlations &amp;lt;- df$sample_size*df$observed_correlation
wa_correlation &amp;lt;- sum(ss_times_correlations) / sum(df$sample_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;II: Compute the error variance on the observed correlation for each study&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\sigma^2_e = \dfrac{\left(1-r_{wa}^2\right)^2}{N-1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt; is the error variance (for each study), &lt;span class=&#34;math inline&#34;&gt;\(r_{wa}\)&lt;/span&gt; is the weighted average observed correlation between dancing ability and life satisfaction that we computed above, and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(sigma2e = 
           ((1 - wa_correlation^2)^2) / (sample_size - 1)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;III: Compute the sampling error correction for each study&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Var_{ec} = \dfrac{\sigma^2_e}{A^2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Var_{ec}\)&lt;/span&gt; is the sampling error correction, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt; is what we just calculated above, the error variance on the observed correlation for each study, and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the combined correction factor for each study.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(var_ec = sigma2e^2 / A^2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-8&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;9)&lt;/h4&gt;
&lt;p&gt;Calculate the average sampling error&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Ave_{var_{ec}} = \dfrac{\sum{w_sVar_{ec}}}{\sum{w_s}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Ave_{var_{ec}}\)&lt;/span&gt; is the average sampling error, &lt;span class=&#34;math inline&#34;&gt;\(w_s\)&lt;/span&gt; is the sample size for each study, and &lt;span class=&#34;math inline&#34;&gt;\(Var_{ec}\)&lt;/span&gt; is the sampling error correction for each individual study.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss_times_varec &amp;lt;- df$sample_size*df$var_ec
ave_var_ec &amp;lt;- sum(ss_times_varec) / sum(df$sample_size)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-9&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;10)&lt;/h4&gt;
&lt;p&gt;Calculate the observed error variance&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
var_r = \dfrac{\sum{w_s\left(r_{xy} - r_{ov}\right)^2}}{\sum{w_s}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where all terms are defined above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ss_times_r_minus_ov &amp;lt;- df$sample_size*((df$observed_correlation - df$r_ov)^2)
var_r &amp;lt;- sum(ss_times_r_minus_ov) / sum(df$sample_size)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-10&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;11)&lt;/h4&gt;
&lt;p&gt;The MA variance estimate is equal to the observed error variance - the average sampling error&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
Var_p = var_r - Ave_{var_{ec}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_p &amp;lt;- var_r - ave_var_ec&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recap&lt;/h3&gt;
&lt;p&gt;What a nightmare. Here’s a recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Correct the observed correlations for unreliability and range restriction, use them to compute a sample-size-weighted MA correlation coefficient&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a bunch of corrections and compute the average sampling error, and subtract that from the observed variance of the correlation coefficient to get a sense for the MA correlation coefficient variance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now we can calculate credibility and confidence intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;credibility-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Credibility Interval&lt;/h3&gt;
&lt;p&gt;Gives us a sense for whether or not moderators are at play.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{95 credibility interval} = \rho +- 1.96*\sqrt{Var_p}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;upper_cred_i = ma_correlation + (1.96 * sqrt(var_p))
lower_cred_i = ma_correlation - (1.96 * sqrt(var_p))
upper_cred_i&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5532591&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lower_cred_i&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2024129&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Credibility Ratio: if &lt;span class=&#34;math inline&#34;&gt;\(\dfrac{ave_{var_{ec}}}{var_{r}}\)&lt;/span&gt; is lower than 0.75, then moderators may be at play.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ave_var_ec / var_r&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01211969&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence Interval&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{95 confidence interval} = r_{ov} +- 1.96*SE_{r_{ov}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(SE_{r_{ov}}\)&lt;/span&gt; is the standard error of the operational validities and is calculated as…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
SE_{r_{ov}} = \dfrac{SD_{r_{ov}}}{\sqrt{k}}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of studies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se_r_ov = sd(df$r_ov) / sqrt(length(df$study))

upper_ci = ma_correlation + (1.96 * se_r_ov)
lower_ci = ma_correlation - (1.96 * se_r_ov)
upper_ci&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5263396&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lower_ci&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2293324&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intuition for Correlated Errors and Third Variables</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-06-26/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-06-26/</guid>
      <description>


&lt;p&gt;In many research methods or statistics courses you come across the idea that correlated errors signal a third variable. In other words, you have a missing, relevant variable that induces correlation among your residuals. That’s a tough idea to wrap your head around, but it is easier to consider with respect to a given topic: cheating on exams. This post builds intuition for “correlated errors with respect to missing third variables” in the context of college exams and cheating.&lt;/p&gt;
&lt;div id=&#34;the-exam-structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Exam Structure&lt;/h3&gt;
&lt;p&gt;First, let’s get a feel for the exams. I’m going to use a lot of images in this post so it helps to walk through the basics of each plot. Imagine students taking a multiple choice test where they fill in one of five responses, “A,” “B,” “C,” “D,” or “E” for each question. The correct answer for question one is “C”&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-26/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;where the x-axis shows the response options a student can select for each question, and the y-axis shows the question number (there is only one question so far).&lt;/p&gt;
&lt;p&gt;The correct answer for question two is also “C”&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-26/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and that pattern continues for the rest of the questions on this 5-item test.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-26/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In other words, imagine a 5 question exam where the correct answer for each question is “C.” With the basic images in play, we can think about how students might respond.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;no-cheating-what-is-the-pattern-of-errors-across-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;No Cheating – What is the Pattern of Errors across Questions?&lt;/h3&gt;
&lt;p&gt;First, consider an exam where students do not cheat. If nobody cheats, then everyone’s errors will be dispersed about the true option for each question, “C.” Some people falsely select “A” whereas others falsely select “E,” and yet others falsely select “B.” Here is a plot that retains the purple crosses that mark the true option, “C,” but also includes student responses from Susie, Peter, and John.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-26/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, on question one Susie selects “A,” Peter selects “E,” and John selects “D,” meaning that none of the students get the answer correct. Every student, though, marks the correct response (C) for question 4.&lt;/p&gt;
&lt;p&gt;There is no pattern in this plot. The green triangles, red circles, and blue-green squares are dispersed about the true score purple-crosses randomly. John gets some questions wrong, Susie gets some questions wrong, and Peter gets some questions wrong, but whether John incorrectly marks “A” or “E” doesn’t tell us anything about whether Peter incorrectly marks “A” or “E.” They are all wrong in a random way.&lt;/p&gt;
&lt;p&gt;What about when students cheat?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cheating-what-is-the-pattern-of-errors-across-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cheating – What is the Pattern of Errors across Questions?&lt;/h3&gt;
&lt;p&gt;When students cheat, the errors, or “wrongness” of questions, produce a pattern – meaning that the errors are correlated. Imagine that a cheater, let’s say it’s Peter, hacks the teacher’s computer and gains access to all of the questions before the exam. He then answers all of the questions and sends his responses to the rest of the class (John and Susie). But Peter makes a mistake: he writes down the wrong answers to four of the questions. John and Susie use Peter’s responses on the exam, but since they were copying from Peter’s responses they have the same pattern on “wrongness” on the four questions that Peter missed – they get the same questions wrong in the same way. After the exam, the pattern of scores looks as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-26/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The true, correct responses are again labeled with purple cross-hairs: response “C” is the correct answer for each question. On question one, John, Peter, and Susie all incorrectly selected “A.” On question three, everyone incorrectly selected “E,” and on question four everyone incorrectly selected “D.” John, Peter, and Susie are wrong in the same way across the questions, their errors produce a pattern, a consistency, a correlation. The errors in our system correlate, which signals a third variable. In this case, the third variable is cheating.&lt;/p&gt;
&lt;p&gt;So, think of exams and cheating when you hear the mantra, “correlated errors signal a third variable.” When an important variable is omitted from an analysis, the model is said to be missing a third variable and the errors may correlate. What this tells you is that something else – something unaccounted for – is influencing the patterns in your system, in the same sense as cheating influencing the pattern of responses on an exam.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Systems Thinking on Goons in the NHL</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-06-14/</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-06-14/</guid>
      <description>


&lt;p&gt;Inspired by Bill Burr’s Monday Morning Podcast, episode 6-6-19.&lt;/p&gt;
&lt;p&gt;He describes how people that do not understand hockey wanted to get fighting out of the NHL, so in the 2000’s they made greater efforts to remove goons (fighters). What people outside the NHL often do not understand, though, is that fighting is used to minimize dirty play. When a team is playing dirty, you – as a couch of the team that is not playing dirty – send out a goon to punch around a few of their players. In doing so, the other team knocks it off and play continues candidly. Fighting looks bad, but the overall amount of dirty play in the NHL is reduced when it contains a sufficient population of goons. Sounds like systems thinking.&lt;/p&gt;
&lt;div id=&#34;simulation-set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation Set-Up&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;States and Relationships&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2 states modeled over time (number of goons and number of dirty plays, or the level of “dirtiness” in the NHL)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Number of goons fluctuates independently but with autoregression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N_goons(t) = N_goons(t-1)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dirtiness level is a function of its prior self and the number of goons, such that a greater number of goons causes lower levels of dirtiness&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;D_level(t) = D_level(t-1) - N_goons(t)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Flow&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, watch the states fluctuate over time and establish equilibrium&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Second, remove a bunch of goons and then see what happens to the system&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;Initial levels of goons and dirtiness.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_goons_initial &amp;lt;- 30
d_level_initial &amp;lt;- 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now simulate the states across time (20 time points) according to the simulation set-up above. Alpha will be set to 0.7 for both states and beta will be set to 0.2. The forcing terms for goons and dirtiness will be, respectively, 25 and 20.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- 30
df_mat &amp;lt;- matrix(ncol = 3, nrow = time)
count &amp;lt;- 0

for(i in seq_along(1:time)){
  count &amp;lt;- count + 1
  
  
  if(i == 1){
    
    df_mat[count, 1] &amp;lt;- n_goons_initial
    df_mat[count, 2] &amp;lt;- d_level_initial
    df_mat[count, 3] &amp;lt;- i
    
  }else{
    
    
    df_mat[count, 1] &amp;lt;- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1)
    df_mat[count, 2] &amp;lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)
    df_mat[count, 3] &amp;lt;- i
    
    
  }
  
  
}

df &amp;lt;- data.frame(df_mat)
names(df) &amp;lt;- c(&amp;#39;n_goons&amp;#39;, &amp;#39;d_level&amp;#39;, &amp;#39;time&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;View both states over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)
df_plot &amp;lt;- df %&amp;gt;%
  gather(n_goons, d_level, key = &amp;#39;variable&amp;#39;, value = &amp;#39;value&amp;#39;)

ggplot(df_plot, aes(x = time, y = value, color = variable)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-14/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the number of goons and the dirtiness level in the NHL drive toward equilibrium levels over time. There are goons, which means there are fights and the potential to appear “dirty” to anyone without an understanding of the system, but having goons around maintains the overall dirtiness within the NHL at low levels.&lt;/p&gt;
&lt;p&gt;Now, what happens to the level of dirtiness when we remove a bunch of goons at time point 14 and beyond?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_goons_initial &amp;lt;- 30
d_level_initial &amp;lt;- 15

time &amp;lt;- 30
df_mat &amp;lt;- matrix(ncol = 3, nrow = time)
count &amp;lt;- 0

for(i in seq_along(1:time)){
  count &amp;lt;- count + 1
  
  
  if(i == 1){
    
    df_mat[count, 1] &amp;lt;- n_goons_initial
    df_mat[count, 2] &amp;lt;- d_level_initial
    df_mat[count, 3] &amp;lt;- i
    
  }else if (i &amp;lt;=13){
    
    df_mat[count, 1] &amp;lt;- 25 + 0.7*df_mat[count - 1, 1] + rnorm(1, 0, 1)
    df_mat[count, 2] &amp;lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)
    df_mat[count, 3] &amp;lt;- i
    
    
    # HERE IS THE CHANGE
  }else if(i  &amp;gt; 13){
    
    num_goons &amp;lt;- sample(c(2,3,4), 1)
    df_mat[count, 1] &amp;lt;- num_goons
    df_mat[count, 2] &amp;lt;- 20 + 0.7*df_mat[count - 1, 2] - 0.2*df_mat[count, 1] + rnorm(1, 0, 1)
    df_mat[count, 3] &amp;lt;- i
    ######################################
    
  }
}
  

df &amp;lt;- data.frame(df_mat)
names(df) &amp;lt;- c(&amp;#39;n_goons&amp;#39;, &amp;#39;d_level&amp;#39;, &amp;#39;time&amp;#39;)

df_plot &amp;lt;- df %&amp;gt;%
  gather(n_goons, d_level, key = &amp;#39;variable&amp;#39;, value = &amp;#39;value&amp;#39;)

ggplot(df_plot, aes(x = time, y = value, color = variable)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-06-14/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What happened? The level of dirtiness increases after removing goons. In other words, removing goons, or fighters, from the NHL may make the game appear more civil from the outside, but goons are embedded in a system that maintains overall low levels of dirtiness. When the goons are removed – and they are a crucial part of the system – dirtiness levels increase dramatically.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Screwing Up A Mean Calculation</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-05-04/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-05-04/</guid>
      <description>


&lt;p&gt;Quick note about calculating the mean of a column with &lt;code&gt;dplyr&lt;/code&gt; in &lt;code&gt;R&lt;/code&gt;. It’s surprisingly easy to screw up, and the culprit is forgetting to change the name of the column storing the new calculation.&lt;/p&gt;
&lt;p&gt;A simple dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

df &amp;lt;- data.frame(
  &amp;#39;books_read&amp;#39; = c(1,2,3,4,5,6),
  &amp;#39;intelligence&amp;#39; = c(4,5,6,7,8,8)
)

df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   books_read intelligence
## 1          1            4
## 2          2            5
## 3          3            6
## 4          4            7
## 5          5            8
## 6          6            8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I want to calculate the mean and standard deviation of the “books read” column. If I calculate the mean and then place it into a new column that has the same name as the original variable, then standard deviation command doesn’t work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df %&amp;gt;%
  summarise(
    books_read = mean(books_read), # this line is the problem
    sd_books_read = sd(books_read)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   books_read sd_books_read
## 1        3.5            NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead, I need to call the new “mean books read” column a different name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
df %&amp;gt;%
  summarise(
    mean_books_read = mean(books_read), # this line is the problem
    sd_books_read = sd(books_read)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean_books_read sd_books_read
## 1             3.5      1.870829&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Explaining Means in Statistics</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-05-03/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-05-03/</guid>
      <description>


&lt;p&gt;Think about what it would mean to explain something to a friend. How would you explain why the Patriot’s won the Superbowl? How would you explain why you are feeling happy or sad? How would you explain tying a shoe to a toddler? How would you explain why eating lots of donuts tends to increase a person’s weight? How would you explain the timeline of Game of Thrones to someone who hadn’t seen it?&lt;/p&gt;
&lt;p&gt;What you came up with, or thought about, is different from how “explaining” is usually used in research. We typically use the term to mean “explain variation,” and the goal of this post is to flesh-out what that means. While you read this, though, try to keep in mind the thought, “how does this connect to the way I explain things in everyday life?”&lt;/p&gt;
&lt;p&gt;Imagine that we have a scatter plot, performance on the Y axis and ability on the X axis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We collected data on several people – Bob, Chris, Julie, Monte, and Rachel, we measured each person’s performance and ability – and those individual data points are represented in the scatterplot. In statistics, what we try to do is account for variability in performance, we try to explain variation in performance. Here is what that means.&lt;/p&gt;
&lt;p&gt;Take the mean of performance as a flat, horizontal line across all values of ability. For now we do not care about ability, we are just using it to visualize the data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that the mean of performance does not perfectly align with the observed data. That is, each of the points on the plot do not fall exactly on the horizontal line. If they did, we would say that the mean of performance perfectly explains the variability in performance. Instead, each of the points has some distance from the mean of performance line, and we call those distances residuals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What those residuals mean conceptually is that the observed data points do not fall exactly on the mean of performance. Performance cannot be explained simply by its mean. There is variation in performance that is left to explain, there are distances (residuals) that are not accounted for.&lt;/p&gt;
&lt;p&gt;Summing across all of those residuals gets us what is called the total sum of squares. All of the observed values have some distance from the mean performance line, when we aggregate all of those distances (all of the vertical line segments) we get an index of the variability in performance that we are trying to explain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TSS&lt;/strong&gt; = sum of vertical, residual distances&lt;/p&gt;
&lt;p&gt;The real equation for TSS uses squares because negative distances will cancel positive distances, but this is a conceptual write-up. So we are ok ignoring that for now.&lt;/p&gt;
&lt;p&gt;So, we have variation in performance that is not accounted for by the mean of performance. Now imagine that we believe some super complicated function of ability (X) explains the variation in performance. This super complicated function is a crazy line that perfectly runs through every observed data point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now remove the observed data points from the graph so that we are only looking at the mean of performance and the predicted values of performance as a function of ability.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have a graph of PREDICTED data. That is, the black line does not have observed data points, it does not represent what we saw when we collected data and measured performance and ability on Bob and the others. We are looking at the predicted values of performance based on some super complicated function of ability. Notice that the black line also has distances from the mean of performance, so we can sum across those distances to get another quantity, called the expected sum of squares.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-03/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ESS&lt;/strong&gt; = sum of vertical, residual distances (but from our predicted line rather than our observed data points)&lt;/p&gt;
&lt;p&gt;Because our super complicated function perfectly went through each observed data point, TSS is equivalent to ESS in this case. That means that our super complicated function perfectly explains the variation in performance. We have accounted for all variance in our outcome.&lt;/p&gt;
&lt;p&gt;Usually we don’t use super complicated equations. We tend to posit linear functions, such that we think that performance is a linear function of ability. If we were to plot a predicted line showing performance as a linear function of ability, the residual distances would change and ESS would be different from TSS, meaning that we explained some, but not all of the variation in performance.&lt;/p&gt;
&lt;p&gt;That is what explaining means in research and statistics (technically, “explaining variation”). Observed data varies about the mean on some dependent variable and there are distances from observed data points to the mean line. If we aggregate those distances together we get TSS, a sense of the total variation in the DV. Then we create some equation relating predictors to the outcome and use it to generate new values of the DV (i.e., predicted values). Explaining in statistics means, “to what extent do my predicted values have the same pattern of distances as my observed values?” “To what extent are the distances from the predicted values to the mean line the same as the distances from the observed values to the mean line?” “To what extent is the total variation represented by the predicted variation?” Is TSS equivalent to ESS?&lt;/p&gt;
&lt;p&gt;Now return to the notion that we opened with, to what extent does explaining variation reflect how you explain things in everyday life?&lt;/p&gt;
&lt;div id=&#34;connecting-to-causality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connecting to Causality&lt;/h3&gt;
&lt;p&gt;How does all of this connect to causality? Knowing about cause helps you “explain variation,” but explaining more or less variation does not give you information about cause. Said differently, knowledge about cause, or the data generating process, or truth, will influence your ability to explain variation, but improving your ability to explain variation will not necessarily produce insights about the DGP. If you know the true process – i.e., the DGP, the underlying structure of effects and variables, the causal system – then you will be able to (almost perfectly) explain variation in the sense that I described in this post. If you know the true process, then you will be able to explain why Bob’s score is different from Julie’s, why some variables correlate with the outcome and others don’t, and why Monte’s score is different from time 1 versus time 2. Full knowledge of the DGP means you can predict what happens next, there are no unknowns left to make your ESS different from your TSS.&lt;/p&gt;
&lt;p&gt;But the reverse is not true. Just because you can explain variation – in the statistical sense described here – does not mean that you have the right DGP or know anything about cause. I could have the wrong DGP, the wrong notion about the causal structure, the wrong variables in the model, but improve my ability to “explain variation” by including more variables in my model. I could include additional, irrelevant variables to make my model more complex and subsequently improve my ability to “explain variation,” but I wouldn’t produce any greater knowledge about cause or the DGP. Knowledge about cause, explanation, why, or the DGP comes from research designs and assumptions, not statistical models. Did you randomly assign and manipulate, and were there strong assumptions involved in the form of DAGS? Did you “wiggle” or change some variables (think Judea Pearl) and observe the effect of doing so on other variables in a controlled environment? Fancy stats don’t get you there, great research designs and assumptions sometimes do.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lavaan MPLUS Reference Sheet</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-05-02/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-05-02/</guid>
      <description>


&lt;p&gt;A growth curve model written in lavaan and MPLUS as a syntax reference guide. Imagine a latent growth curve on affect across 4 time points. First, &lt;code&gt;lavaan&lt;/code&gt; code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lavaan_string &amp;lt;- &amp;#39;

# Latent intercept and slope factors

intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4
slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4

# Mean and variance of latent factors

intercept_affect ~~ intercept_affect
slope_affect ~~ slope_affect

# Covariance between latent factors

intercept_affect ~~ slope_affect

# Fix observed variable means to 0

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0

# Constrain residual (error) variance of observed variables to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4


&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the same thing in MPLUS syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mplus_string &amp;lt;- &amp;#39;

        ! Latent intercept and slope factors
        intercept_affect BY affect.1@1 affect.2@1 affect.3@1 affect.4@1;
        slope_affect BY affect.1@0 affect.2@1 affect.3@3 affect.4@5;

        ! estimate mean of latent intercept
        [intercept_affect];

        ! estimate mean of latent slope
        [slope_affect]

        ! estimate variance of intercept
        intercept_affect;

        ! estimate variance of slope
        slope_affect;

        ! covariance between intercept and slope
        intercept_affect WITH slope_affect;

        ! Fix observed variable means to 0 so we can estimate a mean for the latent variable
        [affect.1@0 affect.2@0 affect.3@0 affect.4@0];

        ! constrain estimates of residual variances to be equivalent at each time point
        affect.1(res_var); affect.2(res_var); affect.3(res_var); affect.4(res_var);



&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Degrees of Freedom Intuition</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-05-01/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-05-01/</guid>
      <description>


&lt;p&gt;This post is about building intuition for degrees of freedom. There are two ways to think about it, the “information” way and the “line” way.&lt;/p&gt;
&lt;div id=&#34;the-information-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Information Way&lt;/h3&gt;
&lt;p&gt;The quantity, degrees of freedom, is the amount of information available in our data set minus the amount of information we want to pull from it. Here are a bunch of different ways of representing that idea:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DF} = \textrm{Knowns} - \textrm{Unknowns}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DF} = \textrm{Unique information} - \textrm{Used information}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DF} = \textrm{Unique information in observed data} - \textrm{Information we want to pull from our data}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DF} = \textrm{Stuff in the correlation matrix} - \textrm{Stuff we want to use the correlation matrix for}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\textrm{DF} = \textrm{Unique entries in the variance/covariance matrix} - \textrm{Estimated parameters}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a data set, information is (roughly) the unique variances and covariances that I can use. If I estimate too many parameters without enough information (i.e., without enough observed variables), then I loose DFs and I can’t estimate anything. I cannot estimate a super complicated function if I only observe one measurement of performance and one measurement of motivation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-line-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Line Way&lt;/h3&gt;
&lt;p&gt;The line way builds off the idea that DF = unique information - used information, but we’re going to walk through it visually. Imagine the equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y = mx + b.
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As written, an infinite number of lines can represent that equation. The intercept can be any value and the slope can be any value. There are no constraints.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-01/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now imagine an equation with a constraint, such that the intercept must be 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y = mx + 2.
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can still draw infinite lines here, but all of them must go through 2 as their intercept.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-05-01/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can count our degrees of freedom. Let’s say each example above had 10 pieces of information (10 observed variables).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y = mx + b \textrm{;} \textrm{ with 10 pieces of information and 2 estimated parameters. DF = 10 - 2. DF = 8}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y = mx + 2 \textrm{;} \textrm{ with 10 pieces of information and 1 estimated parameter. DF = 10 - 1. DF = 9}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When you estimate an additional parameter you lose a degree of freedom. When you constrain things, you gain degrees of freedom. So, the second example has more degrees of freedom, even though it feels like we’re not as “free” to draw our lines. Tricky.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Everything Partialled From Everything in Regression</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-04-19/</link>
      <pubDate>Fri, 19 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-04-19/</guid>
      <description>


&lt;p&gt;In regression, everything is partialled from everything. Let’s work through that notion with images and code. Imagine that emotion and ability cause an outcome, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_variance.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What this image represents is that &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has variability (across people or time), and its variability is associated with variability in emotion and variability in ability. Notice that there is variability overlap between ability and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here1.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;emotion and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here2.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;emotion and ability,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here3.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and all three variables.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/here4.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once we regress &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; on emotion and ability, the regression coefficients represent the unique variance components of each predictor&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_coefficients.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;but the technique also removes outcome-relevant variance&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_no_middle.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and overlapping variance in emotion and ability not related to the outcome.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_full_partial.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, in regression we get coefficients that represent the unique variance contribution of each predictor while partialling overlapping, outcome-relevant variance and overlapping, non-relevant variance. Emotion and ability get to account for their own causal effects of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, but neither predictor gets the overlapping variance in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and the emotion and ability coefficients are adjusted for the emotion-ability overlap situated outside &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s do it with code.&lt;/p&gt;
&lt;p&gt;Our sample contains 500 people with correlated emotion and ability (&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; = 0.4).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- 500
emotion &amp;lt;- rnorm(people, 0, 10)
ability &amp;lt;- 0.4*emotion + rnorm(people, 0, 1) # could also do it with MASS&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ability and emotion cause &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error &amp;lt;- rnorm(people, 0, 1)
Y &amp;lt;- 2 + 0.5*ability + 0.38*emotion + error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regression will recover the parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(
  &amp;#39;emotion&amp;#39; = c(emotion),
  &amp;#39;ability&amp;#39; = c(ability),
  &amp;#39;y&amp;#39; = c(Y)
)

summary(lm(y ~ ability + emotion,
           data = df))$coefficients[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)     ability     emotion 
##   1.9531540   0.4584228   0.4002376&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember, each coefficient is consistent with the “lightning bolt” variance components above. Outcome-relevant overlap is removed and overlap between emotion and ability is removed. Since emotion and ability are partialled from each other, we won’t recover the 0.38 parameter relating emotion to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; if we remove ability from the equation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ emotion,
           data = df))$coefficients[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)     emotion 
##   1.9648929   0.5861932&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can we modify our variables to represent the “partialled multiple regression coefficient” for emotion? Naively, it seems that if we remove ability from &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and then regress &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; on emotion we will recover the appropriate 0.38 parameter. Let’s try.&lt;/p&gt;
&lt;p&gt;Regress &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; on just ability&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;just_ability &amp;lt;- lm(y ~ ability,
               data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and take the residuals, meaning that in our next regression we will examine the effect of emotion on “leftover &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;” – &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with no influence from ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_with_ability_removed &amp;lt;- resid(just_ability)
df$y_with_ability_removed &amp;lt;- y_with_ability_removed

summary(lm(y_with_ability_removed ~ emotion,
           data = df))$coefficients[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)     emotion 
##  0.01927374  0.02051948&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nope. Why not? Think back to the diagrams, what we just assessed was&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/remove_ability.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;where the estimate accounts for the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;-relevant overlap of emotion and ability, but it is wrong because it doesn’t account for the overlap between emotion and ability situated outside of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. In regression, everything is partialled from everything…we have not yet accounted for the overlap between emotion and ability in the space not in the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; variance sphere. Now we will.&lt;/p&gt;
&lt;p&gt;Partial ability from emotion&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_with_ability_removed &amp;lt;- resid(lm(emotion ~ ability,
                                         data = df))

df$emotion_with_ability_removed &amp;lt;- emotion_with_ability_removed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and now when we regress “Y with ability removed” on “emotion with ability removed” we will recover the 0.38 parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y_with_ability_removed ~ emotion_with_ability_removed,
           data = df))$coefficients[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  (Intercept) emotion_with_ability_removed 
##                -7.545488e-17                 4.002376e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In regression, everything is partialled from everything.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/content/Computational_Notes/partial_images/partial_full_partial.png&#34; width=&#34;500&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The technique partials overlapping predictor variance both within and outside of the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; space. Neither predictor accounts for overlapping variance within &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and if an important predictor is excluded then it will artificially account for variance it shouldn’t be capturing.&lt;/p&gt;
&lt;p&gt;Note that all of this is relevant for III sums of squares…there are other approaches but III is by far the most common.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convert Multiple Columns to Numeric or Character</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-04-10/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-04-10/</guid>
      <description>


&lt;p&gt;Quick piece of code that turns all selected columns to numeric in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df[, c(&amp;#39;col1&amp;#39;, &amp;#39;col2&amp;#39;)] &amp;lt;- as.numeric(as.character(unlist(df[, c(&amp;#39;col1&amp;#39;, &amp;#39;col2&amp;#39;)])))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mutating within &lt;code&gt;tidyverse&lt;/code&gt; is always a good options as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  mutate_at(vars(&amp;#39;column1&amp;#39;, &amp;#39;column2&amp;#39;), as.character)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulating a Moving Average Process</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-29/</link>
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-29/</guid>
      <description>


&lt;p&gt;Two ways to simulate a moving average process. A moving average is a linear combination of concurrent and historic noises:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_t = z_t + z_{t-1} + z_{t-2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is the outcome variable that is influenced by noise at this moment (&lt;span class=&#34;math inline&#34;&gt;\(z_t\)&lt;/span&gt;) and noise from the last two time points. MA(q) processes can occur at any lag, I will use a two lag version here.&lt;/p&gt;
&lt;p&gt;The first way to simulate this process is to generate all noise terms and then sample from that distribution throughout our recursive routine.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

time &amp;lt;- 200
noise &amp;lt;- rnorm(time)
ma_2 &amp;lt;- NULL
for(i in 3:time){
  
  ma_2[i] &amp;lt;- noise[i] + 0.7*noise[i-1] + 0.2*noise[i-2]
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That simulation results in the following.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)

df1 &amp;lt;- data.frame(
  &amp;#39;time&amp;#39; = c(1:time),
  &amp;#39;y&amp;#39; = c(ma_2)
)

ggplot(df1, aes(x = time, y = y)) + 
  geom_point() + 
  geom_line() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-01-29/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The second way to simulate it is to generate noise within the loop itself, store the noise, and then apply it to the outcome across time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(15)

yt &amp;lt;- numeric(time)
zs &amp;lt;- numeric(time)

for(i in 1:time){
  
  if(i == 1){
    
    zs[i] &amp;lt;- rnorm(1,0,1)
    yt[i] &amp;lt;- zs[i]
  
  }else if(i == 2){
    
    zs[i] &amp;lt;- rnorm(1,0,1) 
    yt[i] &amp;lt;- zs[i] + 0.7*zs[i-1]
    
    }else{
  
    zs[i] &amp;lt;- rnorm(1,0,1)
    yt[i] &amp;lt;- zs[i] + 0.7*zs[i-1] + 0.2*zs[i-2]
  
    }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- data.frame(
  &amp;#39;time&amp;#39; = c(1:time),
  &amp;#39;y&amp;#39; = c(yt)
)

ggplot(df2, aes(x = time, y = yt)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-01-29/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The second simulation style takes more code but I find it more intuitive. It is difficult for me to wrap my head around simulating all of the noise first and then applying it to the process as if the two are independent components – which is what the first simulation code mimics. To each their own.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Turning Unequal Dates into Days</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-15/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-15/</guid>
      <description>


&lt;p&gt;Longitudinal data of a group or team often have missing days. For example, only Bob reports a stress score on January 3rd even though Joe and Sam are also part of the sample.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    id       date stress
## 1 bob 2019-01-01      4
## 2 joe 2019-01-01      5
## 3 sam 2019-01-01      6
## 4 bob 2019-01-02      6
## 5 joe 2019-01-02      5
## 6 bob 2019-01-03      4
## 7 bob 2019-01-04      5
## 8 joe 2019-01-04      6
## 9 sam 2019-01-04      7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to create an additional column called “day” and use integers rather than dates to make plotting easier/prettier. To do so, we need to create a new data frame of unique dates and unique days, and then we need to merge that new data fram with the original to align the new “day” integer values.&lt;/p&gt;
&lt;p&gt;Turn the dates into a character vector so that they are easier to work with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$date &amp;lt;- as.character(df$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now give each unique date a respective integer “day” value in a new data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;uniq_dates &amp;lt;- unique(df$date)

day_integers &amp;lt;- data.frame(
  &amp;#39;date&amp;#39; = c(uniq_dates),
  &amp;#39;day&amp;#39; = c(1:length(uniq_dates))
)

day_integers$date &amp;lt;- as.character(day_integers$date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, merge the new &lt;code&gt;day_integers&lt;/code&gt; data frame with the original so that we have easy numbers for plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_df &amp;lt;- left_join(df, day_integers)

plot_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id       date stress day
## 1 bob 2019-01-01      4   1
## 2 joe 2019-01-01      5   1
## 3 sam 2019-01-01      6   1
## 4 bob 2019-01-02      6   2
## 5 joe 2019-01-02      5   2
## 6 bob 2019-01-03      4   3
## 7 bob 2019-01-04      5   4
## 8 joe 2019-01-04      6   4
## 9 sam 2019-01-04      7   4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One additional note. It can be instructive to see the inefficient way to get the same result using a &lt;code&gt;for-loop&lt;/code&gt;. Here is un-evaluated code that is the for-loop equivalent to above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# take unique date
# which rows match 
# plug in counter to those values
# increase counter by 1

time_vec &amp;lt;- numeric(nrow(original_df))
unique_dates &amp;lt;- unique(original_df$date)

counter &amp;lt;- 0

for(i in 1:length(unique_dates)){
  
  # take unique date
  
  datey &amp;lt;- unique_dates[i]
  
  # which rows match this date?
  
  use_rows &amp;lt;- which(original_df$date == datey)
  
  # increase counter
  
  counter &amp;lt;- counter + 1
  
  # plug in counter in time vec
  
  time_vec[use_rows] &amp;lt;- counter
  
}

original_df$day &amp;lt;- time_vec&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating Time in a Data Frame</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-14/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-14/</guid>
      <description>


&lt;p&gt;There are two code variations I use to generate time indexes. If I need time cycles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   id    score time
## 1  a 24.81885    1
## 2  b 18.46513    2
## 3  c 15.95025    3
## 4  a 24.79416    1
## 5  b 18.14886    2
## 6  c 18.99802    3
## 7  a 14.98879    1
## 8  b 23.75162    2
## 9  c 12.85643    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then I use a sequence command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- seq(from = 1, to = 3, each = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I need time ordered&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   id    score time
## 1  a 26.85878    1
## 2  a 26.10859    1
## 3  a 20.01426    1
## 4  b 22.19061    2
## 5  b 17.04311    2
## 6  b 19.55141    2
## 7  c 16.98356    3
## 8  c 24.42635    3
## 9  c 23.62059    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then I use a replicate command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- rep(c(1:3), each = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Only Store Successful Output - A Counter Placement Issue</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-13/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-13/</guid>
      <description>


&lt;p&gt;Sometimes I store every result in my initialized vector/matrix.&lt;/p&gt;
&lt;p&gt;Here is the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   people    values day
## 1   john 10.215066   1
## 2  teddy 10.270607   1
## 3  clare 10.216560   1
## 4   john  9.712433   2
## 5  teddy  8.988547   2
## 6   john  9.441930   3
## 7  teddy 10.482020   3
## 8  clare  8.922451   3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the code. I want to find the days where I have responses from John, Teddy, and Clare (as you can tell, I only have responses from all three of them on days 1 and 3).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_days &amp;lt;- numeric(length(unique(df$days))) # initialized vector
counter &amp;lt;- 0

select_days &amp;lt;- c(1, 2, 3) 

for(i in 1:length(select_days)){
  counter &amp;lt;- counter + 1
  
  
  # select the i-th day
  
  filter_data &amp;lt;- df %&amp;gt;%
    filter(day == select_days[i])
  
  # are there three responses on this day?
  
  if(length(filter_data$day) == 3){ 
  use_days[counter] &amp;lt;- filter_data$day
  }
}

use_days&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That code works, but what if I don’t want to store that NA during the second iteration? To only store successful output, put the counter in the “if statement.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;use_days &amp;lt;- numeric(length(unique(df$days))) # initialized vector
counter &amp;lt;- 0

select_days &amp;lt;- c(1, 2, 3) 

for(i in 1:length(select_days)){
  
  # select the i-th day
  
  filter_data &amp;lt;- df %&amp;gt;%
    filter(day == select_days[i])
  
  # are there three responses on this day?
  
  if(length(filter_data$day) == 3){ 
      counter &amp;lt;- counter + 1            # HERE IS THE CHANGE

  use_days[counter] &amp;lt;- filter_data$day
  }
}

use_days&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Row Labels Needed to Spread</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-11/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-11/</guid>
      <description>


&lt;p&gt;No explanation for this set of notes, just a few reminders when spreading and gathering.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   b_partial b_wo_partial se_partial se_wo_partial
## 1         1            4          6             3
## 2         2            5          7             2
## 3         3            6          8             1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want the columns to be “model,” “result,” and “value.”&lt;/p&gt;
&lt;p&gt;Here is my incorrect attempt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_try &amp;lt;- cd_try %&amp;gt;%
  gather(b_partial, b_wo_partial, key = &amp;#39;model&amp;#39;, value = &amp;#39;b1&amp;#39;) 

cd_try&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   se_partial se_wo_partial        model b1
## 1          6             3    b_partial  1
## 2          7             2    b_partial  2
## 3          8             1    b_partial  3
## 4          6             3 b_wo_partial  4
## 5          7             2 b_wo_partial  5
## 6          8             1 b_wo_partial  6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_try &amp;lt;- cd_try %&amp;gt;%
  gather(se_partial, se_wo_partial, key = &amp;#39;se_model&amp;#39;, value = &amp;#39;sd&amp;#39;)

cd_try # not evaluated because it won&amp;#39;t work&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead, I need to gather everything in at the same time, split, and then spread.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   b_partial b_wo_partial se_partial se_wo_partial
## 1         1            4          6             3
## 2         2            5          7             2
## 3         3            6          8             1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gather&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_try &amp;lt;- cd_try %&amp;gt;%
  gather(b_partial, b_wo_partial, 
         se_partial, se_wo_partial,
         key = &amp;#39;result_model&amp;#39;, value = &amp;#39;value&amp;#39;) # gather everything

cd_try&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     result_model value
## 1      b_partial     1
## 2      b_partial     2
## 3      b_partial     3
## 4   b_wo_partial     4
## 5   b_wo_partial     5
## 6   b_wo_partial     6
## 7     se_partial     6
## 8     se_partial     7
## 9     se_partial     8
## 10 se_wo_partial     3
## 11 se_wo_partial     2
## 12 se_wo_partial     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Split&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_try &amp;lt;- cd_try %&amp;gt;%
  separate(result_model, into = c(&amp;#39;result&amp;#39;, &amp;#39;model&amp;#39;), sep = &amp;quot;_&amp;quot;)

cd_try&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    result   model value
## 1       b partial     1
## 2       b partial     2
## 3       b partial     3
## 4       b      wo     4
## 5       b      wo     5
## 6       b      wo     6
## 7      se partial     6
## 8      se partial     7
## 9      se partial     8
## 10     se      wo     3
## 11     se      wo     2
## 12     se      wo     1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spread, BUT WHEN YOU SPREAD MAKE SURE TO INCLUDE ROW IDENTIFIERS.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_try &amp;lt;- cd_try %&amp;gt;%
  mutate(row_help = rep(1:6, 2))

cd_try &amp;lt;- cd_try %&amp;gt;%
  spread(result, value)

cd_try&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     model row_help b se
## 1 partial        1 1  6
## 2 partial        2 2  7
## 3 partial        3 3  8
## 4      wo        4 4  3
## 5      wo        5 5  2
## 6      wo        6 6  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reveal Hidden NA&#39;s in Longitudinal Data</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-10/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-10/</guid>
      <description>


&lt;p&gt;Longitudinal data sets often have hidden NAs when they are in long-form. For example, in the data set below Zoe is missing on days 2 and 4, but it isn’t obvious because there are no specific “NA’s” within the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    time   id q1 q2
## 1     1  Jac  4  3
## 2     1 Jess  5  2
## 3     1  Zoe  3  4
## 4     2  Jac  6  1
## 5     2 Jess  7  2
## 6     3  Jac  5  3
## 7     3 Jess  4  4
## 8     3  Zoe  3  2
## 9     4  Jac  4  3
## 10    4 Jess  5  4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usually I recommend cleaning within the &lt;code&gt;tidyverse&lt;/code&gt; package, but in this case I prefer &lt;code&gt;reshape&lt;/code&gt;. Change the data frame to wide&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)
wide_cd &amp;lt;- reshape(cd, timevar = &amp;#39;time&amp;#39;, idvar = &amp;#39;id&amp;#39;, direction = &amp;#39;wide&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then back to long to reveal the hidden NA’s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd_reveal &amp;lt;- reshape(wide_cd, timevar = &amp;#39;time&amp;#39;, idvar = &amp;#39;id&amp;#39;, direction = &amp;#39;long&amp;#39;)
cd_reveal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          id time q1.1 q2.1
## Jac.1   Jac    1    4    3
## Jess.1 Jess    1    5    2
## Zoe.1   Zoe    1    3    4
## Jac.2   Jac    2    6    1
## Jess.2 Jess    2    7    2
## Zoe.2   Zoe    2   NA   NA
## Jac.3   Jac    3    5    3
## Jess.3 Jess    3    4    4
## Zoe.3   Zoe    3    3    2
## Jac.4   Jac    4    4    3
## Jess.4 Jess    4    5    4
## Zoe.4   Zoe    4   NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is possible to do all of this within &lt;code&gt;tidyverse&lt;/code&gt;, but it’s tricky because the &lt;code&gt;spread&lt;/code&gt; command only applies to one column (the value parameter only takes one entry), so anytime your data frame contains multiple columns to spread over (almost always the case) then &lt;code&gt;spread&lt;/code&gt; does not work well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
cd %&amp;gt;%
 spread(key = time, value = q1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     id q2  1  2  3  4
## 1  Jac  1 NA  6 NA NA
## 2  Jac  3  4 NA  5  4
## 3 Jess  2  5  7 NA NA
## 4 Jess  4 NA NA  4  5
## 5  Zoe  2 NA NA  3 NA
## 6  Zoe  4  3 NA NA NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how it only used q1. The proper way to go from long to wide and then back to long to reveal the NA’s using &lt;code&gt;tidyverse&lt;/code&gt; is either of the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  select(time, id, q1) %&amp;gt;%
  spread(key = time, value = q1) %&amp;gt;%
  gather(key = time, value = &amp;#39;q1&amp;#39;, &amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;) # string code needed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      id time q1
## 1   Jac    1  4
## 2  Jess    1  5
## 3   Zoe    1  3
## 4   Jac    2  6
## 5  Jess    2  7
## 6   Zoe    2 NA
## 7   Jac    3  5
## 8  Jess    3  4
## 9   Zoe    3  3
## 10  Jac    4  4
## 11 Jess    4  5
## 12  Zoe    4 NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_string &amp;lt;- as.character(unique(cd$time))

cd %&amp;gt;%
  select(time, id, q1) %&amp;gt;%
  spread(key = time, value = q1) %&amp;gt;%
  gather(key = time, value = &amp;#39;q1&amp;#39;, time_string) # string code not needed due to pre-allocation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      id time q1
## 1   Jac    1  4
## 2  Jess    1  5
## 3   Zoe    1  3
## 4   Jac    2  6
## 5  Jess    2  7
## 6   Zoe    2 NA
## 7   Jac    3  5
## 8  Jess    3  4
## 9   Zoe    3  3
## 10  Jac    4  4
## 11 Jess    4  5
## 12  Zoe    4 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, I prefer &lt;code&gt;reshape&lt;/code&gt; because the &lt;code&gt;spread&lt;/code&gt; commands in &lt;code&gt;tidyverse&lt;/code&gt; are not easy to read.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Premature Covariate</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-08/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-08/</guid>
      <description>


&lt;p&gt;A replication of Patricia Cohen’s wonderful, “problem of the premature covariate” (chapter 2 in Collins &amp;amp; Horn, 1991). Here is a simple version of the problem. Imagine that we want to know the influence of a life event, like meeting a friend, on happiness. We conduct a study where we measure people’s happiness at time one, wait two weeks, and then measure their happiness again along with whether or not they met a friend since we last observed them. To assess the relationship between meeting a friend and happiness, we regress post happiness on both pre happiness and whether or not they met a friend. That is, our regression takes the form:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Happy&lt;span class=&#34;math inline&#34;&gt;\(_{post}\)&lt;/span&gt; ~ Happy&lt;span class=&#34;math inline&#34;&gt;\(_{pre}\)&lt;/span&gt; + Met_Friend.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;What Dr. Cohen draws attention to is that the coefficient relating “met friend” to happiness will be biased if (a) there is some non-zero probability of reverse causality and (b) we do not measure “met friend” exactly when it occurs. Remember, in our mock study we assessed both happiness and whether or not someone met a friend during our post-measure. Is that exactly when different people across the sample actually met a new friend? Perhaps, but most of our sample either did or did not meet a friend &lt;em&gt;at some unknown time within the past two weeks&lt;/em&gt;; Dr. Cohen’s point is that this unknown is an issue to linger on.&lt;/p&gt;
&lt;div id=&#34;simulation-explanation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation Explanation&lt;/h1&gt;
&lt;p&gt;If Dr. Cohen’s issue is worthwhile, then, under a system where “met friend” truly does &lt;em&gt;not&lt;/em&gt; influence happiness, we should be able to make “met friend” appear to influence happiness based on the points she raises. That is, when “met friend” does not influence happiness we should be able to make it appear so if we create a system where (1) happiness instead influences the probability of meeting a friend (reverse causality) and (2) we do not measure “met friend” exactly when it occurs. Below, we generate data where “met friend” does not influence happiness but the coefficient relating “met friend” to happiness will still be significant (and large) because of the issues raised.&lt;/p&gt;
&lt;p&gt;Here are the steps to the simulation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Start with a random value for happiness (distributed normally across 600 people) at time one.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Happiness at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt; is its previous value plus one of the following, all with equal probability: +0.25, -0.25, or 0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At each time point, concurrent happiness influences the probability of meeting a friend. When happiness is low people are unlikely to meet a friend, whereas when happiness is high people are more likely to meet a friend. Meeting a friend is coded as 0 or 1 for each time point (i.e., no or yes).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Continue for 25 time points.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assess the relationships between post happiness, pre happiness, and “met friend.” Pre happiness is always time one, whereas we will explore different post assesssments (e.g., post happiness is time 25 vs. post happiness is time 20). “Met friend” will always be whether the individual met a friend within 5 time points of the post happiness assessment. So, if we analyze post happiness as time 20, then “met friend” is whether the individual met a friend during times 15 through 20.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that the simulation captures the notions raised above: “met friend” does not influence happiness, instead the reverse happens. And after making a decision about the timing of our pre and post assessment we lose information about when “met friend” actually happened. We know &lt;em&gt;whether&lt;/em&gt; it happened but not when; we also don’t retain information on the differences in timing across our sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;meeting-a-friend-or-not&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Meeting a Friend or Not&lt;/h1&gt;
&lt;p&gt;The most difficult aspect of the simulation is specifying step 3: “met friend” is some function of concurrent happiness. Dr. Cohen’s original explanation is, “the probability of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; for each unit of time was determined by a Markov process, with probability increasing as a function of the level of contemporaneous &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Probabilities used increased from 0 for those with current &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; less than -1.00 to 0.25 for those with current scores of 1.5 or greater” (she uses different variables for x and y in her discussion). What does that mean? How do we specify a Markov process where the probability of “met friend” is between 0 and 0.25 with respect to happiness cutoffs like -1.00 and 1.5? I don’t know either. But we can make it easier by recognizing that, at its core, the idea is simply, “meeting a friend is more likely when people are happier,” which we can represent with a simple linear equation like &lt;span class=&#34;math inline&#34;&gt;\(y = mx + b\)&lt;/span&gt;. All we need to do is to find the slope and y-intercept, then we’ll have an equation where we can plug in “happiness” and get “probability of meeting a friend.” Here is how.&lt;/p&gt;
&lt;p&gt;Remember that we can find the slope and y-intercept of a line if we know the location of two of its points. Here, we know that the probability of “met friend” needs to be between 0 and 0.25, and the happiness cutoffs need to be -1.00 and 1.5. If I want to relate happiness to “met friend,” then, I can put happiness on the x-axis and “met friend” on the y-axis and recognize that by combining these cutoffs I get the end-points of a line: (1.5, 0.25) is one point and (-1, 1.5) is the other. Computing rise-over-run and then solving for the intercept gives me the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Probability of meeting a friend = 0.1*Happy + 0.01&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we have a way to compute the probability of meeting a friend based on happiness. It is not as precise as the Markov process but it will work just fine. (Note: I actuallly use the points (1.4999, 2.4999) and (-0.999, 1.4999) to calculate the slope and intercept in the simulation because I will also use if-statements for the cutoffs)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-one-person&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulate One Person&lt;/h1&gt;
&lt;p&gt;It’s always helpful to make sure we can get a simulation to work on one person. In the simulation below, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is happiness and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is “met friend.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time &amp;lt;- 25
y &amp;lt;- numeric(time)
x &amp;lt;- numeric(time)

count &amp;lt;- 0

for(i in 1:time){
  count &amp;lt;- count + 1
  
  if(i == 1){
    
    y[count] &amp;lt;- rnorm(1, mean = 0.5, sd = 0.5)
    x[count] &amp;lt;- 0
    
  }else{
    
    
    # y up or down with autoregression
    
    updownsame &amp;lt;- sample(c(&amp;#39;up&amp;#39;, &amp;#39;down&amp;#39;, &amp;#39;same&amp;#39;), 1)
    
    if(updownsame == &amp;#39;up&amp;#39;){
      
      y[count] &amp;lt;- y[count - 1] + 0.25
      
    }else if(updownsame == &amp;#39;down&amp;#39;){
      
      y[count] &amp;lt;- y[count - 1] - 0.25
      
    }else{
      
      y[count] &amp;lt;- y[count - 1]
      
    }
    
    # x is a function of y
    
    if(y[count] &amp;lt;= -1.00){
      
      x_prob &amp;lt;- 0
      
    }else if(y[count] &amp;gt;= 1.5){
      
      x_prob &amp;lt;- 0.25
      
    }else{
      
      x_prob &amp;lt;- 0.10004*y[count] + 0.09994
      
      
    }
    
    x[count] &amp;lt;- rbinom(1, 1, x_prob)
    
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;full-sample&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full Sample&lt;/h1&gt;
&lt;p&gt;That script worked, so now let’s update the code slightly and run it across 600 people.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;people &amp;lt;- 600
time &amp;lt;- 25
df &amp;lt;- matrix(, nrow = people*time, ncol = 4)

count &amp;lt;- 0

for(j in 1:people){
  
  

for(i in 1:time){
  count &amp;lt;- count + 1
  
  if(i == 1){
    
    df[count, 1] &amp;lt;- j
    df[count, 2] &amp;lt;- i
    df[count, 3] &amp;lt;- rnorm(1, mean = 0.5, sd = 0.5)
    df[count, 4] &amp;lt;- 0
    
  }else{
    
    df[count, 1] &amp;lt;- j
    df[count, 2] &amp;lt;- i
    
    # y up or down with autoregression
    
    updownsame &amp;lt;- sample(c(&amp;#39;up&amp;#39;, &amp;#39;down&amp;#39;, &amp;#39;same&amp;#39;), 1)
    
    if(updownsame == &amp;#39;up&amp;#39;){
      
      df[count, 3] &amp;lt;- df[count - 1, 3] + 0.25
      
    }else if(updownsame == &amp;#39;down&amp;#39;){
      
      df[count, 3] &amp;lt;- df[count - 1, 3] - 0.25
      
    }else{
      
      df[count, 3] &amp;lt;- df[count - 1, 3]
      
    }
    
    # x is a function of y
    
    if(df[count, 3] &amp;lt;= -1.00){
      
      x_prob &amp;lt;- 0
      
    }else if(df[count, 3] &amp;gt;= 1.5){
      
      x_prob &amp;lt;- 0.25
      
    }else{
      
      x_prob &amp;lt;- 0.10004*df[count, 3] + 0.09994
      
    }
    
    df[count, 4] &amp;lt;- rbinom(1, 1, x_prob)
    
  }
  
}

  
  
}


df &amp;lt;- data.frame(df)
names(df) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;happy&amp;#39;, &amp;#39;met_friend&amp;#39;)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;Remember, we generated data where “met friend” did not influence happiness. Now we are going to assess the coefficient relating “met friend” to happiness to see if it differs from zero. First, let’s say our post-assessment happened at time 10.&lt;/p&gt;
&lt;p&gt;Trim down our data set to just that time frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happy10_sample &amp;lt;- df %&amp;gt;%
  filter(time &amp;lt; 11)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many friends did each person meet between times 5 and 10?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friend_count &amp;lt;- happy10_sample %&amp;gt;%
  filter(time &amp;gt; 4) %&amp;gt;%
  group_by(id) %&amp;gt;%
  summarise(
    friend_count = sum(met_friend)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now change that to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 = did not happen&lt;/li&gt;
&lt;li&gt;1 = happened at least once (meaning sum of friend_count is not equal to 0)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;friend_count &amp;lt;- friend_count %&amp;gt;%
  mutate(friend_event = case_when(
    friend_count == 0 ~ 0,
    friend_count != 0 ~ 1
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Merge that count back into the happy10 data set and prepare the data for regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Merge back into y10 df

happy10_sample &amp;lt;- left_join(happy10_sample, friend_count)

# Filter down to what&amp;#39;s needed for regression

happy10_filter &amp;lt;- happy10_sample %&amp;gt;%
  select(id, time, happy, friend_event) %&amp;gt;%
  filter(time == 1 | time == 10)

library(reshape2)

happy10_wide &amp;lt;- reshape(happy10_filter, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)

# The x columns are synonymous, so I can remove one 

happy10_wide &amp;lt;- happy10_wide[, c(&amp;#39;id&amp;#39;, &amp;#39;happy.10&amp;#39;, &amp;#39;happy.1&amp;#39;, &amp;#39;friend_event.1&amp;#39;)]
names(happy10_wide) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;happy_post&amp;#39;, &amp;#39;happy_pre&amp;#39;, &amp;#39;met_friend&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now regress post happy on pre happy and whether or not they met a friend between times 5 and 10.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(happy_post ~ happy_pre + met_friend,
           data = happy10_wide))$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) -0.1050699 0.04102004 -2.561429 1.066877e-02
## happy_pre    0.9337432 0.04815831 19.389036 2.537842e-65
## met_friend   0.2526980 0.04992485  5.061568 5.545673e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient relating “met friend” to happiness is about 0.3 and it is significant (remember there was no influence from “met friend” to happiness).&lt;/p&gt;
&lt;p&gt;What about when we change the post assessment to time point 15?&lt;/p&gt;
&lt;p&gt;First create a function out of all the “tidying” steps above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_create &amp;lt;- function(time1){
  library(reshape2)
  library(tidyverse)
  time2 &amp;lt;- time1 - 5
  
  y_sample &amp;lt;- df %&amp;gt;%
    filter(time &amp;lt;= time1)
  
  friend_count &amp;lt;- y_sample %&amp;gt;%
    filter(time &amp;gt;= time2) %&amp;gt;%
    group_by(id) %&amp;gt;%
    summarise(
      friend_count = sum(met_friend)
    )
  
  friend_count &amp;lt;- friend_count %&amp;gt;%
    mutate(friend_event = case_when(
      friend_count == 0 ~ 0,
      friend_count != 0 ~ 1
    ))
  
  y_sample &amp;lt;- left_join(y_sample, friend_count)
  y_filter &amp;lt;- y_sample %&amp;gt;%
    select(id, time, happy, friend_event) %&amp;gt;%
    filter(time == 1 | time == time1)
  
  y_wide &amp;lt;- reshape(y_filter, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)
  
  yname &amp;lt;- paste(&amp;#39;happy.&amp;#39;, time1, sep = &amp;#39;&amp;#39;)
  
  y_wide &amp;lt;- y_wide[, c(&amp;#39;id&amp;#39;, yname, &amp;#39;happy.1&amp;#39;, &amp;#39;friend_event.1&amp;#39;)]
  names(y_wide) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;happy_post&amp;#39;, &amp;#39;happy_pre&amp;#39;, &amp;#39;met_friend&amp;#39;)
  
  return(y_wide)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happy15_wide &amp;lt;- df_create(15)

summary(lm(happy_post ~ happy_pre + met_friend,
           data = happy15_wide))$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) -0.2154859 0.04700472 -4.584346 5.551550e-06
## happy_pre    0.8440452 0.05624981 15.005297 2.076223e-43
## met_friend   0.5000608 0.05837959  8.565678 9.187047e-17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about when we select time 25 as our post assessment?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;happy25_wide &amp;lt;- df_create(25)

summary(lm(happy_post ~ happy_pre + met_friend,
           data = happy25_wide))$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) -0.3849218 0.06220349 -6.188106 1.132745e-09
## happy_pre    0.8216318 0.07466252 11.004608 9.061944e-26
## met_friend   0.8663628 0.07697388 11.255282 8.833209e-27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how large the coefficient relating “met friend” to happiness is here: close to 0.9 – remember, there truly is no effect relating “met friend” to happiness.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;If there is some probability of reverse causality and we don’t measure the event exactly when it occurs then the estimate relating that event to our outcome will be biased. If many “event opportunities” occur between our pre and post measure then our estimate will be extremely biased.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m = )&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Column Names As Parameters with GGplot2</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-06/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-06/</guid>
      <description>


&lt;p&gt;Another example of using column names as parameters with &lt;code&gt;quo&lt;/code&gt;, this time within &lt;code&gt;ggplot2&lt;/code&gt;. A snippet of the data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   day   id stress performance
## 1   1 Josh      8          17
## 2   2 Josh      5           6
## 3   3 Josh      7          15
## 4   4 Josh      7          10
## 5   5 Josh      4           7
## 6   6 Josh      7          13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say we want to plot each person’s stress over time: three time-series trajectories.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)

ggplot(df, aes(x = day, y = stress, color = id)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-01-06/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Great, but imagine having a data set with 300 different DVs. Instead of re-calling &lt;code&gt;ggplot&lt;/code&gt; each time we can create a function where the column (DV) is the paramter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_it &amp;lt;- function(col_name){
  
  g &amp;lt;- ggplot(df, aes(x = day, y = !!col_name, color = id)) + 
  geom_point() + 
  geom_line()
  
  return(g)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the &lt;code&gt;!!&lt;/code&gt; before the parameter. Now, to plot the new graph we use &lt;code&gt;quo&lt;/code&gt; within the function call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_it(quo(performance))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2019-01-06/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mutating Scale Items with NA</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-05/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-05/</guid>
      <description>


&lt;p&gt;Creating item totals with a data set containing NAs is surprisingly difficult. Here is the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

cd &amp;lt;- data.frame(
  &amp;quot;q1&amp;quot; = c(1,2,NA),
  &amp;quot;q2&amp;quot; = c(2,2,2),
  &amp;#39;q3&amp;#39; = c(NA, NA,2),
  &amp;#39;id&amp;#39; = c(&amp;#39;201&amp;#39;, &amp;#39;202&amp;#39;, &amp;#39;203&amp;#39;)
)

cd&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   q1 q2 q3  id
## 1  1  2 NA 201
## 2  2  2 NA 202
## 3 NA  2  2 203&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mutating directly over columns with NA does not work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  mutate(cohesion = 
           q1 + q2 + q3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   q1 q2 q3  id cohesion
## 1  1  2 NA 201       NA
## 2  2  2 NA 202       NA
## 3 NA  2  2 203       NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Filtering removes the data we are interested in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  filter(!is.na(q1) == T &amp;amp;&amp;amp; !is.na(q2) == T &amp;amp;&amp;amp; !is.na(q3) == T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] q1 q2 q3 id
## &amp;lt;0 rows&amp;gt; (or 0-length row.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We cannot use &lt;code&gt;rowMeans&lt;/code&gt; in combination with &lt;code&gt;mutate&lt;/code&gt; because the two are not compatible. The code below is not evaluated, but if you run it it does not work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  mutate(cohesion =
           rowMeans(q1, q2, q3, na.rm = T))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;rowwise&lt;/code&gt; command within a pipe gets us close&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(mean = mean(q1, q2, q3, na.rm = T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
## # Rowwise: 
##      q1    q2    q3 id     mean
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     2    NA 201       1
## 2     2     2    NA 202       2
## 3    NA     2     2 203     NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but the mean value is not calculated correctly. We need to include &lt;code&gt;c()&lt;/code&gt; to vectorize the items.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(mean = mean(c(q1, q2, q3), na.rm = T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
## # Rowwise: 
##      q1    q2    q3 id     mean
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     2    NA 201     1.5
## 2     2     2    NA 202     2  
## 3    NA     2     2 203     2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally the right answer. Use &lt;code&gt;rowwise&lt;/code&gt; in combination with a vectorized &lt;code&gt;mutate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Frequentist Confidence Intervals</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2019-01-04/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2019-01-04/</guid>
      <description>


&lt;div id=&#34;purpose&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Purpose&lt;/h3&gt;
&lt;p&gt;Imagine that you are interested in the relationship between stress and performance. To assess it, you observe 600 people at work and measure their stress via a self-report (e.g., “I feel stressed”) and their performance via objective performance scores for the day (e.g., number of sales). You regress performance on stress and find that the estimated coefficient relating to two is 0.45. You then build a 95% confidence interval using the standard error that the analysis spit out and find that the CI is 0.45 +- 0.1.&lt;/p&gt;
&lt;p&gt;What does that confidence interval actually mean? The purpose of this exercise is to build intuition behind frequentist CIs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Steps&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Generate the population&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sample the population. On that sample…&lt;/p&gt;
&lt;p&gt;2a) Regress performance on stress&lt;/p&gt;
&lt;p&gt;2b) Calculate a CI&lt;/p&gt;
&lt;p&gt;2c) Does the CI contain the population parameter?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Re-sample and repeat&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;generate-the-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1) Generate the population&lt;/h3&gt;
&lt;p&gt;Our population will contain 100,000 people&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pop_number &amp;lt;- 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with stress scores distributed about zero. The scale here doesn’t matter – we care about the relationship between stress and performance and less about (in this example) the distributions of stress and performance themselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;population_stress &amp;lt;- rnorm(pop_number, 0, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The true relationship between stress and performance will be 0.45. Let’s set that parameter&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stress_performance_coefficient &amp;lt;- 0.45&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then generate performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;population_performance &amp;lt;- stress_performance_coefficient*population_stress + rnorm(pop_number, 0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now plug everything into a data set. Remember, this is the population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(
  &amp;#39;person&amp;#39; = c(1:pop_number),
  &amp;#39;stress&amp;#39; = c(population_stress),
  &amp;#39;performance&amp;#39; = c(population_performance)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the paramter relating stress to performance? 0.45, keep that in mind. Time to sample the population as if we conducted a study and run our regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-the-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2) Sample the population&lt;/h3&gt;
&lt;p&gt;Randomly select 600 people from our population. That is, pretend we ran a study on 600 subjects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_size &amp;lt;- 600
random_numbers &amp;lt;- sample(c(1:pop_number), sample_size)

sample_df &amp;lt;- df %&amp;gt;%
  filter(person %in% random_numbers)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-regress-performance-on-stress&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2a) Regress Performance on Stress&lt;/h3&gt;
&lt;p&gt;Use the &lt;code&gt;lm&lt;/code&gt; command for regression in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(performance ~ stress,
           data = sample_df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = performance ~ stress, data = sample_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4448 -0.7170 -0.0222  0.6474  2.9734 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.036526   0.039568  -0.923    0.356    
## stress       0.454806   0.007939  57.285   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9686 on 598 degrees of freedom
## Multiple R-squared:  0.8459, Adjusted R-squared:  0.8456 
## F-statistic:  3282 on 1 and 598 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;b-compute-the-ci&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2b) Compute the CI&lt;/h3&gt;
&lt;p&gt;Save the output of the regression in an object so we can pull out the specific coefficients that we are interested in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output &amp;lt;- summary(lm(performance ~ stress,
                  data = sample_df))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pull out the coefficient relating stress to performance&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;slope_coefficient &amp;lt;- output$coefficients[2,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use it along with the SEs to calculate the confidence interval.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se_upper &amp;lt;- slope_coefficient + 1.96*output$coefficients[2,2]
se_lower &amp;lt;- slope_coefficient - 1.96*output$coefficients[2,2]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;c-does-the-ci-contain-the-population-parameter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2c) Does the CI contain the population parameter?&lt;/h3&gt;
&lt;p&gt;Remember that the parameter is 0.45.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;contain_parameter &amp;lt;- NULL

if(se_lower &amp;lt;= stress_performance_coefficient &amp;amp;&amp;amp; se_upper &amp;gt;= stress_performance_coefficient){
  contain_parameter &amp;lt;- &amp;#39;yes&amp;#39;
}else{
  contain_parameter &amp;lt;- &amp;#39;no&amp;#39;
}

contain_parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;yes&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What did we do? We sampled the population, ran a regression to relate stress to performance, and then calculated a CI on the slope term. The interpretation of a CI, however, is across infinite samples. Now we need to run through the sample, regress, and calculate CI procedure again and again and again – Monte Carlo.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;re-sample-and-repeat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3) Re sample and repeat&lt;/h3&gt;
&lt;p&gt;I created a function that samples the population, runs a regression, calculates the CI, and then saves whether or not the interval contained 0.45 (‘yes’ or ‘no’). You can view that code in the raw rmarkdown file. For now, just know that the function is called &lt;code&gt;sample_regress_calc_ci&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We are going to re-run step 2 from above 900 times&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sims &amp;lt;- 900&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and store the ‘yes’ or ‘no’ result in a vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_ci_contains &amp;lt;- numeric(sims)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the full Monte Carlo code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sims &amp;lt;- 900
all_ci_contains &amp;lt;- numeric(sims)

for(i in 1:sims){
  
  result &amp;lt;- sample_regress_calc_ci()
  all_ci_contains[i] &amp;lt;- result
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation&lt;/h3&gt;
&lt;p&gt;How many computed intervals contain the population parameter?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(all_ci_contains == &amp;#39;yes&amp;#39;) / sims&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9533333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://christopherdishop.netlify.app/projects/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://christopherdishop.netlify.app/about/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://christopherdishop.netlify.app/resume/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/resume/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talks &amp; Workshops</title>
      <link>https://christopherdishop.netlify.app/talks/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More on Column Names as Parameters</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-09-06/</link>
      <pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-09-06/</guid>
      <description>


&lt;p&gt;Use &lt;code&gt;quo&lt;/code&gt; or &lt;code&gt;enquo&lt;/code&gt; when you want to include column names as parameters in a function. For example, a function like the following would not work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bad_function &amp;lt;- function(data, col_name){
  
  newdf &amp;lt;- data %&amp;gt;%
    mutate(&amp;#39;adjusted_column&amp;#39; = col_name + 1)
  
  return(newdf)
  
}

bad_function(df, column_i_care_about)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;because &lt;code&gt;column_i_care_about&lt;/code&gt; isn’t specified in a form that &lt;code&gt;mutate&lt;/code&gt; can work with.&lt;/p&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;The data are contained in &lt;code&gt;df1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- data.frame(
  a = c(1,2,NA),
  b = c(NA,3,4)
)

df1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    a  b
## 1  1 NA
## 2  2  3
## 3 NA  4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function: take the column specified by the parameter and add one to every value. Then return the new data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adder &amp;lt;- function(col_use){
  
  newdf &amp;lt;- df1 %&amp;gt;%
    mutate(&amp;#39;adder&amp;#39; = 
             (!!col_use) + 1)  # correct form here using !!
    
  return(newdf)
  
}

adder(quo(a))                 # correct form here using quo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    a  b adder
## 1  1 NA     2
## 2  2  3     3
## 3 NA  4    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more complicated function by incorporating &lt;code&gt;is.na&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;na_tagger &amp;lt;- function(col_use){
  
  newdf &amp;lt;- df1 %&amp;gt;%
    mutate(&amp;#39;na_tag&amp;#39; = 
             ifelse(is.na((!!col_use)) == T, 1, 0))
  
  return(newdf)
}

na_tagger(quo(a))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    a  b na_tag
## 1  1 NA      0
## 2  2  3      0
## 3 NA  4      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the examples above I used &lt;code&gt;quo&lt;/code&gt; interactively. You get the same result by instead using &lt;code&gt;enquo&lt;/code&gt; within the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adder2 &amp;lt;- function(col_use){
  
  col_use &amp;lt;- enquo(col_use)
  
  newdf &amp;lt;- df1 %&amp;gt;%
    mutate(&amp;#39;adder&amp;#39; = 
             (!!col_use) + 1)
  
  return(newdf)
}

adder2(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    a  b adder
## 1  1 NA     2
## 2  2  3     3
## 3 NA  4    NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;one-more-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One More Note&lt;/h1&gt;
&lt;p&gt;Sometimes I also need to specify the data set and column within a &lt;code&gt;dplyr&lt;/code&gt; command and then use the parameter to select a specific row. The following format seems to work well: &lt;code&gt;data[[&#39;col_name&#39;]][row]&lt;/code&gt;. Here is a function that is inefficient but demonstrates the point well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selector2 &amp;lt;- function(x, y){
  
  new &amp;lt;- df1 %&amp;gt;%
    filter(robby == df1[[&amp;#39;robby&amp;#39;]][x]) %&amp;gt;%
    filter(ruddy == df1[[&amp;#39;ruddy&amp;#39;]][y])
  
  return(new)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Monte Carlo Approximation</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-08-12/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-08-12/</guid>
      <description>


&lt;p&gt;Monte Carlo helps us understand processes that we can describe but don’t yet have analytic solutions for. Here are two examples: the birthday problem and the tasting tea problem.&lt;/p&gt;
&lt;div id=&#34;birthday-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Birthday Problem&lt;/h1&gt;
&lt;p&gt;If you are standing in a room with 25 other people, what is the probability that at least two people share the same birthday? This question has a mathematical solution, but if we don’t know it we can use Monte Carlo to help.&lt;/p&gt;
&lt;p&gt;Select 25 people with random birthdays&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_birthdays &amp;lt;- sample(1:365, 15, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then check whether two of them share a birthday.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shared_birthday &amp;lt;- length(group_birthdays[duplicated(group_birthdays)])

# Returns 1 if yes and 0 if no&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now place everything into a loop and evaluate 5000 times for the final Monte Carlo:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_size &amp;lt;- 15
iterations &amp;lt;- 5000
shared_birthdays_counter &amp;lt;- 0

for(i in 1:iterations){
  
  
  group_birthdays &amp;lt;- sample(1:365, 15, replace = TRUE)
  
  shared_birthday &amp;lt;- length(group_birthdays[duplicated(group_birthdays)])
  
  if(shared_birthday == 1){
    
    shared_birthdays_counter &amp;lt;- shared_birthdays_counter + 1
  }
  
  
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of a shared birthday among a group of 15 is…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shared_birthdays_counter / iterations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The probability of a shared birthday as we increase group size…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sizes &amp;lt;- 2:25
prob_store &amp;lt;- numeric(length(sizes))

for(j in 1:24){
  
  


group_size &amp;lt;- j
iterations &amp;lt;- 5000
shared_birthdays_counter &amp;lt;- 0

for(i in 1:iterations){
  
  
  group_birthdays &amp;lt;- sample(1:365, group_size, replace = TRUE)
  
  shared_birthday &amp;lt;- length(group_birthdays[duplicated(group_birthdays)])
  
  if(shared_birthday == 1){
    
    shared_birthdays_counter &amp;lt;- shared_birthdays_counter + 1
  }
  
  
  
}

prob_store[j] &amp;lt;- shared_birthdays_counter / iterations

}

df &amp;lt;- data.frame(
  &amp;#39;group_size&amp;#39; = c(2:25),
  &amp;#39;probability&amp;#39; = c(prob_store)
)

library(ggplot2)

plot1 &amp;lt;- ggplot(df, aes(x = group_size, y = probability)) + 
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;orange&amp;#39;)

plot1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-08-12/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The equation to solve the birthday problem is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
n! / (n - k)!
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of possible birthdays and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is group size. The beauty of Monte Carlo is that we didn’t need the above equation to learn about our shared birthday process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tasting-tea&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tasting Tea&lt;/h1&gt;
&lt;p&gt;Imagine that I make one cup of tea with milk and then ask you the following: did I pour the tea or milk first? I repeat this for eight cups of tea. What is the probability that you guess correctly for 3 of the cups? For all 8 cups?&lt;/p&gt;
&lt;p&gt;First, we generate truth. For each cup, ‘M’ means I poured milk first and ‘T’ means I poured tea first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;possible_pours &amp;lt;- c(rep(&amp;#39;M&amp;#39;, 4), rep(&amp;#39;T&amp;#39;, 4))
true_pours &amp;lt;- sample(possible_pours, size = 8)

# The true first pours

true_pours&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;M&amp;quot; &amp;quot;M&amp;quot; &amp;quot;T&amp;quot; &amp;quot;T&amp;quot; &amp;quot;M&amp;quot; &amp;quot;T&amp;quot; &amp;quot;T&amp;quot; &amp;quot;M&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you make a guess for each cup.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;guess &amp;lt;- c(&amp;#39;M&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;M&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;M&amp;#39;, &amp;#39;M&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, you guessed that I poured milk first for cup 1 and tea first for cup 2. How many of your guesses are correct?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;correct &amp;lt;- sum(true_pours == guess)

correct&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can put all of that into a Monte Carlo loop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iterations &amp;lt;- 5000
correct_store &amp;lt;- numeric(iterations)

for(i in 1:iterations){
  
  possible_pours &amp;lt;- c(rep(&amp;#39;M&amp;#39;, 4), rep(&amp;#39;T&amp;#39;, 4))
  true_pours &amp;lt;- sample(possible_pours, size = 8)
  
  guess &amp;lt;- c(&amp;#39;M&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;M&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;T&amp;#39;, &amp;#39;M&amp;#39;, &amp;#39;M&amp;#39;)
  
  correct &amp;lt;- sum(true_pours == guess)
  
  correct_store[i] &amp;lt;- correct

  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the probability of you guessing correctly for 2 cups…6?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(correct_store))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## correct_store
##      0      2      4      6      8 
## 0.0144 0.2324 0.5022 0.2372 0.0138&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like the birthday problem, there are equations that govern this “tea problem.” We don’t know what they are, but we can still learn about the process by using Monte Carlo approximation.&lt;/p&gt;
&lt;p&gt;These examples can be found with greater discussion in &lt;em&gt;Quantitative Social Science&lt;/em&gt; by Kosuke Imai.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Plotting</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-07-04/</link>
      <pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-07-04/</guid>
      <description>


&lt;p&gt;A few random notes about plotting, describing, and thinking about trajectories.&lt;/p&gt;
&lt;div id=&#34;plotting-trajectories&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Plotting Trajectories&lt;/h1&gt;
&lt;p&gt;Imagine we record “affect” (&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;) for five people over 20 time points. ggplot2 produces poor longitudinal trajectories if you only specify time and affect as variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)

plot1 &amp;lt;- ggplot(df1, aes(x = time, y = affect)) + 
  geom_point() + 
  geom_line()

plot1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead, specify “id” either as the grouping variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot2 &amp;lt;- ggplot(df1, aes(x = time, y = affect, group = id)) + 
  geom_point() + 
  geom_line()

plot2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;or a color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot3 &amp;lt;- ggplot(df1, aes(x = time, y = affect, color = id)) + 
  geom_point() + 
  geom_line()

plot3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you have a data set with too many trajectories&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;then select a random sample to keep dark&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2_sample_ids &amp;lt;- sample(df2$id, 5)
df2_sample &amp;lt;- df2 %&amp;gt;%
  filter(id %in% df2_sample_ids)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and change the color of the background trajectories to a lighter color.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot5 &amp;lt;- ggplot(df2, aes(x = time, y = affect, group = id)) + 
  geom_point(color = &amp;#39;gray85&amp;#39;) + 
  geom_line(color = &amp;#39;gray85&amp;#39;) + 
  
  
  # HERE COMES ADDITIONAL CHANGES
  
  geom_point(data = df2_sample, aes(x = time, y = affect, group = id)) + 
  geom_line(data = df2_sample, aes(x = time, y = affect, group = id))

plot5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that I had to evoke two additional geom commands and source my new data sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trajectory-descriptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trajectory Descriptions&lt;/h1&gt;
&lt;div id=&#34;equilibrium&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Panel A: Increasing equilibrium level with constant variance.&lt;/p&gt;
&lt;p&gt;Panel B: Decreasing equilibrium level with constant variance.&lt;/p&gt;
&lt;p&gt;Panel C: Decreasing equilibrium level with increasing variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-growth-intercepts-and-slopes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Latent Growth Intercepts and Slopes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Panel A: Between person differences in intercept but no differences in slope.&lt;/p&gt;
&lt;p&gt;Panel B: Between person differences in slope but no differences in intercept.&lt;/p&gt;
&lt;p&gt;Panel C: Between person differences in intercepts and slopes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;between-and-within-person-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Between and Within Person Variance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Panel A: Between person differences in level (intercept in LGC literature) but no between person differences in variability.&lt;/p&gt;
&lt;p&gt;Panel B: No between person differences in level (intercept) or variability, but the amount of variability in these trajectories is greater than Panel A.
Panel C: No between person differences in level (intercept) but there are between person differences in variability.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;main-effects-and-interactions-cross-sectional-vs.over-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Main Effects and Interactions (Cross Sectional vs. Over Time)&lt;/h1&gt;
&lt;p&gt;Imagine we re-test the main and interaction effects from a cross-sectional study several times. If the results are stable across time, what would they look like?&lt;/p&gt;
&lt;div id=&#34;main-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Main Effect&lt;/h3&gt;
&lt;p&gt;Group A (difficult, specific goals) higher performance than group B (vague goals).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interaction&lt;/h3&gt;
&lt;p&gt;For males: Group A (difficult, specific goals) higher performance than group B (vague goals).
For females: Group B (vague goals) higher performance than group B (difficult, specific goals).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction-and-main-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interaction and Main Effect&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-07-04/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LICENSE: CC-BY-SA</title>
      <link>https://christopherdishop.netlify.app/license/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://christopherdishop.netlify.app/license/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://christopherdishop.netlify.app/post/&#34;&gt;blog posts&lt;/a&gt; are released under a 
&lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;i class=&#34;fab fa-creative-commons fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-by fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-sa fa-2x&#34;&gt;&lt;/i&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Column Names As Parameters</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-06-02/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-06-02/</guid>
      <description>


&lt;p&gt;I always forget how to use column names as function parameters, so here is an example.&lt;/p&gt;
&lt;div id=&#34;function-with-no-column-name-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Function with no column name parameters&lt;/h1&gt;
&lt;p&gt;Function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Select columns&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Replace the Jimmy and James ‘v_1’ values with 99&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

dish &amp;lt;- data.frame(
  &amp;#39;person&amp;#39; = c(&amp;#39;jimmy&amp;#39;, &amp;#39;james&amp;#39;, &amp;#39;johnny&amp;#39;),
  &amp;#39;v_1&amp;#39; = c(rnorm(3, 0, 1)),
  &amp;#39;v_2&amp;#39; = c(rnorm(3, 10, 5)),
  &amp;#39;v_3&amp;#39; = c(rnorm(3, 50, 10)),
  &amp;#39;v_4&amp;#39; = c(rnorm(3, 25, 15))
)

mini &amp;lt;- dish %&amp;gt;%
  select(person, v_1, v_2)

mini[mini$person == &amp;#39;jimmy&amp;#39;, 2] &amp;lt;- 99
mini[mini$person == &amp;#39;james&amp;#39;, 2] &amp;lt;- 99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The original data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   person         v_1       v_2      v_3      v_4
## 1  jimmy -0.05192334 14.323487 57.23598 34.88207
## 2  james  0.85977779  4.580808 48.94879 32.64684
## 3 johnny -0.94709073 12.253920 51.35526 10.04864&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we changed it to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   person        v_1       v_2
## 1  jimmy 99.0000000 14.323487
## 2  james 99.0000000  4.580808
## 3 johnny -0.9470907 12.253920&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the function equivalent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;impute_99 &amp;lt;- function(data){
  
  
  new_data &amp;lt;- data %&amp;gt;%
    select(person, v_1, v_2)
  
  new_data[new_data$person == &amp;#39;jimmy&amp;#39;, 2] &amp;lt;- 99
  new_data[new_data$person == &amp;#39;james&amp;#39;, 2] &amp;lt;- 99
  
  return(new_data)
  
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adjusted_data &amp;lt;- impute_99(dish)
adjusted_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   person        v_1       v_2
## 1  jimmy 99.0000000 14.323487
## 2  james 99.0000000  4.580808
## 3 johnny -0.9470907 12.253920&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function-with-column-names-as-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Function with column names as parameters&lt;/h1&gt;
&lt;p&gt;Now, what if we want to use specific column names as parameters in our function? We could change the function to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;impute_99_column_specific &amp;lt;- function(data, column1, column2){
  
  new_data &amp;lt;- data %&amp;gt;%
    select(person, column1, column2)
  
  new_data[new_data$person == &amp;#39;jimmy&amp;#39;, 2] &amp;lt;- 99 # column1 change
  new_data[new_data$person == &amp;#39;james&amp;#39;, 2] &amp;lt;- 99 # column2 change
  
  return(new_data)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where ‘column1’ and ‘column2’ can be replaced by specific names. Here is where I usually get confused, the following code does not work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cool_data &amp;lt;- impute_99_column_specific(dish, v_1, v_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fortunately the correction is simple, just put quotes around the column names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cool_data &amp;lt;- impute_99_column_specific(dish, &amp;#39;v_1&amp;#39;, &amp;#39;v_2&amp;#39;)
cool_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   person        v_1       v_2
## 1  jimmy 99.0000000 14.323487
## 2  james 99.0000000  4.580808
## 3 johnny -0.9470907 12.253920&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spline Modeling</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-05-05/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-05-05/</guid>
      <description>


&lt;p&gt;A few spline models (also known as piecewise models). As in previous posts, ‘affect’ is the name given to values of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; throughout.&lt;/p&gt;
&lt;div id=&#34;growth-and-even-more-growth&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1) Growth and Even More Growth&lt;/h1&gt;
&lt;p&gt;A model that captures a process that increases initially and then increases at an even greater rate once it reaches time point 5. The data generating process:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 
  \begin{cases}
  4 + 0.3t + error_{t}, &amp;amp; \text{if time &amp;lt; 5}\\
  8 + 0.9t + error_{t}, &amp;amp; \text{otherwise}
  \end{cases}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The data generating code and plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lavaan)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 10

intercept_1 &amp;lt;- 4
intercept_2 &amp;lt;- 8

growth1 &amp;lt;- 0.3
growth2 &amp;lt;- 0.9


df_matrix &amp;lt;- matrix(, ncol = 3, nrow = N*time)


count &amp;lt;- 0

for(i in 1:N){
  
  unob_het_y &amp;lt;- rnorm(1,0,1)
  
  
  for(j in 1:time){
    
    count &amp;lt;- count + 1
    
    if(j &amp;lt; 5){
    df_matrix[count, 1] &amp;lt;- i
    df_matrix[count, 2] &amp;lt;- j
    df_matrix[count, 3] &amp;lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)
    
    }else{
      
      df_matrix[count, 1] &amp;lt;- i
      df_matrix[count, 2] &amp;lt;- j
      df_matrix[count, 3] &amp;lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)
      
      
    }
  }
  
}

df &amp;lt;- data.frame(df_matrix)

names(df) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

df1 &amp;lt;- df %&amp;gt;%
  filter(time &amp;lt; 5)

df2 &amp;lt;- df %&amp;gt;%
  filter(time &amp;gt;= 5)

df_sum1 &amp;lt;- df1 %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

df_sum2 &amp;lt;- df2 %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

ggplot() + 
  geom_point(data = df1, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df1, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = df2, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df2, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df_sum1, aes(x = time, y = affect)) + 
  geom_line(data = df_sum2, aes(x = time, y = affect))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-05-05/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Estimating the parameters using SEM:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)

df_wide &amp;lt;- reshape(df, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


spline_string &amp;lt;- &amp;#39;

# latent intercept for first half

level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10

# latent intercept for second half

level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 1*affect.10

# latent slope for first half basis coefficients

slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10

# latent slope for second half basis coefficients

slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10

# means and variance of latent factors

level1_affect ~~ level1_affect
level2_affect ~~ level2_affect
slope1_affect ~~ slope1_affect
slope2_affect ~~ slope2_affect

# covariance between latent factors

level1_affect ~~ level2_affect
level1_affect ~~ slope1_affect
level1_affect ~~ slope2_affect

level2_affect ~~ slope1_affect
level2_affect ~~ slope2_affect

slope1_affect ~~ slope2_affect

# constrain means of indicators to zero across time

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0
affect.7 ~ 0
affect.8 ~ 0
affect.9 ~ 0
affect.10 ~ 0

# constrain residual variance to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6
affect.7 ~~ res_var*affect.7
affect.8 ~~ res_var*affect.8
affect.9 ~~ res_var*affect.9
affect.10 ~~ res_var*affect.10

&amp;#39;

spline_model &amp;lt;- growth(spline_string, data = df_wide)
summary(spline_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 78 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         24
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                46.543
##   Degrees of freedom                                50
##   P-value (Chi-square)                           0.613
## 
## Model Test Baseline Model:
## 
##   Test statistic                              2013.966
##   Degrees of freedom                                45
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.002
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -6210.676
##   Loglikelihood unrestricted model (H1)      -6187.405
##                                                       
##   Akaike (AIC)                               12451.352
##   Bayesian (BIC)                             12511.224
##   Sample-size adjusted Bayesian (BIC)        12463.628
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent confidence interval - lower         0.000
##   90 Percent confidence interval - upper         0.028
##   P-value RMSEA &amp;lt;= 0.05                          1.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.039
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          1.000                           
##     affect.3          1.000                           
##     affect.4          1.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##   level2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          1.000                           
##     affect.6          1.000                           
##     affect.7          1.000                           
##     affect.8          1.000                           
##     affect.9          1.000                           
##     affect.10         1.000                           
##   slope1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          2.000                           
##     affect.3          3.000                           
##     affect.4          4.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##   slope2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          5.000                           
##     affect.6          6.000                           
##     affect.7          7.000                           
##     affect.8          8.000                           
##     affect.9          9.000                           
##     affect.10        10.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect ~~                                    
##     level2_affect     1.174    0.186    6.315    0.000
##     slope1_affect     0.011    0.046    0.232    0.817
##     slope2_affect    -0.006    0.020   -0.326    0.744
##   level2_affect ~~                                    
##     slope1_affect     0.014    0.050    0.282    0.778
##     slope2_affect    -0.039    0.039   -1.012    0.312
##   slope1_affect ~~                                    
##     slope2_affect    -0.003    0.006   -0.573    0.567
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##    .affect.7          0.000                           
##    .affect.8          0.000                           
##    .affect.9          0.000                           
##    .affect.10         0.000                           
##     level1_affect     3.951    0.079   49.895    0.000
##     level2_affect     7.962    0.111   71.470    0.000
##     slope1_affect     0.307    0.022   13.780    0.000
##     slope2_affect     0.898    0.012   71.924    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     lvl1_ff           0.987    0.183    5.404    0.000
##     lvl2_ff           1.536    0.365    4.212    0.000
##     slp1_ff          -0.005    0.015   -0.317    0.752
##     slp2_ff           0.004    0.005    0.945    0.345
##    .affct.1 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.2 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.3 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.4 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.5 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.6 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.7 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.8 (rs_v)    1.014    0.029   34.641    0.000
##    .affct.9 (rs_v)    1.014    0.029   34.641    0.000
##    .affc.10 (rs_v)    1.014    0.029   34.641    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the basis coefficients is the important piece that allows us to capture the change in slope:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;#39;
# latent slope for first half basis coefficients

slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10

# latent slope for second half basis coefficients

slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10

&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;growth-and-negative-growth&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2) Growth and Negative Growth&lt;/h1&gt;
&lt;p&gt;A model that captures a process that goes up and then goes down. The data generating process:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 
  \begin{cases}
  4 + 0.5t + error_{t}, &amp;amp; \text{if time &amp;lt; 5}\\
  4 - 0.5t + error_{t}, &amp;amp; \text{otherwise}
  \end{cases}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The data generating code and plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lavaan)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 10

intercept_1 &amp;lt;- 4
intercept_2 &amp;lt;- 4

growth1 &amp;lt;- 0.8
growth2 &amp;lt;- -0.8

df_matrix_b &amp;lt;- matrix(, ncol = 3, nrow = N*time)


count &amp;lt;- 0

for(i in 1:N){
  
  unob_het_y &amp;lt;- rnorm(1,0,1)
  
  
  for(j in 1:time){
    
    count &amp;lt;- count + 1
    
    if(j &amp;lt; 5){
      df_matrix_b[count, 1] &amp;lt;- i
      df_matrix_b[count, 2] &amp;lt;- j
      df_matrix_b[count, 3] &amp;lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)
      
    }else{
      
      df_matrix_b[count, 1] &amp;lt;- i
      df_matrix_b[count, 2] &amp;lt;- j
      df_matrix_b[count, 3] &amp;lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)
      
      
    }
  }
  
}

df_b &amp;lt;- data.frame(df_matrix_b)

names(df_b) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

df1_b &amp;lt;- df_b %&amp;gt;%
  filter(time &amp;lt; 5)

df2_b &amp;lt;- df_b %&amp;gt;%
  filter(time &amp;gt;= 5)

df_sum1_b &amp;lt;- df1_b %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

df_sum2_b &amp;lt;- df2_b %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

ggplot() + 
  geom_point(data = df1_b, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df1_b, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = df2_b, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df2_b, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df_sum1_b, aes(x = time, y = affect)) + 
  geom_line(data = df_sum2_b, aes(x = time, y = affect))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-05-05/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Estimating the parameters using SEM:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)


df_wide_b &amp;lt;- reshape(df_b, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


spline_string_b &amp;lt;- &amp;#39;

# latent intercept for first half

level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10

# latent intercept for second half

level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 1*affect.10

# latent slope for first half basis coefficients

slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10

# latent slope for second half basis coefficients

slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 10*affect.10

# means and variance of latent factors

level1_affect ~~ level1_affect
level2_affect ~~ level2_affect
slope1_affect ~~ slope1_affect
slope2_affect ~~ slope2_affect

# covariance between latent factors

level1_affect ~~ level2_affect
level1_affect ~~ slope1_affect
level1_affect ~~ slope2_affect

level2_affect ~~ slope1_affect
level2_affect ~~ slope2_affect

slope1_affect ~~ slope2_affect

# constrain means of indicators to zero across time

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0
affect.7 ~ 0
affect.8 ~ 0
affect.9 ~ 0
affect.10 ~ 0

# constrain residual variance to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6
affect.7 ~~ res_var*affect.7
affect.8 ~~ res_var*affect.8
affect.9 ~~ res_var*affect.9
affect.10 ~~ res_var*affect.10

&amp;#39;

spline_model_b &amp;lt;- growth(spline_string_b, data = df_wide_b)
summary(spline_model_b, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 88 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         24
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                50.005
##   Degrees of freedom                                50
##   P-value (Chi-square)                           0.473
## 
## Model Test Baseline Model:
## 
##   Test statistic                              1843.189
##   Degrees of freedom                                45
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.000
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -6219.629
##   Loglikelihood unrestricted model (H1)      -6194.626
##                                                       
##   Akaike (AIC)                               12469.257
##   Bayesian (BIC)                             12529.129
##   Sample-size adjusted Bayesian (BIC)        12481.533
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent confidence interval - lower         0.000
##   90 Percent confidence interval - upper         0.032
##   P-value RMSEA &amp;lt;= 0.05                          1.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.030
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          1.000                           
##     affect.3          1.000                           
##     affect.4          1.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##   level2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          1.000                           
##     affect.6          1.000                           
##     affect.7          1.000                           
##     affect.8          1.000                           
##     affect.9          1.000                           
##     affect.10         1.000                           
##   slope1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          2.000                           
##     affect.3          3.000                           
##     affect.4          4.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##   slope2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          5.000                           
##     affect.6          6.000                           
##     affect.7          7.000                           
##     affect.8          8.000                           
##     affect.9          9.000                           
##     affect.10        10.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect ~~                                    
##     level2_affect     1.204    0.172    6.981    0.000
##     slope1_affect     0.101    0.043    2.372    0.018
##     slope2_affect    -0.027    0.018   -1.488    0.137
##   level2_affect ~~                                    
##     slope1_affect    -0.015    0.045   -0.342    0.733
##     slope2_affect     0.038    0.035    1.091    0.275
##   slope1_affect ~~                                    
##     slope2_affect     0.004    0.005    0.776    0.438
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##    .affect.7          0.000                           
##    .affect.8          0.000                           
##    .affect.9          0.000                           
##    .affect.10         0.000                           
##     level1_affect     3.820    0.077   49.653    0.000
##     level2_affect     4.102    0.105   39.064    0.000
##     slope1_affect     0.847    0.021   39.473    0.000
##     slope2_affect    -0.819    0.012  -69.844    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     lvl1_ff           0.752    0.174    4.326    0.000
##     lvl2_ff           0.769    0.329    2.338    0.019
##     slp1_ff          -0.031    0.014   -2.150    0.032
##     slp2_ff          -0.007    0.004   -1.542    0.123
##    .affct.1 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.2 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.3 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.4 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.5 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.6 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.7 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.8 (rs_v)    1.077    0.031   34.641    0.000
##    .affct.9 (rs_v)    1.077    0.031   34.641    0.000
##    .affc.10 (rs_v)    1.077    0.031   34.641    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the string syntax is the exact same because the process changes at the same point in time, it does not matter if the process changes to ‘more positive’ or ‘more negative.’&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-growth-growth-and-negative-growth&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3) Negative Growth, Growth, and Negative Growth&lt;/h1&gt;
&lt;p&gt;Now a process that goes down, goes up, and then goes back down. The data generating process:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 
  \begin{cases}
  4 - 0.5t + error_{t}, &amp;amp; \text{if time &amp;lt; 5}\\
  4 + 0.5t + error_{t}, &amp;amp; \text{if 5 &amp;lt; time &amp;lt; 10}\\
  4 - 0.5t + error_{t}, &amp;amp; \text{otherwise}
  \end{cases}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The data generating code and plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lavaan)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 15

intercept_1 &amp;lt;- 4
intercept_2 &amp;lt;- 4
intercept_3 &amp;lt;- 4

growth1 &amp;lt;- -0.5
growth2 &amp;lt;- 0.5
growth3 &amp;lt;- -0.5


df_matrix_c &amp;lt;- matrix(, ncol = 3, nrow = N*time)


count &amp;lt;- 0

for(i in 1:N){
  
  unob_het_y &amp;lt;- rnorm(1,0,1)
  
  
  for(j in 1:time){
    
    count &amp;lt;- count + 1
    
    if(j &amp;lt; 5){
      df_matrix_c[count, 1] &amp;lt;- i
      df_matrix_c[count, 2] &amp;lt;- j
      df_matrix_c[count, 3] &amp;lt;- intercept_1 + growth1*j + unob_het_y + rnorm(1,0,1)
      
    }else if(j &amp;gt;= 5 &amp;amp;&amp;amp; j &amp;lt; 10){
      
      df_matrix_c[count, 1] &amp;lt;- i
      df_matrix_c[count, 2] &amp;lt;- j
      df_matrix_c[count, 3] &amp;lt;- intercept_2 + growth2*j + unob_het_y + rnorm(1,0,1)
      
      
    }else{
      
      df_matrix_c[count, 1] &amp;lt;- i
      df_matrix_c[count, 2] &amp;lt;- j
      df_matrix_c[count, 3] &amp;lt;- intercept_3 + growth3*j + unob_het_y + rnorm(1,0,1)
      
    }
  }
  
}

df_c &amp;lt;- data.frame(df_matrix_c)

names(df_c) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

df1_c &amp;lt;- df_c %&amp;gt;%
  filter(time &amp;lt; 5)

df2_c &amp;lt;- df_c %&amp;gt;%
  filter(time &amp;gt;= 5 &amp;amp; time &amp;lt; 10)

df3_c &amp;lt;- df_c %&amp;gt;%
  filter(time &amp;gt;= 10)

df_sum1_c &amp;lt;- df1_c %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

df_sum2_c &amp;lt;- df2_c %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

df_sum3_c &amp;lt;- df3_c %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarise(
    affect = mean(affect)
  )

ggplot() + 
  geom_point(data = df1_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df1_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = df2_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df2_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df_sum1_c, aes(x = time, y = affect)) + 
  geom_line(data = df_sum2_c, aes(x = time, y = affect)) + 
  geom_point(data = df3_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df3_c, aes(x = time, y = affect, group = id), color = &amp;#39;gray85&amp;#39;) + 
  geom_line(data = df_sum3_c, aes(x = time, y = affect))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-05-05/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now estimate the parameters using SEM:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)

df_wide_c &amp;lt;- reshape(df_c, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


spline_string_c &amp;lt;- &amp;#39;

# latent intercept for first third

level1_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15

# latent intercept for second third

level2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 1*affect.5 + 1*affect.6 + 1*affect.7 + 1*affect.8 + 1*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15

# latent intercept for final third

level3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 1*affect.10 + 1*affect.11 + 1*affect.12 + 1*affect.13 + 1*affect.14 + 1*affect.15


# latent slope for first third basis coefficients

slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15

# latent slope for second third basis coefficients

slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 0*affect.13 + 0*affect.14 + 0*affect.15

# latent slope for final third basis coefficients

slope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + 13*affect.13 + 14*affect.14 + 15*affect.15



# means and variance of latent factors

level1_affect ~~ level1_affect
level2_affect ~~ level2_affect
level3_affect ~~ level3_affect
slope1_affect ~~ slope1_affect
slope2_affect ~~ slope2_affect
slope3_affect ~~ slope3_affect

# covariance between latent factors

level1_affect ~~ level2_affect
level1_affect ~~ level3_affect
level1_affect ~~ slope1_affect
level1_affect ~~ slope2_affect
level1_affect ~~ slope3_affect

level2_affect ~~ level3_affect
level2_affect ~~ slope1_affect
level2_affect ~~ slope2_affect
level2_affect ~~ slope3_affect

level3_affect ~~ slope1_affect
level3_affect ~~ slope2_affect
level3_affect ~~ slope3_affect

slope1_affect ~~ slope2_affect
slope1_affect ~~ slope3_affect

slope2_affect ~~ slope3_affect

# constrain means of indicators to zero across time

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0
affect.7 ~ 0
affect.8 ~ 0
affect.9 ~ 0
affect.10 ~ 0

# constrain residual variance to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6
affect.7 ~~ res_var*affect.7
affect.8 ~~ res_var*affect.8
affect.9 ~~ res_var*affect.9
affect.10 ~~ res_var*affect.10

&amp;#39;

spline_model_c &amp;lt;- growth(spline_string_c, data = df_wide_c)
summary(spline_model_c, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 156 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         42
##   Number of equality constraints                     9
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                90.139
##   Degrees of freedom                               102
##   P-value (Chi-square)                           0.793
## 
## Model Test Baseline Model:
## 
##   Test statistic                              3303.179
##   Degrees of freedom                               105
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.004
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -9108.717
##   Loglikelihood unrestricted model (H1)      -9063.647
##                                                       
##   Akaike (AIC)                               18283.433
##   Bayesian (BIC)                             18415.151
##   Sample-size adjusted Bayesian (BIC)        18310.440
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent confidence interval - lower         0.000
##   90 Percent confidence interval - upper         0.018
##   P-value RMSEA &amp;lt;= 0.05                          1.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.032
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          1.000                           
##     affect.3          1.000                           
##     affect.4          1.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##     affect.11         0.000                           
##     affect.12         0.000                           
##     affect.13         0.000                           
##     affect.14         0.000                           
##     affect.15         0.000                           
##   level2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          1.000                           
##     affect.6          1.000                           
##     affect.7          1.000                           
##     affect.8          1.000                           
##     affect.9          1.000                           
##     affect.10         0.000                           
##     affect.11         0.000                           
##     affect.12         0.000                           
##     affect.13         0.000                           
##     affect.14         0.000                           
##     affect.15         0.000                           
##   level3_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         1.000                           
##     affect.11         1.000                           
##     affect.12         1.000                           
##     affect.13         1.000                           
##     affect.14         1.000                           
##     affect.15         1.000                           
##   slope1_affect =~                                    
##     affect.1          1.000                           
##     affect.2          2.000                           
##     affect.3          3.000                           
##     affect.4          4.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10         0.000                           
##     affect.11         0.000                           
##     affect.12         0.000                           
##     affect.13         0.000                           
##     affect.14         0.000                           
##     affect.15         0.000                           
##   slope2_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          5.000                           
##     affect.6          6.000                           
##     affect.7          7.000                           
##     affect.8          8.000                           
##     affect.9          9.000                           
##     affect.10         0.000                           
##     affect.11         0.000                           
##     affect.12         0.000                           
##     affect.13         0.000                           
##     affect.14         0.000                           
##     affect.15         0.000                           
##   slope3_affect =~                                    
##     affect.1          0.000                           
##     affect.2          0.000                           
##     affect.3          0.000                           
##     affect.4          0.000                           
##     affect.5          0.000                           
##     affect.6          0.000                           
##     affect.7          0.000                           
##     affect.8          0.000                           
##     affect.9          0.000                           
##     affect.10        10.000                           
##     affect.11        11.000                           
##     affect.12        12.000                           
##     affect.13        13.000                           
##     affect.14        14.000                           
##     affect.15        15.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   level1_affect ~~                                    
##     level2_affect     1.063    0.199    5.340    0.000
##     level3_affect     0.782    0.262    2.987    0.003
##     slope1_affect    -0.034    0.049   -0.701    0.483
##     slope2_affect    -0.009    0.024   -0.369    0.712
##     slope3_affect     0.018    0.019    0.953    0.341
##   level2_affect ~~                                    
##     level3_affect     0.655    0.383    1.710    0.087
##     slope1_affect     0.045    0.056    0.803    0.422
##     slope2_affect     0.104    0.051    2.041    0.041
##     slope3_affect     0.049    0.028    1.732    0.083
##   level3_affect ~~                                    
##     slope1_affect     0.069    0.075    0.911    0.362
##     slope2_affect     0.065    0.047    1.375    0.169
##     slope3_affect     0.013    0.058    0.223    0.824
##   slope1_affect ~~                                    
##     slope2_affect    -0.006    0.007   -0.930    0.353
##     slope3_affect    -0.004    0.006   -0.725    0.468
##   slope2_affect ~~                                    
##     slope3_affect    -0.007    0.004   -2.005    0.045
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##    .affect.7          0.000                           
##    .affect.8          0.000                           
##    .affect.9          0.000                           
##    .affect.10         0.000                           
##    .affect.11         0.000                           
##    .affect.12         0.000                           
##    .affect.13         0.000                           
##    .affect.14         0.000                           
##    .affect.15         0.000                           
##     level1_affect     4.030    0.081   49.969    0.000
##     level2_affect     3.805    0.119   31.980    0.000
##     level3_affect     3.976    0.161   24.768    0.000
##     slope1_affect    -0.500    0.023  -21.345    0.000
##     slope2_affect     0.533    0.015   36.391    0.000
##     slope3_affect    -0.494    0.012  -41.505    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     lvl1_ff           1.060    0.190    5.589    0.000
##     lvl2_ff           0.417    0.430    0.971    0.331
##     lvl3_ff           1.057    0.791    1.336    0.181
##     slp1_ff           0.014    0.017    0.813    0.416
##     slp2_ff          -0.017    0.007   -2.511    0.012
##     slp3_ff          -0.001    0.005   -0.300    0.764
##    .affct.1 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.2 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.3 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.4 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.5 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.6 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.7 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.8 (rs_v)    1.028    0.031   33.596    0.000
##    .affct.9 (rs_v)    1.028    0.031   33.596    0.000
##    .affc.10 (rs_v)    1.028    0.031   33.596    0.000
##    .affc.11           1.005    0.079   12.669    0.000
##    .affc.12           0.994    0.076   13.016    0.000
##    .affc.13           1.006    0.077   13.128    0.000
##    .affc.14           1.015    0.079   12.850    0.000
##    .affc.15           1.012    0.087   11.652    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the basis coefficients are the important piece here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;#39;


# latent slope for first third basis coefficients

slope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + 
                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 
                 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 
                 0*affect.13 + 0*affect.14 + 0*affect.15

# latent slope for second third basis coefficients

slope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + 
                 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + 
                 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + 
                 0*affect.13 + 0*affect.14 + 0*affect.15

# latent slope for final third basis coefficients

slope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 +
                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + 
                 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + 
                 13*affect.13 + 14*affect.14 + 15*affect.15



&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;\n\n\n# latent slope for first third basis coefficients\n\nslope1_affect =~ 1*affect.1 + 2*affect.2 + 3*affect.3 + 4*affect.4 + \n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \n                 0*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \n                 0*affect.13 + 0*affect.14 + 0*affect.15\n\n# latent slope for second third basis coefficients\n\nslope2_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 + \n                 5*affect.5 + 6*affect.6 + 7*affect.7 + 8*affect.8 + \n                 9*affect.9 + 0*affect.10 + 0*affect.11 + 0*affect.12 + \n                 0*affect.13 + 0*affect.14 + 0*affect.15\n\n# latent slope for final third basis coefficients\n\nslope3_affect =~ 0*affect.1 + 0*affect.2 + 0*affect.3 + 0*affect.4 +\n                 0*affect.5 + 0*affect.6 + 0*affect.7 + 0*affect.8 + \n                 0*affect.9 + 10*affect.10 + 11*affect.11 + 12*affect.12 + \n                 13*affect.13 + 14*affect.14 + 15*affect.15\n\n\n\n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latent Growth Curves</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-04-15/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-04-15/</guid>
      <description>


&lt;div id=&#34;latent-growth-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent Growth Curves&lt;/h1&gt;
&lt;p&gt;I will progress through three models: linear, quadratic growth, and latent basis. In every example I use a sample of 400, 6 time points, and ‘affect’ as the variable of interest.&lt;/p&gt;
&lt;p&gt;Don’t forget that multiplying by time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.6t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;is different from describing over time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.6_t\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;linear&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1) Linear&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The data generating process:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 4 - 0.6t + e_{t}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 6

intercept &amp;lt;- 4
linear_growth &amp;lt;- -0.6

df_matrix &amp;lt;- matrix(, nrow = N*time, ncol = 3)

count &amp;lt;- 0

for(i in 1:400){
  
  unob_het_affect &amp;lt;- rnorm(1,0,3)

  
  for(j in 1:6){
    
    count &amp;lt;- count + 1
    
    if(j == 1){
      
      df_matrix[count, 1] &amp;lt;- i
      df_matrix[count, 2] &amp;lt;- j
      df_matrix[count, 3] &amp;lt;- intercept + unob_het_affect + rnorm(1,0,1)
    }else{
      
      
      df_matrix[count, 1] &amp;lt;- i
      df_matrix[count, 2] &amp;lt;- j
      df_matrix[count, 3] &amp;lt;- intercept + linear_growth*j + unob_het_affect + rnorm(1,0,1)
      
    }
    
    
    
  }
  
  
}

df &amp;lt;- data.frame(df_matrix)
names(df) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

random_ids &amp;lt;- sample(df$id, 5)

random_df &amp;lt;- df %&amp;gt;%
  filter(id %in% random_ids)
  

ggplot(df, aes(x = time, y = affect, group = id)) + 
  geom_point(color = &amp;#39;gray85&amp;#39;) + 
  geom_line(color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = random_df, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;) + 
  geom_line(data = random_df, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-04-15/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimating the model:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Formatting the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide &amp;lt;- reshape(df, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, an intercept only (no change) model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)

no_change_string &amp;lt;- &amp;#39;

# Latent intercept factor

intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6

# Mean and variance of latent intercept factor

intercept_affect ~~ intercept_affect

# Fix observed variable means to 0

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0

# Constrain residual (error) variance of observed variables to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6


&amp;#39;

no_change_model &amp;lt;- growth(no_change_string, data = df_wide)
summary(no_change_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 19 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          8
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                              1961.248
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              3884.766
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.499
##   Tucker-Lewis Index (TLI)                       0.687
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -5184.361
##   Loglikelihood unrestricted model (H1)      -4203.737
##                                                       
##   Akaike (AIC)                               10374.721
##   Bayesian (BIC)                             10386.696
##   Sample-size adjusted Bayesian (BIC)        10377.176
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.449
##   90 Percent confidence interval - lower         0.432
##   90 Percent confidence interval - upper         0.466
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.192
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect =~                                    
##     affect.1             1.000                           
##     affect.2             1.000                           
##     affect.3             1.000                           
##     affect.4             1.000                           
##     affect.5             1.000                           
##     affect.6             1.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##     intercept_ffct    1.834    0.150   12.244    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     intrcp_           8.535    0.635   13.438    0.000
##    .affct.1 (rs_v)    2.669    0.084   31.623    0.000
##    .affct.2 (rs_v)    2.669    0.084   31.623    0.000
##    .affct.3 (rs_v)    2.669    0.084   31.623    0.000
##    .affct.4 (rs_v)    2.669    0.084   31.623    0.000
##    .affct.5 (rs_v)    2.669    0.084   31.623    0.000
##    .affct.6 (rs_v)    2.669    0.084   31.623    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a linear growth model centered at time point 1. The intercept factor estimate, therefore, is the estimated average affect at time 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)

linear_change_string &amp;lt;- &amp;#39;

# Latent intercept and slope factors

intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6
slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 + 4*affect.5 + 5*affect.6

# Mean and variance of latent factors

intercept_affect ~~ intercept_affect
slope_affect ~~ slope_affect

# Covariance between latent factors

intercept_affect ~~ slope_affect

# Fix observed variable means to 0

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0

# Constrain residual (error) variance of observed variables to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6


&amp;#39;

linear_change_model &amp;lt;- growth(linear_change_string, data = df_wide)
summary(linear_change_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 36 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         11
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                90.124
##   Degrees of freedom                                21
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              3884.766
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.982
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -4248.799
##   Loglikelihood unrestricted model (H1)      -4203.737
##                                                       
##   Akaike (AIC)                                8509.598
##   Bayesian (BIC)                              8533.547
##   Sample-size adjusted Bayesian (BIC)         8514.508
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.091
##   90 Percent confidence interval - lower         0.072
##   90 Percent confidence interval - upper         0.110
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.036
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect =~                                    
##     affect.1             1.000                           
##     affect.2             1.000                           
##     affect.3             1.000                           
##     affect.4             1.000                           
##     affect.5             1.000                           
##     affect.6             1.000                           
##   slope_affect =~                                        
##     affect.1             0.000                           
##     affect.2             1.000                           
##     affect.3             2.000                           
##     affect.4             3.000                           
##     affect.5             4.000                           
##     affect.6             5.000                           
## 
## Covariances:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect ~~                                    
##     slope_affect         0.072    0.037    1.976    0.048
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##     intercept_ffct    3.535    0.150   23.546    0.000
##     slope_affect     -0.680    0.012  -56.769    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     intrcp_           8.461    0.638   13.265    0.000
##     slp_ffc          -0.003    0.005   -0.683    0.495
##    .affct.1 (rs_v)    1.060    0.037   28.284    0.000
##    .affct.2 (rs_v)    1.060    0.037   28.284    0.000
##    .affct.3 (rs_v)    1.060    0.037   28.284    0.000
##    .affct.4 (rs_v)    1.060    0.037   28.284    0.000
##    .affct.5 (rs_v)    1.060    0.037   28.284    0.000
##    .affct.6 (rs_v)    1.060    0.037   28.284    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(linear_change_model, &amp;#39;cov.lv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  intrc_ slp_ff
## intercept_affect  8.461       
## slope_affect      0.072 -0.003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model does an adequate job recovering the intercept and slope parameters.&lt;/p&gt;
&lt;p&gt;If I wanted to center the model at time point 3 the latent intercept term would be interpreted as the estimated average affect at time 3 and the syntax would change to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;#39;
slope_affect =~ -2*affect.1 + -1*affect.2 + 0*affect.3 + 1*affect.4 + 2*affect.5 + 3*affect.6

&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2) Quadratic&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;The data generating process:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 4 + 0.2t + 0.7t^2 + e_{t}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 6



intercept_mu &amp;lt;- 4
linear_growth2 &amp;lt;- 0.2
quad_growth &amp;lt;- 0.7

df_matrix2 &amp;lt;- matrix(, nrow = N*time, ncol = 3)

count &amp;lt;- 0

for(i in 1:400){
  
  unob_het_affect &amp;lt;- rnorm(1,0,3)

  
  for(j in 1:6){
    
    count &amp;lt;- count + 1
    
    if(j == 1){
      
      df_matrix2[count, 1] &amp;lt;- i
      df_matrix2[count, 2] &amp;lt;- j
      df_matrix2[count, 3] &amp;lt;- intercept + rnorm(1,0,1) + rnorm(1,0,1)
    }else{
      
      
      df_matrix2[count, 1] &amp;lt;- i
      df_matrix2[count, 2] &amp;lt;- j
      df_matrix2[count, 3] &amp;lt;- intercept + linear_growth2*j + quad_growth*(j^2) + unob_het_affect + rnorm(1,0,1)
      
    }
    
    
    
  }
  
  
}

df2 &amp;lt;- data.frame(df_matrix2)
names(df2) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

random_ids2 &amp;lt;- sample(df2$id, 5)

random_df2 &amp;lt;- df2 %&amp;gt;%
  filter(id %in% random_ids2)
  

ggplot(df2, aes(x = time, y = affect, group = id)) + 
  geom_point(color = &amp;#39;gray85&amp;#39;) + 
  geom_line(color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = random_df2, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;) + 
  geom_line(data = random_df2, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;) + 
  theme_wsj()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-04-15/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimating the model:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Quadratic growth model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide2 &amp;lt;- reshape(df2, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


library(lavaan)

quad_change_string &amp;lt;- &amp;#39;

# Latent intercept, linear slope, and quad slope factors

intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6
slope_affect =~ 0*affect.1 + 1*affect.2 + 2*affect.3 + 3*affect.4 + 4*affect.5 + 5*affect.6
quad_slope_affect =~ 0*affect.1 + 1*affect.2 + 4*affect.3 + 9*affect.4 + 16*affect.5 + 25*affect.6

# Mean and variance of latent factors

intercept_affect ~~ intercept_affect
slope_affect ~~ slope_affect
quad_slope_affect ~~ quad_slope_affect

# Covariance between latent factors

intercept_affect ~~ slope_affect
intercept_affect ~~ quad_slope_affect
slope_affect ~~ quad_slope_affect

# Fix observed variable means to 0

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0

# Constrain residual (error) variance of observed variables to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6


&amp;#39;

quad_change_model &amp;lt;- growth(quad_change_string, data = df_wide2)
summary(quad_change_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 91 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         15
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                               629.047
##   Degrees of freedom                                17
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              3252.120
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.811
##   Tucker-Lewis Index (TLI)                       0.833
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -4613.101
##   Loglikelihood unrestricted model (H1)      -4298.577
##                                                       
##   Akaike (AIC)                                9246.202
##   Bayesian (BIC)                              9286.116
##   Sample-size adjusted Bayesian (BIC)         9254.386
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.300
##   90 Percent confidence interval - lower         0.280
##   90 Percent confidence interval - upper         0.320
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.237
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                        Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect =~                                     
##     affect.1              1.000                           
##     affect.2              1.000                           
##     affect.3              1.000                           
##     affect.4              1.000                           
##     affect.5              1.000                           
##     affect.6              1.000                           
##   slope_affect =~                                         
##     affect.1              0.000                           
##     affect.2              1.000                           
##     affect.3              2.000                           
##     affect.4              3.000                           
##     affect.5              4.000                           
##     affect.6              5.000                           
##   quad_slope_affect =~                                    
##     affect.1              0.000                           
##     affect.2              1.000                           
##     affect.3              4.000                           
##     affect.4              9.000                           
##     affect.5             16.000                           
##     affect.6             25.000                           
## 
## Covariances:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect ~~                                    
##     slope_affect         0.880    0.150    5.855    0.000
##     quad_slop_ffct      -0.137    0.024   -5.747    0.000
##   slope_affect ~~                                        
##     quad_slop_ffct      -0.516    0.054   -9.627    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##     intercept_ffct    4.109    0.067   61.634    0.000
##     slope_affect      2.109    0.108   19.450    0.000
##     quad_slop_ffct    0.624    0.017   36.005    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     intrcp_           0.388    0.138    2.816    0.005
##     slp_ffc           3.473    0.336   10.328    0.000
##     qd_slp_           0.075    0.009    8.599    0.000
##    .affct.1 (rs_v)    1.691    0.069   24.495    0.000
##    .affct.2 (rs_v)    1.691    0.069   24.495    0.000
##    .affct.3 (rs_v)    1.691    0.069   24.495    0.000
##    .affct.4 (rs_v)    1.691    0.069   24.495    0.000
##    .affct.5 (rs_v)    1.691    0.069   24.495    0.000
##    .affct.6 (rs_v)    1.691    0.069   24.495    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model recovers the intercept and quadratic parameters but not the linear growth parameter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-basis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3) Latent Basis&lt;/h3&gt;
&lt;p&gt;This model allows us to see where a majority of the change occurs in the process. For example, does more change occur between time points 2 and 3 or 5 and 6? In this model we are not trying to recover the parameters, but describe the change process in detail.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Data generating process:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Time 1 - Time 3:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 4 + 0.2t + e_{t}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Time 4 - Time 6:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{it} = 4 + 0.8t + e_{t}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)
library(MASS)

N &amp;lt;- 400
time &amp;lt;- 6


intercept_mu &amp;lt;- 4
growth_1 &amp;lt;- 0.2
growth_2 &amp;lt;- 0.8


df_matrix3 &amp;lt;- matrix(, nrow = N*time, ncol = 3)

count &amp;lt;- 0

for(i in 1:400){
  
  unob_het_affect &amp;lt;- rnorm(1,0,3)
  
  
  for(j in 1:6){
    
    count &amp;lt;- count + 1
    
    if(j &amp;lt; 4){
      
      df_matrix3[count, 1] &amp;lt;- i
      df_matrix3[count, 2] &amp;lt;- j
      df_matrix3[count, 3] &amp;lt;- intercept + growth_1*j + unob_het_affect + rnorm(1,0,1)
      
    }else{
      
      
      df_matrix3[count, 1] &amp;lt;- i
      df_matrix3[count, 2] &amp;lt;- j
      df_matrix3[count, 3] &amp;lt;- intercept + growth_2*j + unob_het_affect + rnorm(1,0,1)
      
    }
    
    
    
  }
  
  
}

df3 &amp;lt;- data.frame(df_matrix3)
names(df3) &amp;lt;- c(&amp;#39;id&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;affect&amp;#39;)

random_ids3 &amp;lt;- sample(df3$id, 5)

random_df3 &amp;lt;- df3 %&amp;gt;%
  filter(id %in% random_ids3)
  

ggplot(df3, aes(x = time, y = affect, group = id)) + 
  geom_point(color = &amp;#39;gray85&amp;#39;) + 
  geom_line(color = &amp;#39;gray85&amp;#39;) + 
  geom_point(data = random_df3, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;) + 
  geom_line(data = random_df3, aes(x = time, y = affect, group = id), color = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-04-15/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimating the model:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Latent basis:&lt;/p&gt;
&lt;p&gt;Similar to a linear growth model but we freely estimate the intermediate basis coefficients. Remember to constrain the first basis coefficient to zero and the last to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_wide3 &amp;lt;- reshape(df3, idvar = &amp;#39;id&amp;#39;, timevar = &amp;#39;time&amp;#39;, direction = &amp;#39;wide&amp;#39;)


library(lavaan)

lb_string &amp;lt;- &amp;#39;

# Latent intercept and slope terms with intermediate time points freely estimated

intercept_affect =~ 1*affect.1 + 1*affect.2 + 1*affect.3 + 1*affect.4 + 1*affect.5 + 1*affect.6
slope_affect =~ 0*affect.1 + bc1*affect.2 + bc2*affect.3 + bc3*affect.4 + bc4*affect.5 + 1*affect.6

# Mean and variance of latent factors

intercept_affect ~~ intercept_affect
slope_affect ~~ slope_affect

# Covariance between latent factors

intercept_affect ~~ slope_affect

# Fix observed variable means to 0

affect.1 ~ 0
affect.2 ~ 0
affect.3 ~ 0
affect.4 ~ 0
affect.5 ~ 0
affect.6 ~ 0

# Constrain residual (error) variance of observed variables to equality across time

affect.1 ~~ res_var*affect.1
affect.2 ~~ res_var*affect.2
affect.3 ~~ res_var*affect.3
affect.4 ~~ res_var*affect.4
affect.5 ~~ res_var*affect.5
affect.6 ~~ res_var*affect.6


&amp;#39;

lb_model &amp;lt;- growth(lb_string, data = df_wide3)
summary(lb_model, fit.measures = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-6 ended normally after 64 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         15
##   Number of equality constraints                     5
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                17.419
##   Degrees of freedom                                17
##   P-value (Chi-square)                           0.426
## 
## Model Test Baseline Model:
## 
##   Test statistic                              3864.063
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.000
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -4225.520
##   Loglikelihood unrestricted model (H1)      -4216.810
##                                                       
##   Akaike (AIC)                                8471.040
##   Bayesian (BIC)                              8510.955
##   Sample-size adjusted Bayesian (BIC)         8479.224
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.008
##   90 Percent confidence interval - lower         0.000
##   90 Percent confidence interval - upper         0.046
##   P-value RMSEA &amp;lt;= 0.05                          0.970
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.021
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect =~                                    
##     affect.1             1.000                           
##     affect.2             1.000                           
##     affect.3             1.000                           
##     affect.4             1.000                           
##     affect.5             1.000                           
##     affect.6             1.000                           
##   slope_affect =~                                        
##     affect.1             0.000                           
##     affect.2 (bc1)       0.016    0.016    1.014    0.311
##     affect.3 (bc2)       0.075    0.015    4.936    0.000
##     affect.4 (bc3)       0.643    0.014   46.582    0.000
##     affect.5 (bc4)       0.807    0.014   55.866    0.000
##     affect.6             1.000                           
## 
## Covariances:
##                       Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   intercept_affect ~~                                    
##     slope_affect         0.111    0.151    0.739    0.460
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .affect.1          0.000                           
##    .affect.2          0.000                           
##    .affect.3          0.000                           
##    .affect.4          0.000                           
##    .affect.5          0.000                           
##    .affect.6          0.000                           
##     intercept_ffct    4.456    0.156   28.509    0.000
##     slope_affect      4.578    0.071   64.645    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     intrcp_           8.735    0.643   13.580    0.000
##     slp_ffc          -0.068    0.078   -0.864    0.388
##    .affct.1 (rs_v)    1.037    0.037   28.284    0.000
##    .affct.2 (rs_v)    1.037    0.037   28.284    0.000
##    .affct.3 (rs_v)    1.037    0.037   28.284    0.000
##    .affct.4 (rs_v)    1.037    0.037   28.284    0.000
##    .affct.5 (rs_v)    1.037    0.037   28.284    0.000
##    .affct.6 (rs_v)    1.037    0.037   28.284    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;bc1&lt;/code&gt; represents the percentage of change for the average individual between time 1 and 2. &lt;code&gt;bc2&lt;/code&gt; represents the percentage change betwen time 1 and 3, &lt;code&gt;bc4&lt;/code&gt; is the percentage change between time 1 and 5, etc.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Social Trait Development Computational Model</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-03-30/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-03-30/</guid>
      <description>


&lt;p&gt;I built the following simple computational model for an individual differences class in the Spring of 2018 to demonstrate how to incorporate explantory elements for trait development into a computational framework. This model assumes that an individual’s trait development depends on 1) the environment and 2) interactions with others inside and outside of the individual’s social group. Moreover, the model assumes traits are somewhat stable and exhibit self-similarity across time. The main properties I am trying to capture, therefore, include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The development of a stable trait through interactions with…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a social group&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;random others&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the environment&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These properties do not represent what I think of as “true” aspects of trait development (although I think they are important). I use them, instead, to show the translation from verbal concepts to code representations.&lt;/p&gt;
&lt;p&gt;Here is the pseudocode for the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Build agent&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Random initial trait value&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Peer group holder (initially 0)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build global population of people with the trait (normally distributed)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time 1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;People in peer group?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If yes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;What is their average trait level?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use that level to filter who the agent interacts with from the global population&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If no:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move to next step&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select person from the global population to interact with&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Uniform (-1, 1) = quality of the interaction&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If it goes well, agent’s trait is influenced by this person&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e., If uniform &amp;gt; 0&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If it does not go well, agent keeps own trait&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i.e., If uniform &amp;lt; 0&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Environment&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Random number that influences trait&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update trait and peer holder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the interaction went well, the new person joins the agent’s social group&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Iterate&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;the-incomplete-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Incomplete Model&lt;/h1&gt;
&lt;p&gt;First I present the model without a loop in very simple code. We begin with a distribution of the trait in the population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;global_population &amp;lt;- data.frame(
  &amp;quot;People&amp;quot; = c(1:1000),
  &amp;quot;SDO&amp;quot; = c(rnorm(1000, 100, 10))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I create the agent. I used SDO as my example in class, so that will be the “trait” here. The agent is given an initial value of the trait.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agent &amp;lt;- list(
  SDO = 0,
  Peeps = NULL
)


initial_sdo_value &amp;lt;- rnorm(1, 100, 10)

agent[[1]][1] &amp;lt;- initial_sdo_value

agent&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $SDO
## [1] 89.24501
## 
## $Peeps
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the agent has a social group (‘peeps’), then we would take the mean of their trait levels to inform who the agent interacts with from the global population.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# if peeps &amp;gt; 0, take the average of their trait level

num_peeps &amp;lt;- length(agent$Peeps)

trait_of_peeps &amp;lt;- mean(agent$Peeps)

# use average to bias how I sample the population
# use filter (+ or - 25 from average)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this is the first time point, however, the agent does not have a social group. Now we select a person from the global population for our agent to interact with. If our agent had a social group, the social group’s average trait would inform who we select, but again in this case the interaction is random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;other &amp;lt;- sample(global_population$SDO, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interaction is good or bad…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interaction_quality &amp;lt;- runif(1, min = -1, max = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the interaction is good, our agent’s trait is influenced by this new individual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# quality good? interaction_quality &amp;gt; 0

new_sdo &amp;lt;- agent$SDO + (other - agent$SDO)*interaction_quality

# quality bad? interaction quality &amp;lt; 0

new_sdo &amp;lt;- agent$SDO&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we throw in some environmental disturbance for fun&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Environment

environment_sdo &amp;lt;- sample(c(-20:20), 1)

new_sdo &amp;lt;- new_sdo + environment_sdo&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and conclude by updating the agent&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Update agent

agent$SDO &amp;lt;- c(agent$SDO, new_sdo)

# If the interaction went well, this person goes into friend group. If not, leave them out

agent$Peeps &amp;lt;- c(agent$Peeps, other)

agent&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $SDO
## [1]  89.24501 103.24501
## 
## $Peeps
## [1] 99.37445&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-full-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Full Model&lt;/h1&gt;
&lt;p&gt;Here is the full model and a plot of the agent’s trait over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# - -----------------------------------------------------------------------


# - -----------------------------------------------------------------------


# - -----------------------------------------------------------------------


# - -----------------------------------------------------------------------


# - -----------------------------------------------------------------------


# - -----------------------------------------------------------------------

library(tidyverse)

# Generate over time

time_points &amp;lt;- 400


global_population &amp;lt;- data.frame(
  &amp;quot;People&amp;quot; = c(1:1000),
  &amp;quot;SDO&amp;quot; = c(rnorm(1000, 100, 10))
)


agent &amp;lt;- list(
  SDO = rep(0,time_points),
  Peeps = rep(0,time_points)
)


initial_sdo_value &amp;lt;- rnorm(1, 100, 10)

agent[[1]][1] &amp;lt;- initial_sdo_value

other &amp;lt;- sample(global_population$SDO, 1)

agent[[2]][1] &amp;lt;- other

count &amp;lt;- 0

for(i in 2:time_points){
  
    count &amp;lt;- count + 1
    
    
    # sample global population and interact with them
    # filter based on peeps average
    
    # need to change this to only use values that are not zero
    
    use_non_zero_values &amp;lt;- agent$Peeps[agent$Peeps &amp;gt; 0]
    
    use_vals &amp;lt;- mean(use_non_zero_values)
    filter_top &amp;lt;- use_vals + 20
    filter_lower &amp;lt;- use_vals - 20

    new_df &amp;lt;- global_population %&amp;gt;%
      filter(SDO &amp;lt; filter_top &amp;amp; SDO &amp;gt; filter_lower)
    
    other &amp;lt;- sample(new_df$SDO, 1)
    
    interaction_quality &amp;lt;- runif(1, min = -1, max = 1)
    
    # quality good or bad?
    if(interaction_quality &amp;gt; 0){
      new_sdo &amp;lt;- agent$SDO[i - 1] + (other - agent$SDO[i - 1])*interaction_quality
    }else{
      new_sdo &amp;lt;- agent$SDO[i - 1]
    }
    
    
    # Environment
    
    environment_sdo &amp;lt;- sample(c(-20:20), 1)
    
    new_sdo &amp;lt;- new_sdo + environment_sdo
    
    
    # Update agent
    
    
    agent$SDO[i] &amp;lt;- new_sdo
    
    if(interaction_quality &amp;gt; 0){
      agent$Peeps[i] &amp;lt;- other
    }else{
      agent$Peeps &amp;lt;- agent$Peeps
    }
    
    
    
    
}


library(ggplot2)
plot_agent &amp;lt;- data.frame(
  &amp;#39;Agent_SDO&amp;#39; = c(agent$SDO),
  &amp;quot;Peeps_SDO&amp;quot; = c(agent$Peeps),
  &amp;quot;Time&amp;quot; = c(1:time_points)
)


new_data &amp;lt;- plot_agent %&amp;gt;%
  filter(Peeps_SDO &amp;gt; 0) %&amp;gt;%
  gather(Agent_SDO, Peeps_SDO, key = &amp;#39;variable&amp;#39;, value = &amp;#39;SDO&amp;#39;)


ggplot(new_data, aes(x = Time, y = SDO)) + 
  geom_point() + 
  geom_line(color = &amp;#39;blue&amp;#39;) + 
  facet_wrap(~variable) + 
  ylab(&amp;quot;Level&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-03-30/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Restart &amp; Clear Environment</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-02-18/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-02-18/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# restart session
.rs.restartR()

# clear environment
remove(list = ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Integration and Optimization</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-02-16/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-02-16/</guid>
      <description>


&lt;div id=&#34;integration&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Integration&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Trapezoid Rule&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To find the area under a curve we can generate a sequence of trapezoids that follow the rules of the curve (i.e., the data generating function for the curve) along the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis and then add all of the trapezoids together. To create a trapezoid we use the following equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;let &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; equal the width of the trapezoid (along the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis), then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Area = (&lt;span class=&#34;math inline&#34;&gt;\(w/2\)&lt;/span&gt; * &lt;span class=&#34;math inline&#34;&gt;\(f(x_i)\)&lt;/span&gt;) + &lt;span class=&#34;math inline&#34;&gt;\(f(x_i+1)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;for a single trapezoid. That procedure then iterates across our entire &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis and adds all of the components together.&lt;/p&gt;
&lt;p&gt;Here is an example function: &lt;span class=&#34;math inline&#34;&gt;\(f(x) = 8 + cos(x^3)\)&lt;/span&gt; and we will evaluate it over the interval [1, 10]. First, a plot of the curve itself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(from = 1, to = 10, by = 1)

f_x &amp;lt;- 8 + cos(x^3)



# Plot

library(ggplot2)
library(ggthemes)

ex_plot &amp;lt;- data.frame(
  &amp;quot;x&amp;quot; = c(x),
  &amp;quot;y&amp;quot; = c(f_x)
)

g_plot &amp;lt;- ggplot(ex_plot, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(se = F, span = 0.2) + 
  scale_x_continuous(breaks = c(1:10)) + 
  theme_wsj()

g_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-02-16/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The trapezoid algorithm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Parameters = the function, x-axis beginning, x-axis end, the number of trapezoids to create

trapezoid_rule &amp;lt;- function(fx, start, end, num_traps){
  
  # The width of each trapezoid
  
  w &amp;lt;- (end - start) / num_traps
  
  # the x-axis to evaluate our function along
  
  x_axis &amp;lt;- seq(from = start, to = end, by = w)
  
  # the y axis: apply the function (fx) to each value of our x-axis
  
  y_axis &amp;lt;- sapply(x_axis, fx)
  
  # The trapezoid rule: find the area of each trapezoid and then add them together
  
  trap_total &amp;lt;- w * ( (y_axis[1] / 2) + sum(y_axis[2:num_traps]) + (y_axis[num_traps + 1] / 2) )
  
  return(trap_total)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can evaluate our function (&lt;span class=&#34;math inline&#34;&gt;\(f(x) = 8 + cos(x^3)\)&lt;/span&gt;) with our trapezoid algorithm to find the area under its curve.&lt;/p&gt;
&lt;p&gt;Using only 3 trapezoids:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_function &amp;lt;- function(x){
  
  8 + cos(x^3)
  
}

trapezoid_rule(eval_function, 1, 10, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 72.29808&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using 10 trapezoids:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trapezoid_rule(eval_function, 1, 10, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 72.84693&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using 50000 trapezoids:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trapezoid_rule(eval_function, 1, 10, 50000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 71.84439&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optimization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Optimization&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;The Golden-Section Method&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Newton’s methods are great for finding local maxima or minima, but they also require knowing the derivative of whatever function we are evaluating. The goldent section method does not, and works in the following way:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Define three points along the x-axis: left (&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;), right (&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;), and middle (&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose one of the following sections along the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis according to which is larger:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;middle to right (section “right”)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;middle to left (section “left”)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a point on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis within section “right” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apply our function to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; &amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(f(m)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Else &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a point on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis within section “left” according to the ‘golden rule’ (for our purposes the specifics of the golden rule are not important)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apply our function to &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(f(y)\)&lt;/span&gt; &amp;gt; &lt;span class=&#34;math inline&#34;&gt;\(f(m)\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Else &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Continue until the size of the “right” or “left” window diminishes to some a priori set tolerance value&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that this method assumes that the&lt;/p&gt;
&lt;p&gt;Now in code:&lt;/p&gt;
&lt;p&gt;Our example function: &lt;span class=&#34;math inline&#34;&gt;\(f(x) = sin(x * 3)\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_2 &amp;lt;- seq(from = -5, to = 5, by = 1)

f_x_2 &amp;lt;- -0.5 * (x_2^2) + 4


# Plot

library(ggplot2)

ex_plot_2 &amp;lt;- data.frame(
  &amp;quot;x&amp;quot; = c(x_2),
  &amp;quot;y&amp;quot; = c(f_x_2)
)

g_plot_2 &amp;lt;- ggplot(ex_plot_2, aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(se = F)

g_plot_2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-02-16/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The golden section algorithm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;golden_section &amp;lt;- function(fx, x.l, x.r, x.m, tolerance){
  
  # The golden ratio rule to help select &amp;#39;y&amp;#39; when needed
  
  grule &amp;lt;- 1 + (1 * sqrt(5)) / 2
  
  # Apply the function at each of our starting locations (left, right, middle)
  
  # left
  
  f.l &amp;lt;- fx(x.l)
  
  # right
  
  f.r &amp;lt;- fx(x.r)
  
  # middle
  
  f.m &amp;lt;- fx(x.m)
  
  # continue to iterate until we pass our tolderance level for how big the &amp;quot;right&amp;quot; &amp;quot;left&amp;quot; window should be
  
  while (( x.r - x.l) &amp;gt; tolerance){
    
    
    # if the right window is larger than the left window, then operate on the right window side
    
    if ( (x.r - x.m) &amp;gt; (x.m - x.l) ){
      
      # select a point, y, according to the golden ratio rule
      
      y &amp;lt;- x.m + (x.r - x.m) / grule
      
      # apply the function to our selected y point
      
      f.y &amp;lt;- fx(y)
      
      # if the function at point y is higher than the function at the mid point
      
      if(f.y &amp;gt;= f.m){
        
        # reassign our points according to the algorithm steps outlined above
        
        # in this case, within the right window y was higher than the middle. So &amp;#39;left&amp;#39; needs to become our new middle, and &amp;#39;middle&amp;#39; needs to become y
        
        x.l &amp;lt;- x.m
        f.l &amp;lt;- f.m
        
        x.m &amp;lt;- y
        f.m &amp;lt;- f.y
      } else {
        
        # if the function at y was lower than the function at the mid point
        
        # shift &amp;#39;right&amp;#39; to our y point
        
        x.r &amp;lt;- y
        f.r &amp;lt;- f.y
        
      }
      
      
      
      
      
    } else{
      
      # if the right window is not larger than the left window, select the left window to operate on
      
      
      # choose a point, y, within the left window according to the golden ratio
      
      y &amp;lt;- x.m - (x.m - x.l) / grule
      
      # apply our function to that point
      
      f.y &amp;lt;- fx(y)
      
      
      # if the function at y is greater than the function at the mid point (within the left window)
      
      if(f.y &amp;gt;= f.m){
        
        # reassign values according to the golden section method discussed above
        
        # in this case, within the left window our selected point is higher than the mid point (which is to the right of the selected y point)
        # so our &amp;quot;mid&amp;quot; point needs to become our &amp;quot;right&amp;quot; point and y needs to become &amp;quot;left&amp;quot;
        
        x.r &amp;lt;- x.m
        f.r &amp;lt;- f.m
        
        x.m &amp;lt;- y
        f.m &amp;lt;- f.y
        
        
        
      }else{
        
        # if the y point is lower than the function at the mid point
        
        # now our y needs to become &amp;quot;left&amp;quot;
        
        x.l &amp;lt;- y
        f.l &amp;lt;- f.y
      }
      
      
    }
    
    
    
  }
  
  # return the mid point
  
  return(x.m)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, the algorithm splits the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis into windows (left, middle, right) and then evaluates the function across those windows. The dimensions of the windows change over time depending on whether the function at &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is higher or lower than a specific window dimension.&lt;/p&gt;
&lt;p&gt;These examples are described in more detail in Jones, Maillardet, and Robinson, &lt;strong&gt;Introduction to Scientific Programming and Simulation Using R&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Horizontal Y Axis GGplot2</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-01-17/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-01-17/</guid>
      <description>


&lt;p&gt;I always forget how to make the y-axis horizontal in &lt;code&gt;ggplot2&lt;/code&gt;. Here’s a note.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme(axis.title.y = element_text(angle = 0)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random Walks</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-01-11/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-01-11/</guid>
      <description>
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Some random walk fun. I use 400 steps in each example.&lt;/p&gt;
&lt;div id=&#34;one-dimensional-random-walk&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One-Dimensional Random Walk&lt;/h1&gt;
&lt;p&gt;A random walk using a recursive equation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Empty vector to store the walk

rw_1 &amp;lt;- numeric(400)

# Initial value

rw_1[1] &amp;lt;- 7

# The Random Walk equation in a for-loop

for(i in 2:400){
  
  rw_1[i] &amp;lt;- 1*rw_1[i - 1] + rnorm(1,0,2)
  
}

plot(rw_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-01-11/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A random walk using R’s “cumsum” command. Here, I will generate a vector of randomly selected 1’s and -1’s. “Cumsum” then compiles those values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A vector of 1&amp;#39;s and -1&amp;#39;s

rw_2 &amp;lt;- sample(c(1, -1), 400, replace = T)

rw_2 &amp;lt;- cumsum(rw_2)

plot(rw_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2018-01-11/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-dimensional-random-walk&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-Dimensional Random Walk&lt;/h1&gt;
&lt;p&gt;Now for the real fun. Here, the walk can move forward (1) or backward (-1) along either dimension 1 or 2. So, if the walk moves forward (1) in dimension 1, dimension 2 receives a value of 0 for that step. If the walk moves backward (-1) in dimension 2, dimension 1 receives a 0 for that step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A matrix to store our walk

# Column 1 is dimension 1, column 2 is dimension 2

rw_3 &amp;lt;- matrix(0, ncol = 2, nrow = 400)

index &amp;lt;- cbind(
      1:400, sample(c(1, 2),
      400,
      replace = T)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The “index” merits some explaining. The walk will randomly choose to move in dimension 1 (column 1 in “rw_3”) or 2 (column 2 in “rw_3”). This index establishes a way of assigning which choice the walk makes. Here is what “index” looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(index)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    2
## [2,]    2    1
## [3,]    3    1
## [4,]    4    2
## [5,]    5    2
## [6,]    6    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first column values tell the random walk which step its on (i.e., which row in “rw_3”), and the second column values tell the random walk which dimension it will step through (i.e., which column in “rw_3”).&lt;/p&gt;
&lt;p&gt;So the “index” represents a random selection of dimension 1 or 2 at each step. Now I can apply that random choice to the random choice of stepping forward or backward (1 or -1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# At each step, select a dimension (specified by the index; column 1 or 2 of rw_3)

# Then randomly select forward or backward

rw_3[index] &amp;lt;- sample(c(-1, 1), 
                      400, 
                      replace = T)



# Now sum each column (dimension) just like our 1-dimensional walks

rw_3[,1] &amp;lt;- cumsum(rw_3[,1])
rw_3[,2] &amp;lt;- cumsum(rw_3[,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a visualization of the walk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plotly)

rw_3 &amp;lt;- data.frame(rw_3)
rw_3$step &amp;lt;- c(1:400)

names(rw_3)[1:2] &amp;lt;- c(&amp;quot;Dim_1&amp;quot;, &amp;quot;Dim_2&amp;quot;)

plot_ly(rw_3, x = ~step, y = ~Dim_1, z = ~Dim_2, type = &amp;#39;scatter3d&amp;#39;, mode = &amp;#39;lines&amp;#39;,
        line = list(color = &amp;#39;#1f77b4&amp;#39;, width = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;82dd591c3b6d&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;82dd591c3b6d&#34;,&#34;attrs&#34;:{&#34;82dd591c3b6d&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;z&#34;:{},&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;#1f77b4&#34;,&#34;width&#34;:1},&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;type&#34;:&#34;scatter3d&#34;}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;step&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Dim_1&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;Dim_2&#34;}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;x&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400],&#34;y&#34;:[0,-1,-2,-2,-2,-2,-1,-2,-2,-2,-1,0,1,0,-1,-1,0,0,1,1,1,1,2,3,3,3,3,4,4,3,2,3,4,3,4,4,5,4,4,5,6,5,5,4,5,5,4,3,4,4,4,4,4,3,3,3,3,2,3,2,3,2,3,3,3,4,4,4,4,4,5,5,4,4,4,4,5,5,4,4,3,3,2,2,3,4,5,6,6,5,6,6,6,6,5,5,5,4,4,5,4,5,5,5,5,5,6,6,6,5,5,5,5,4,4,5,6,6,6,5,5,6,6,6,6,6,7,7,7,8,9,8,9,10,10,10,11,11,11,10,11,12,11,10,10,10,9,9,9,8,7,6,6,7,7,8,8,7,8,7,7,8,7,7,7,6,6,7,7,8,8,8,9,10,11,11,11,10,11,12,13,13,12,12,12,13,13,14,14,14,13,14,15,15,14,13,13,13,13,14,15,14,15,14,13,13,12,12,12,13,13,13,13,13,14,13,13,13,13,13,13,13,13,12,12,12,13,13,13,13,13,13,14,13,12,12,12,12,12,12,12,13,13,13,13,13,13,12,12,12,12,12,12,11,11,11,10,11,11,10,10,10,10,10,11,10,10,11,12,12,12,12,12,13,12,13,13,14,15,15,15,16,17,16,16,16,16,16,16,17,17,16,15,16,16,17,18,18,19,18,18,19,18,18,18,18,19,19,19,19,20,20,20,20,19,19,19,20,20,21,20,21,22,21,22,22,22,22,21,20,20,20,20,20,20,20,19,19,19,20,20,21,21,21,22,23,22,21,20,20,20,20,20,20,21,20,21,20,21,22,22,23,23,23,24,24,23,23,23,23,23,23,24,24,24,25,25,25,25,26,25,25,25,25,25,26,25,25,25,25,24,23,23,23,22,22,22,22,22,22],&#34;z&#34;:[1,1,1,2,3,4,4,4,5,4,4,4,4,4,4,3,3,4,4,3,4,3,3,3,2,1,2,2,1,1,1,1,1,1,1,2,2,2,1,1,1,1,2,2,2,1,1,1,1,2,1,0,-1,-1,0,1,2,2,2,2,2,2,2,3,4,4,5,6,5,6,6,5,5,4,5,6,6,5,5,6,6,5,5,6,6,6,6,6,7,7,7,8,7,8,8,9,10,10,11,11,11,11,12,13,14,15,15,14,13,13,12,11,12,12,11,11,11,12,11,11,10,10,9,10,9,8,8,9,8,8,8,8,8,8,9,10,10,9,10,10,10,10,10,10,9,8,8,7,6,6,6,6,7,7,6,6,7,7,7,7,8,8,8,7,8,8,7,7,6,6,5,6,6,6,6,5,4,4,4,4,4,5,5,6,5,5,4,4,5,6,6,6,6,5,5,5,4,3,4,4,4,4,4,4,4,5,5,6,5,5,4,5,4,3,3,3,2,1,0,-1,0,-1,0,0,-1,0,0,1,0,1,0,1,1,1,1,2,1,2,3,2,3,3,2,3,2,3,2,2,1,2,3,2,1,1,0,1,1,1,2,2,1,0,-1,-2,-2,-2,-1,-1,-1,-2,-3,-4,-3,-3,-3,-3,-4,-4,-4,-3,-4,-4,-4,-4,-3,-4,-5,-4,-5,-5,-4,-4,-4,-4,-5,-5,-5,-4,-4,-4,-3,-3,-3,-2,-1,-2,-2,-1,0,-1,-1,0,1,0,0,-1,-2,-2,-1,-1,-1,-1,-1,-1,-1,-2,-1,0,0,0,1,2,3,4,3,4,4,5,4,4,5,5,4,3,3,3,3,3,3,2,3,2,3,2,2,2,2,2,2,2,1,1,2,1,1,0,0,-1,-2,-1,0,1,1,0,1,1,0,-1,-2,-2,-2,-3,-4,-5,-4,-4,-4,-3,-4,-5,-5,-5,-4,-5,-5,-6,-5,-6,-5,-6],&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;#1f77b4&#34;,&#34;width&#34;:1},&#34;type&#34;:&#34;scatter3d&#34;,&#34;marker&#34;:{&#34;color&#34;:&#34;rgba(31,119,180,1)&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(31,119,180,1)&#34;}},&#34;error_y&#34;:{&#34;color&#34;:&#34;rgba(31,119,180,1)&#34;},&#34;error_x&#34;:{&#34;color&#34;:&#34;rgba(31,119,180,1)&#34;},&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Combining CSV Files</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-01-03/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-01-03/</guid>
      <description>


&lt;p&gt;A couple quick pieces of code to assist any time I need to work with many CSV files.&lt;/p&gt;
&lt;div id=&#34;into-list&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Into List&lt;/h1&gt;
&lt;p&gt;This first code chunk loads all of the CSV files in a folder, makes each into data frame, and stores each separately in a list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;enter path&amp;quot;)

# A character vector of every file name

files &amp;lt;- Sys.glob(&amp;quot;*.csv&amp;quot;)

# A list of all CSV files in the respective folder as data.frames

myfiles &amp;lt;- lapply(files, FUN = read.csv)

# To load any single data set...

data_set1 &amp;lt;- myfiles[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;into-single-data-frame&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Into Single Data Frame&lt;/h1&gt;
&lt;p&gt;The code above stores each file into a list as a separate data frame. If I want to combine every CSV file into the same data frame I can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;enter path&amp;quot;)

# A character vector of every file name

files &amp;lt;- list.files(pattern = &amp;quot;*.csv&amp;quot;)

# Now the full command

data_set &amp;lt;- do.call(cbind, 
                    lapply(files, 
                           function(x) read.csv(x, stringsAsFactors = FALSE)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code shown uses “cbind” so every variable within every CSV file will receive its own column in my “data_set.” If every CSV file has the same variable names replace “cbind” with “rbind.”&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Formatting Qualtrics Responses</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2018-01-02/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2018-01-02/</guid>
      <description>


&lt;p&gt;Here is a quick piece of code to create numeric response scores when data are read in as strings (e.g., “Strongly Agree, Agree, Neutral”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dplyr)
library(plyr)

df &amp;lt;- read.csv(&amp;quot;path&amp;quot;)

labels_to_values1 &amp;lt;- function(x){
  
  mapvalues(x, from = c(&amp;quot;Strongly Agree&amp;quot;, 
                        &amp;quot;Agree&amp;quot;, 
                        &amp;quot;Slightly Agree&amp;quot;, 
                        &amp;quot;Slightly Disagree&amp;quot;, 
                        &amp;quot;Disagree&amp;quot;, 
                        &amp;quot;Strongly Disagree&amp;quot;),
                        to = c(6,5,4,3,2,1))
  
}

recode_df &amp;lt;- df %&amp;gt;%
  select(column_to_modify1, column_to_modify2, column_to_modify2, etc) %&amp;gt;%
  apply(2, FUN = labels_to_values1) %&amp;gt;%
  data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that R will throw you warnings if all of the response options are not used, but the code will still work.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Differencing By Group</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-12-23/</link>
      <pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-12-23/</guid>
      <description>
&lt;script src=&#34;https://christopherdishop.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A bit of practice taking the first difference when the data is not consistent with a typical time-series structure.&lt;/p&gt;
&lt;p&gt;The first set of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(kableExtra)
dff &amp;lt;- tibble(
  &amp;#39;id&amp;#39; = c(&amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;, &amp;#39;c&amp;#39;),
  &amp;#39;survey&amp;#39; = c(1, 2, 1, 2, 1, 2),
  &amp;#39;score&amp;#39; = c(4, 4, 2, 4, 5, 2),
  &amp;#39;team&amp;#39; = c(&amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;a&amp;#39;)
)
dff %&amp;gt;% kable() %&amp;gt;% kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
survey
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
score
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
team
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
b
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
b
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
c
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
c
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
a
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The goal is to subtract scores on the first survey from scores on the second survey. E.g., what are the change scores across the surveys for each participant?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dff %&amp;gt;% 
  group_by(id) %&amp;gt;% 
  mutate(diffscore = score - lag(score))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   id [3]
##   id    survey score team  diffscore
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 a          1     4 a            NA
## 2 a          2     4 a             0
## 3 b          1     2 a            NA
## 4 b          2     4 a             2
## 5 c          1     5 a            NA
## 6 c          2     2 a            -3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second set of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;score &amp;lt;- c(10,30,14,20,6)
group &amp;lt;- c(rep(1001,2),rep(1005,3))
df &amp;lt;- data.frame(score,group)

df %&amp;gt;% kable() %&amp;gt;% kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
score
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
group
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1005
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1005
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1005
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Group 10001 has two scores whereas group 1005 has 3. I want the change from one score to another for each group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  group_by(group) %&amp;gt;%
  mutate(first_diff = score - lag(score))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   group [2]
##   score group first_diff
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1    10  1001         NA
## 2    30  1001         20
## 3    14  1005         NA
## 4    20  1005          6
## 5     6  1005        -14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Subtyping cognitive profiles in Autism Spectrum Disorder using a random forest algorithm</title>
      <link>https://christopherdishop.netlify.app/publication/2017-subtyping/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2017-subtyping/</guid>
      <description></description>
    </item>
    
    <item>
      <title>blogdown: Creating Websites with R Markdown</title>
      <link>https://christopherdishop.netlify.app/publication/2017-blogdown/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2017-blogdown/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why Detecting Interactions is Easier in the Lab</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-11-15/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-11-15/</guid>
      <description>


&lt;p&gt;A fun simulation by McClelland and Judd (1993) in &lt;em&gt;Psychological Bulletin&lt;/em&gt; that demonstrates why detecting interactions outside the lab (i.e., in field studies) is difficult. In experiments, scores on the independent variables are located at the extremes of their respective distributions because we manipulate conditions. The distribution of scores across all of the independent variables in field studies, conversely, is typically assumed to be normal. By creating “extreme groups” in experiments, therefore, it becomes easier to detect interactions.&lt;/p&gt;
&lt;p&gt;Imagine running an experiment where we randomly assign participants to one of two groups on an independent variable, goal difficulty. In one group the goal is challening, in the other group the goal is easy to accomplish. We are then interested in which group performs better on a task. After randomly assigning to groups, the distribution of scores on “goal difficulty” would be as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;where 50 people are assigned to each condition. In this case, the distribution of scores is aligned at the extremes (i.e., -1, or the hard goal, and 1, or the easy goal) because we manipulated that variable. In field studies, where we cannot manipulate goal difficulty, the distribution of scores would be as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;where scores about the independent variable (goal difficulty) are dispersed because we did not manipulate. The same distributional differences occur across other independent variables that we include in our design, and they are the reason behind fewer interaction detections in field studies.&lt;/p&gt;
&lt;p&gt;The cool part is that this happens even when the data generating mechanisms are exactly the same. The mechanism that causes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, in both the experiments and field studies in this simulation, will be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
y_{i} = b_0{i} + b_1{x_i} + b_2{z_i} + b_3{zx_i} + e_{i}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; is the value of the outcome (i.e., performance) for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person, &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the value of one independent variable for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person (i.e., goal difficulty), &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; is the value of another independent variable for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person (e.g., whatever variable you please), &lt;span class=&#34;math inline&#34;&gt;\(zx_i\)&lt;/span&gt; represents the combination of values on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person (i.e., the interaction term), &lt;span class=&#34;math inline&#34;&gt;\(e_i\)&lt;/span&gt; is a normally distributed error term for the &lt;span class=&#34;math inline&#34;&gt;\(i^\text{th}\)&lt;/span&gt; person, and &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; represent the regression intercept and coefficients relating the predictors to the outcome.&lt;/p&gt;
&lt;p&gt;Again, the data generating equation, the thing that causes &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, is the same for both field studies and experiments. We are going to find differences, however, simply because the distribution on the independent variables are different.&lt;/p&gt;
&lt;p&gt;The values for &lt;span class=&#34;math inline&#34;&gt;\(b_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; will be, respectively, 0, 0.20, 0.10, and 1.0 (see McClelland &amp;amp; Judd, 1993). In other words, our interaction coefficient is gigantic.&lt;/p&gt;
&lt;p&gt;Each simulation will use the equation just presented to generate data across 100 individuals in the field and 100 individuals in the lab. The only difference between the two groups will be their initial distribution on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. For the lab group, their scores will be randomly assigned to -1 or 1, and in the field group scores will be randomly dispersed (normally) between -1 and 1. After generating the data I then estimate the coefficients using multiple regression and save the significance value in a vector. The process then interates 1000 times.&lt;/p&gt;
&lt;div id=&#34;the-experiment-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Experiment Data&lt;/h1&gt;
&lt;div id=&#34;the-distribution-of-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of X:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-distribution-of-z&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of Z:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-distribution-of-y-after-using-the-equation-above-to-generate-scores-on-y&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of Y after using the equation above to generate scores on Y:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_values &amp;lt;- b_0 + 0.20*x_values + 0.10*z_values + 
            1.00*x_values*z_values + rnorm(100,0,4)

hist(y_values)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-estimate-the-parameters-using-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now estimate the parameters using regression:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp_data &amp;lt;- data.frame(&amp;quot;X&amp;quot; = c(x_values),
                       &amp;quot;Z&amp;quot; = c(z_values),
                       &amp;quot;Y&amp;quot; = c(y_values))

exp_model &amp;lt;- lm(Y ~ X + Z + X:Z, data = exp_data)
summary(exp_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ X + Z + X:Z, data = exp_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6044 -2.2962  0.1527  2.9004  8.3609 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)   0.4735     0.3939   1.202   0.2323  
## X            -0.7021     0.3939  -1.783   0.0778 .
## Z             0.6931     0.3939   1.760   0.0817 .
## X:Z           0.9612     0.3939   2.440   0.0165 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.907 on 96 degrees of freedom
## Multiple R-squared:  0.1039, Adjusted R-squared:  0.07593 
## F-statistic: 3.712 on 3 and 96 DF,  p-value: 0.01417&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-field-study-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Field Study Data&lt;/h1&gt;
&lt;div id=&#34;the-distribution-of-x-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of X:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-distribution-of-z-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of Z:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-distribution-of-y-after-using-the-equation-above-to-generate-scores-on-y-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The distribution of Y after using the equation above to generate scores on Y:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_y_values &amp;lt;- b_0 + 0.20*f_x_values + 0.10*f_z_values + 
              1.00*f_x_values*f_z_values + rnorm(100,0,4)

hist(f_y_values)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-11-15/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-estimate-the-parameters-using-regression-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now estimate the parameters using regression:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;field_data &amp;lt;- data.frame(&amp;quot;FX&amp;quot; = c(f_x_values),
                           &amp;quot;FZ&amp;quot; = c(f_z_values),
                           &amp;quot;FY&amp;quot; = c(f_y_values))

field_model &amp;lt;- lm(FY ~ FX + FZ + FX:FZ, data = field_data)
summary(field_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = FY ~ FX + FZ + FX:FZ, data = field_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.5347 -2.5822 -0.0993  3.0451 11.0879 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)  -0.4747     0.4032  -1.177    0.242
## FX            0.6387     0.7902   0.808    0.421
## FZ            0.1865     0.7932   0.235    0.815
## FX:FZ         1.9542     1.5989   1.222    0.225
## 
## Residual standard error: 4.025 on 96 degrees of freedom
## Multiple R-squared:  0.02598,    Adjusted R-squared:  -0.004459 
## F-statistic: 0.8535 on 3 and 96 DF,  p-value: 0.4681&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-everything-into-monte-carlo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Putting Everything Into Monte Carlo&lt;/h1&gt;
&lt;div id=&#34;replicate-the-process-above-1000-times-and-save-the-p-value-each-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Replicate the process above 1000 times and save the p-value each time&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sims &amp;lt;- 1000
exp_results &amp;lt;- numeric(1000)
field_results &amp;lt;- numeric(1000)


X_coefficient &amp;lt;- 0.20
Z_coefficient &amp;lt;- 0.10

XZ_coefficient &amp;lt;- 1.00
Mu &amp;lt;- 0

xy_data &amp;lt;- c(-1,1)

library(MASS)

for(i in 1:sims){
  
  # Experiment Data
  
  # X
  x_values &amp;lt;- sample(xy_data, 100, replace = T)
  
  # Z
  z_values &amp;lt;- sample(xy_data, 100, replace = T)
  
  # Y
  y_values &amp;lt;- Mu + X_coefficient * x_values + Z_coefficient * z_values + 
              XZ_coefficient * x_values * z_values + rnorm(100,0,4)
  
  exp_data &amp;lt;- data.frame(&amp;quot;X&amp;quot; = c(x_values),
                         &amp;quot;Z&amp;quot; = c(z_values),
                         &amp;quot;Y&amp;quot; = c(y_values))
  
  
  # Field Data
  
  # X
  f_x_values &amp;lt;- rnorm(100, 0, 0.5)
  
  # Z
  f_z_values &amp;lt;- rnorm(100, 0, 0.5)
  
  # Y
  f_y_values &amp;lt;- Mu + X_coefficient * f_x_values + Z_coefficient * f_z_values + 
                XZ_coefficient * f_x_values * f_z_values + rnorm(100,0,4)
 
  
  field_data &amp;lt;- data.frame(&amp;quot;FX&amp;quot; = c(f_x_values),
                           &amp;quot;FZ&amp;quot; = c(f_z_values),
                           &amp;quot;FY&amp;quot; = c(f_y_values))
  
  
  # Modeling
  
  
  exp_model &amp;lt;- lm(Y ~ X + Z + X:Z, data = exp_data)
  exp_results[i] &amp;lt;- summary(exp_model)$coefficients[4,4]
  
  field_model &amp;lt;- lm(FY ~ FX + FZ + FX:FZ, data = field_data)
  field_results[i] &amp;lt;- summary(field_model)$coefficients[4,4]
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Results&lt;/h1&gt;
&lt;div id=&#34;what-proportion-of-experiments-find-significant-interaction-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What proportion of experiments find significant interaction effects?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(exp_results &amp;lt; 0.05) / 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.672&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-proportion-of-field-studies-find-significant-interaction-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What proportion of field studies find significant interaction effects?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(field_results &amp;lt; 0.05) / 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.082&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workforce Dynamics</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-08-22/</link>
      <pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-08-22/</guid>
      <description>


&lt;p&gt;We can model the states of a system by applying a transition matrix to values represented in an initial distribution and repeating it until we reach an equilibrium.&lt;/p&gt;
&lt;p&gt;Suppose we want to model how job roles in a given company change over time. Let us assume the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There are three (hierarchical) positions in the company:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Analyst&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Project Coordinator&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manager&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;30 new workers enter the company each year, and they all begin as analysts&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability of moving from …&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;an analyst to a project coordinator is 75%&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a project coordinator to a manager is 8%&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The probability of staying in a position is 25%&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The initial distribution of people in each role (analyst, PC, manager) is: c(45, 15, 6)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;the-initial-states&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Initial States:&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;initial &amp;lt;- c(45, 15, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-transition-matrix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Transition Matrix:&lt;/h1&gt;
&lt;p&gt;Consistent with the assumptions described above…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transition &amp;lt;- matrix(c(   0.25, 0.00, 30,
                          0.75, 0.25, 0.00,
                          0.00, 0.08, 0.25  ), 3, 3, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-company-roles-over-50-years&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Company Roles Over 50 Years:&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- matrix(, nrow = 50, ncol = 3)

count &amp;lt;- 0

for(i in 1:50){
  count &amp;lt;- count + 1
  
  if(i == 1){
    
    df[count,] = initial
  
  }
  else{
    
    df[count,] = transition%^%i %*% initial
  }
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If job-movement in a company aligned with our initial assumptions, we would expect the distribution of jobs to follow this pattern across time:&lt;/p&gt;
&lt;p&gt;Some data tidying first…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(df)
names(df) &amp;lt;- c(&amp;quot;Analyst&amp;quot;, &amp;quot;Project_Coordinator&amp;quot;, &amp;quot;Manager&amp;quot;)
df$Time &amp;lt;- rep(1:nrow(df))

data_f &amp;lt;- df %&amp;gt;%
  gather(Analyst, Project_Coordinator, Manager, key = &amp;quot;Position&amp;quot;, value = &amp;quot;Num_People&amp;quot;)

total_value &amp;lt;- data_f %&amp;gt;%
  group_by(Time) %&amp;gt;%
  summarise(
    total = sum(Num_People)
  )

data_f &amp;lt;- left_join(data_f, total_value)


data_f &amp;lt;- data_f %&amp;gt;%
  mutate(Proportion = Num_People / total)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The proportion of people in each position:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)

ggplot(data_f, aes(x = Time, y = Proportion, color = Position)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-08-22/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The amount of people in the company overall:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data_f, aes(x = Time, y = Num_People, color = Position)) + 
  geom_point() + 
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-08-22/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can tell, this is unrealistic =)&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Art With Monte Carlo</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-07-12-art/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-07-12-art/</guid>
      <description>


&lt;p&gt;I like to think of Monte Carlo as a counting method. If a condition is satisfied we make a note (e.g., 1), and if the condition is not satisfied we make a different note (e.g., 0). We then iterate and evaluate the pattern of 1’s and 0’s to learn about our process. Art can be described in a similar way: if a condition is satisfied we use a color, and if a condition is not satisfied we use a different color. After many iterations, we have an image.&lt;/p&gt;
&lt;p&gt;Here is a simulation that “draws” a process, inspired by Caleb Madrigal (link &lt;a href=&#34;https://github.com/calebmadrigal/math-with-python/blob/master/MonteCarloEstimation.ipynb&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;the-data-generating-process&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data Generating Process&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x){
  2*sin(4*x) + 2*sin(5*x) + 12
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;some-initial-values&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some Initial Values&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0, 10, length.out  = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-dgp-to-generate-values-of-y&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the DGP to generate values of Y&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- f(x)

plot(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-07-12-art/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-is-the-process-we-want-to-draw&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;This is the process we want to “draw”&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;now-for-the-monte-carlo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Now for the Monte Carlo&lt;/h1&gt;
&lt;p&gt;We are going to evaluate 10,000 points within our process space (10 x 16).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num_points &amp;lt;- 10000
rect_width &amp;lt;- 10
rect_height &amp;lt;- 16

points &amp;lt;- matrix(, ncol = 2, nrow = num_points)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Column 1 of our points matrix represents the width of our process space while column 2 represents its height. First we fill the matrix with random values within our process space:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:num_points){
  points[i,1] = runif(1, 0, rect_width)
  points[i,2] = runif(1, 0, rect_height)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we iterate across all of those points and evaluate them with respect to our process. Think of the “width” as &lt;em&gt;X&lt;/em&gt; values and the “height” as &lt;em&gt;Y&lt;/em&gt; values. Given a value of &lt;em&gt;X&lt;/em&gt;, is our random value of &lt;em&gt;Y&lt;/em&gt; less than it would be if we created a &lt;em&gt;Y&lt;/em&gt; value by using our function (f(x))? If so, mark it in the “points_under” vector. If not, mark it in the “points_over” vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points_under = matrix(, ncol = 2, nrow = num_points)
points_above = matrix(, ncol = 2, nrow = num_points)

for(i in 1:num_points){
  if(points[i,2] &amp;lt; f(points[i,1])){
    points_under[i,1] &amp;lt;- points[i,1]
    points_under[i,2] &amp;lt;- points[i,2]
  }
  else{
    points_above[i,1] &amp;lt;- points[i,1]
    points_above[i,2] &amp;lt;- points[i,2]
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put the results into new vectors without NA’s. Some NA’s come up because our data generating process is crazy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points_under_x &amp;lt;-  points_under[!is.na(points_under[,1]),1]
points_under_y &amp;lt;-  points_under[!is.na(points_under[,2]),2]

points_over_x &amp;lt;- points_above[!is.na(points_above[,1]),1]
points_over_y &amp;lt;- points_above[!is.na(points_above[,2]),2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have an image…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(points_under_y ~ points_under_x, pch = 20, cex = 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://christopherdishop.netlify.app/Computational_Notes/2017-07-12-art/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidyverse Randoms</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-06-24/</link>
      <pubDate>Sat, 24 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-06-24/</guid>
      <description>


&lt;p&gt;Some tidyverse commands I came across and hadn’t seen before. Thought it would be useful to store them here.&lt;/p&gt;
&lt;div id=&#34;replace-recode&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Replace &amp;amp; Recode&lt;/h3&gt;
&lt;p&gt;Replace missing values with the median.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(stress = replace(stress,
                          is.na(stress),
                          median(stress, na.rm = T)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Change a variable’s label.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- df %&amp;gt;%
  mutate(group = replace(group, group == &amp;quot;A&amp;quot;, &amp;quot;Group-A&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recode is a simple version of &lt;code&gt;case_when&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  mutate(color = recode(color,
                        &amp;quot;g&amp;quot; = &amp;quot;green&amp;quot;,
                        &amp;quot;b&amp;quot; = &amp;quot;blue&amp;quot;,
                        &amp;quot;y&amp;quot; = &amp;quot;y&amp;quot;,
                        .default = &amp;quot;other&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-alternative-to-quosure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Alternative To Quosure&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc &amp;lt;- function(data, group_var) {
  data %&amp;gt;%
    group_by({{ group_var }}) %&amp;gt;%
    summarize(mean = mean(stress))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_m_sd &amp;lt;- function(data, mean_var, sd_var) {
  data %&amp;gt;%
    summarize(
      &amp;quot;mean_{{mean_var}}&amp;quot; := mean({{ mean_var }}),
      &amp;quot;sd_{{sd_var}}&amp;quot; := mean({{ sd_var }})
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-.data-in-a-for-loop&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using .data in a for-loop&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (variable in names(df)) {
  df %&amp;gt;% count(.data[[variable]]) %&amp;gt;% print()
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;select-a-column-if-its-row-values-have-x&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Select a column if it’s row values have x&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  select_if(is.numeric) %&amp;gt;%
  select_if(~mean(., na.rm=TRUE) &amp;gt; 10)


df %&amp;gt;% 
  select_all(any_vars(str_detect(., pattern = &amp;quot;Mu&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;if-with-is-at-with-vars&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;If with “is” At with &amp;quot;vars&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutate_if(is.numeric)

mutate_at(vars(contains(&amp;quot;Q&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Convert Text File</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-04-09/</link>
      <pubDate>Sun, 09 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-04-09/</guid>
      <description>


&lt;p&gt;A quick piece of code that reads a text file, changes something, saves a new text file, and iterates that process for every text file in that folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setwd(&amp;quot;path to the text files&amp;quot;)
library(readr)

all_files = Sys.glob(&amp;quot;*.txt&amp;quot;)



for(i in 1:length(all_files)){
  
  data = all_files[i]
  mystring = read_file(paste(data))
  
  new_data = gsub(&amp;quot;old piece of text&amp;quot;, &amp;quot;new piece of text&amp;quot;, mystring)
  
  write_file(new_data, path = paste(&amp;quot;something&amp;quot;, code, &amp;quot;.txt&amp;quot;, sep = &amp;quot;&amp;quot;)
  
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facet Wrap Across Multiple Pages</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-04-07/</link>
      <pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-04-07/</guid>
      <description>


&lt;p&gt;Great &lt;a href=&#34;https://www.programmingwithr.com/how-to-make-your-facet-wrap-facet-grid-ggplots-span-across-multiple-pages-in-pdf/&#34;&gt;discussion of extending a facet wrap across several pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantitative Analysis of Disfluency in Children with Autism Spectrum Disorder or Language Impairment</title>
      <link>https://christopherdishop.netlify.app/publication/2017-mazes-asd-sli/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2017-mazes-asd-sli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Binomial Effect Size Display</title>
      <link>https://christopherdishop.netlify.app/computational_notes/2017-01-01/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/computational_notes/2017-01-01/</guid>
      <description>


&lt;p&gt;Effect sizes provide information about the magnitude of an effect. Unfortunately, they can be difficult to interpret or appear “small” to anyone unfamiliar with the typical effect sizes in a given research field. &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C23&amp;amp;q=Effect+Sizes%3A+Pearson%27s+Correlation%2C+Its+Display+Via+the+BESD%2C+and+Alternative+Indices&amp;amp;btnG=&#34;&gt;Rosenthal and Rubin (1992)&lt;/a&gt; provide an intuitive effect size, called the Binomial Effect Size Display, that captures the change in success rate due to a treatment.&lt;/p&gt;
&lt;p&gt;The calculation is simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Treamtment BESD = 0.50 + (&lt;em&gt;r&lt;/em&gt; / 2)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Control BESD = 0.50 - (&lt;em&gt;r&lt;/em&gt; / 2)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where &lt;em&gt;r&lt;/em&gt; is the correlation coefficient between treatment and survival (however defined). Many mathematical discussions exist, below is a simulation of one specific example by &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C23&amp;amp;q=using+the+binomial+effect+size+display+to+present+the+magnitude&amp;amp;btnG=&#34;&gt;Randolph and Edmondson (2005)&lt;/a&gt;. Please keep in mind the BESD is not without its critics (e.g., &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C23&amp;amp;q=An+Evaluation+of+Rosenthal+and+Rubin%27s+Binomial+Effect+Size+Display&amp;amp;btnG=&#34;&gt;Thompson 1998&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;the-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Example&lt;/h1&gt;
&lt;p&gt;Aziothymidine (&lt;em&gt;AZT&lt;/em&gt;) is used to treat AIDS, and the correlation between &lt;em&gt;AZT&lt;/em&gt; use and survival is 0.23. Using the equations above, we can calculate the BESD for the treatment and control groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Survival

AZT_survive &amp;lt;- 0.50 + (0.23 / 2)
Placebo_survive &amp;lt;- 0.50 - (0.23 / 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the survival percentages for each group are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AZT_survive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.615&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Placebo_survive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.385&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can simulate that process to see if our results match.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Simulation&lt;/h1&gt;
&lt;p&gt;Preliminary set up:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 1000
percent_treatment_survive &amp;lt;- numeric(k)
percent_control_survive &amp;lt;- numeric(k)

# The correlation between AZT and survival is 0.23

Sigma &amp;lt;- matrix(c(1.0, 0.23,
                    0.23, 1.0), 2, 2, byrow = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the process:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:k){
  
  # Draws from a binomial distribution with 0.50 base rate
  
  # The correlation between both vectors is 0.23
  
  # The first vector is treatment vs control assignment.
  # 1 = treatment ; 0 = control
  
  # The second vector is survive vs. not survive
  # 1 = survive ; 0 = not survive
  
  x &amp;lt;- rmvbin(5000, margprob = c(0.5, 0.5), bincorr = Sigma)
  x &amp;lt;- as.data.frame(x)
  
  # &amp;quot;Survive&amp;quot; is when column 2 is equal to 1
  
  total_survive &amp;lt;- x %&amp;gt;%
        filter(V2 == 1)
  
  # The amount of people in each group that survived
  
  treatment_survive &amp;lt;- sum(total_survive$V1 == 1) / nrow(total_survive)
  
  control_survive &amp;lt;- sum(total_survive$V1 == 0) / nrow(total_survive)
  
  # Save the results from each iteration
  
  percent_treatment_survive[i] &amp;lt;- treatment_survive
  percent_control_survive[i] &amp;lt;- control_survive
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparison&lt;/h1&gt;
&lt;p&gt;Our original calculations were as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AZT_survive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.615&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Placebo_survive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.385&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and here are the simulation results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(percent_treatment_survive)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6158837&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(percent_control_survive)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3841163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind the BESD assumes a 50/50 base rate of success (however defined) with no treatment.&lt;/p&gt;
&lt;p&gt;Bo&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;m =)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Epidemiology of autism spectrum disorders</title>
      <link>https://christopherdishop.netlify.app/publication/2016-primer-on-asd/</link>
      <pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2016-primer-on-asd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Uh and Um in Children With Autism Spectrum Disorders or Language Impairment</title>
      <link>https://christopherdishop.netlify.app/publication/2016-uh-and-um-asd-sli/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2016-uh-and-um-asd-sli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Obesity and Autism</title>
      <link>https://christopherdishop.netlify.app/publication/2015-obesity-in-asd-multisite/</link>
      <pubDate>Thu, 03 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2015-obesity-in-asd-multisite/</guid>
      <description>&lt;p&gt;METHODS: Participants were 5053 children with confirmed diagnosis of ASD in the Autism Speaks Autism Treatment Network. Measured values for weight and height were used to calculate BMI percentiles; Centers for Disease Control and Prevention criteria for BMI for gender and age were used to define overweight and obesity (85th and 95th percentiles, respectively).&lt;/p&gt;
&lt;p&gt;RESULTS: In children age 2 to 17 years, 33.6% were overweight and 18% were obese. Compared with a general US population sample, rates of unhealthy weight were significantly higher among children with ASDs ages 2 to 5 years and among those of non-Hispanic white origin. Multivariate analyses revealed that older age, Hispanic or Latino ethnicity, lower parent education levels, and sleep and affective problems were all significant predictors of obesity.&lt;/p&gt;
&lt;p&gt;CONCLUSIONS: Our results indicate that the prevalence of unhealthy weight is significantly greater among children with ASD compared with the general population, with differences present as early as ages 2 to 5 years. Because obesity is more prevalent among older children in the general population, these findings raise the question of whether there are different trajectories of weight gain among children with ASDs, possibly beginning in early childhood.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Memory in language-impaired children with and without autism</title>
      <link>https://christopherdishop.netlify.app/publication/2015-memory-in-li-asd/</link>
      <pubDate>Sun, 14 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2015-memory-in-li-asd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Epidemiology of autism spectrum disorders</title>
      <link>https://christopherdishop.netlify.app/publication/2015-translational/</link>
      <pubDate>Wed, 10 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2015-translational/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aggressive Behavior Problems in Children with Autism Spectrum Disorders: Prevalence and Correlates in a Large Clinical Sample</title>
      <link>https://christopherdishop.netlify.app/publication/2014-aggression-in-asd/</link>
      <pubDate>Sun, 22 Jun 2014 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2014-aggression-in-asd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Epidemiology of autism spectrum disorders</title>
      <link>https://christopherdishop.netlify.app/publication/2014-handbook/</link>
      <pubDate>Mon, 03 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2014-handbook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overweight and Obesity: Prevalence and Correlates in a Large Clinical Sample of Children with Autism Spectrum Disorder</title>
      <link>https://christopherdishop.netlify.app/publication/2014-obesity-in-asd-oregon/</link>
      <pubDate>Sun, 02 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2014-obesity-in-asd-oregon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Epidemiology of Autism Spectrum Disorder</title>
      <link>https://christopherdishop.netlify.app/publication/2014-asd-epidemiology-in-practice/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2014-asd-epidemiology-in-practice/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying Repetitive Speech in Autism Spectrum Disorders and Language Impairment</title>
      <link>https://christopherdishop.netlify.app/publication/2013-repetitive-speech-in-asd-sli/</link>
      <pubDate>Fri, 12 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2013-repetitive-speech-in-asd-sli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>‘It’s Not That We Hate You’: Understanding Children’s Gender Attitudes and Expectancies About Peer Relationships</title>
      <link>https://christopherdishop.netlify.app/publication/2011-gender-attitudes/</link>
      <pubDate>Tue, 08 Feb 2011 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2011-gender-attitudes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of Different Attentional Cues on Responding to Joint Attention in Younger Siblings of Children with Autism Spectrum Disorders</title>
      <link>https://christopherdishop.netlify.app/publication/2007-rja-in-sibs/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2007-rja-in-sibs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Item-specific Processing Reduces False Memories</title>
      <link>https://christopherdishop.netlify.app/publication/2004-false-memories/</link>
      <pubDate>Wed, 01 Dec 2004 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/publication/2004-false-memories/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://christopherdishop.netlify.app/r_resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/r_resources/</guid>
      <description>&lt;h3 id=&#34;websites-with-many-r-resources&#34;&gt;Websites with many R resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://bookdown.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All R books written with Bookdown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All R cheatsheets&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.psychologicalscience.org/observer/learning-to-work-with-r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APS list of R resources&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.flutterbys.com.au/stats/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Show Us Your R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.econometricsbysimulation.com/2012/10/using-simulations-to-maximize.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics Simulations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://data.library.virginia.edu/statlab/data-science-resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UVA Data Science Resources&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;people-with-helpful-individual-websites&#34;&gt;People with helpful individual websites&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily C. Zabor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://jennybryan.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jenny Bryan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://rpsychologist.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kristoffer Magnusson&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://sachaepskamp.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sacha Epskamp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://debruine.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lisa DeBruine&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://data-se.netlify.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FOM academic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://suzan.rbind.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suzan Baert&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://hadley.nz/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hadley Wickham&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://varianceexplained.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Robinson&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://b-rodrigues.github.io/fput/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;B Rodriguez&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://bookdown.org/ndphillips/YaRrr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nathaniel D. Phillips&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sejdemyr.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simon Ejdemyr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.r-graph-gallery.com/all-graphs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The R Graph Gallery&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://christianburkhart.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian Burkhart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://malco.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Malcolm Barrett&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://jflournoy.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Flournoy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://uc-r.github.io/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UC R Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://crumplab.github.io/Courses.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crump Lab&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://djnavarro.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Danielle Navarro&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://datascience.tntlab.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Richard N. Landers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.gastonsanchez.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaston Sanchez&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://quantscience.rbind.io/#posts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark Lai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://joshualoftus.com/page/teaching/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joshua Loftus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://francish.netlify.app/#posts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Francis L. Huang&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;courses&#34;&gt;Courses&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://had.co.nz/stat480.1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Statistical Computing Course&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://biol607.github.io/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BioStates by Jarrett Byrnes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.datacamp.com/community/open-courses/beginning-bayes-in-r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Bayes in DataCamp (free course)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.datacamp.com/community/open-courses/statistical-inference-and-data-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Analysis and Statistical Inference on Data Camp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jrgant/prob-stats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability and Statistics with Simulations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://quantdev.ssri.psu.edu/resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Penn State Quant Developmental Systems&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;random&#34;&gt;Random&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.datadreaming.org/post/r-markdown-theme-gallery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R Markdown Themes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://blogs.uoregon.edu/rclub/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R club&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.zoology.ubc.ca/~schluter/R/fit-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Various Linear Model Tests in R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.statmethods.net/stats/anova.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANOVA and MANOVA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://tclavelle.github.io/blog/r_and_apis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using R to extract data from web APIs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/ColeBeck/datestimes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Handling date-times in R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.brodrigues.co/blog/2017-02-17-lesser_known_tricks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesser Known Dplyr tricks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.stat.cmu.edu/~cshalizi/rmarkdown/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rmarkdown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://rfortherestofus.com/2019/12/how-to-clean-messy-data-in-r/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Packages for extremely messy data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jessiesunpsych/power-simulations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Power Simulations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://stackoverflow.com/questions/47494975/passing-column-name-as-parameter-to-a-function-using-dplyr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Functions with columns as parameters 1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Functions with columns as parameters 2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://haozhu233.github.io/kableExtra/awesome_table_in_html.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kable Extra&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/Getting-started&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data.Table&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://rpsychologist.com/r-guide-longitudinal-lme-lmer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Growth models with lme and lmer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://threadreaderapp.com/thread/1073313224446009345.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cross-lagged Panel Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://debruine.github.io/faux/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simulate Experiments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://rpubs.com/YaRrr/MLTutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximum Likelihood&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jrgant/citr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;citr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://emoriebeck.github.io/R-tutorials/purrr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Purrr tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://osf.io/2emja/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Psych Networks Winter School Resources&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Top 50 ggplot2 Visualizations&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;dags&#34;&gt;DAGS&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jrgant/causalgraphs_latex/blob/master/graphical-models-causal.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dags using LaTex&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jrgant/quickdag&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QuickDag&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://rich-iannone.github.io/DiagrammeR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DiagrammeR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://ggdag.netlify.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GGdag&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sgfin.github.io/2019/06/19/Causal-Inference-Book-All-DAGs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;All DAGS from Hernan and Robins - Sam Finlayson&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;structural-equations-modeling&#34;&gt;Structural Equations Modeling&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://lavaan.ugent.be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lavaan homepage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://groups.google.com/forum/#!topic/lavaan/oKwP0_6-i1g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Measurement Equivalence&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/osofr/simcausal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sims package&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.draw.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Draw SEMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/brandmaier/onyxR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;onyxR for SEM diagrams with lavaan&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;progressing-through-stats-with-r&#34;&gt;Progressing Through Stats with R&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://ismayc.github.io/rbasics-book/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basics of R and R Markdown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://happygitwithr.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basics of Git and Github with R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/sim.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beginner Concepts for Reproducible Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://learningstatisticswithr.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Statistics with R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://r4ds.had.co.nz&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R for Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://homerhanumat.github.io/r-notes/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beginning Computer Science with R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://leanpub.com/LittleInferenceBook/read&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Inference&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://bookdown.org/ejvanholm/Textbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intro to Quant Methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://rafalab.github.io/dsbook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://leanpub.com/regmods/read&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://bookdown.org/ccolonescu/RPoE4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Econometrics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://otexts.org/fpp2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Time series and forecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://nwfsc-timeseries.github.io/atsa-labs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied time series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.economodel.com/time-series-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced time series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://serialmentor.com/dataviz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Visualization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://bookdown.org/content/4857/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical ReThinking with Tidyverse&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.tidytextmining.com/nasa.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Text Analysis&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;scraping-packages&#34;&gt;Scraping Packages&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/mguideng/gdscrapeR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scrape Glass Door&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;commands-and-help-for-non-r-programming&#34;&gt;Commands and help for non-R programming&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.markomedia.com.au/compress-pdf-on-os-x-in-terminal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compress pdf in terminal&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://stackoverflow.com/questions/33636467/unable-to-click-always-allow-on-git-credential-osxkeychain-popup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mac denying pushing/pulling due to git keychain&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://help.github.com/articles/caching-your-github-password-in-git/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saving username and password through keychain with Git&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://blog.martinfenner.org/2014/08/25/using-microsoft-word-with-git/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Using word with Git&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://tex.stackexchange.com/questions/16010/number-every-line-of-pages&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Line numbering in LaTex&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://jabranham.com/blog/2016/11/using-pandoc-export-to-word/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Exporting .tex to word with pandoc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://apple.stackexchange.com/questions/252362/how-do-i-count-words-in-a-pdf-file-on-os-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word Count on PDF From Terminal&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;so-helpful-they-deserve-their-own-category&#34;&gt;So helpful they deserve their own category&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Math notation with Rmarkdown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.verbosus.com/bibtex-style-examples.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BibTex Examples&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://moser-isi.ethz.ch/docs/typeset_equations.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latex equations basics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://tex.stackexchange.com/questions/132704/how-to-build-knitr-document-from-the-command-line&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compile Rmarkdown and tex files from terminal&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://tex.stackexchange.com/questions/171711/how-to-include-latex-package-in-r-markdown&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Include LaTeX packages in YAML header&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://holtzy.github.io/Pimp-my-rmd/#references&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pimp Your Rmarkdown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.rdocumentation.org/packages/rebus/versions/0.1-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rebus Package&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://haozhu233.github.io/kableExtra/best_practice_for_newline_in_latex_table.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Newline breaks in Rmarkdown tables&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://christopherdishop.netlify.app/rec_reading/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/rec_reading/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s what you read when you don&amp;rsquo;t have to that determines who you will be when you can&amp;rsquo;t help it.&amp;rdquo; - Oscar Wilde&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;Quantifying Life: A Symbiosis of Computation, Mathematics, and Biology&lt;/p&gt;
 &lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/quantlife.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dmitry A. Kondrashov&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Quantifying-Life-Symbiosis-Computation-Mathematics/dp/022637176X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1524678360&amp;amp;sr=8-1&amp;amp;keywords=quantifying&amp;#43;life&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;On Writing Well: The Classic Guide to Writing Nonfiction&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/writingwell.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;William Zinsser&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Writing-Well-Classic-Guide-Nonfiction/dp/0060891548/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679014&amp;amp;sr=1-1&amp;amp;keywords=on&amp;#43;writing&amp;#43;well&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;How We Know What Isn&amp;rsquo;t So: The Fallibility of Human Reason in Everyday Life&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/how_we_know.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thomas Gilovich&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/How-Know-What-Isnt-Fallibility/dp/0029117062/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679970&amp;amp;sr=1-3&amp;amp;keywords=how&amp;#43;we&amp;#43;know&amp;#43;what&amp;#43;isn%27t&amp;#43;so&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;Complexity: A Guided Tour&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/complexity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Melanie Mitchell&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Complexity-Guided-Tour-Melanie-Mitchell/dp/0199798109/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678388&amp;amp;sr=1-1&amp;amp;keywords=complexity&amp;#43;a&amp;#43;guided&amp;#43;tour&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;The Drunkard&amp;rsquo;s Walk: How Randomness Rules Our Lives&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/drunkwalk.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Leonard Mlodinow&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Drunkards-Walk-Randomness-Rules-Lives/dp/0307275175/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678442&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;drunkards&amp;#43;walk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;The Order Of Time&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/order_time.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Carlo Rovelli&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Order-Time-Carlo-Rovelli/dp/073521610X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1526429375&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;order&amp;#43;of&amp;#43;time&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;Style: Lessons in Clarity and Grace&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/style.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;John M Williams&lt;/p&gt;
&lt;p&gt;Joseph Bizup&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Style-Lessons-Clarity-Grace-12th/dp/0134080416/ref=sr_1_1?ie=UTF8&amp;amp;qid=1530631885&amp;amp;sr=8-1&amp;amp;keywords=style&amp;#43;lessons&amp;#43;in&amp;#43;grace&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Ethics in the Real World: 82 Brief Essays on Things That Matter
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/ethics_essays.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Peter Singer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Ethics-Real-World-Essays-Things/dp/069117847X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1534110030&amp;amp;sr=8-1&amp;amp;keywords=Peter&amp;#43;singer&amp;#43;82&amp;#43;brief&amp;amp;smid=ATVPDKIKX0DER&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/tasting_tea.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;David Salsburg&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342/ref=sr_1_1?ie=UTF8&amp;amp;qid=1529349371&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;lady&amp;#43;tasting&amp;#43;tea&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Why Don&#39;t Students Like School?
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/school.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Daniel T. Willingham&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Why-Dont-Students-Like-School/dp/047059196X/ref=sr_1_1_sspa?ie=UTF8&amp;amp;qid=1544905242&amp;amp;sr=8-1-spons&amp;amp;keywords=why&amp;#43;don%27t&amp;#43;student&amp;#43;like&amp;#43;school&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
A Crude Look at the Whole: The Science of Complex Systems in Business, Life, and Society
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/crude_whole.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;John H. Miller&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Crude-Look-Whole-Science-Business/dp/0465055699/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679795&amp;amp;sr=1-1&amp;amp;keywords=a&amp;#43;crude&amp;#43;look&amp;#43;at&amp;#43;the&amp;#43;whole&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Digital Minimalism
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/digital_min.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Cal Newport&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Digital-Minimalism-Choosing-Focused-Noisy/dp/0525536515/ref=sr_1_1?keywords=digital&amp;#43;minimalism&amp;amp;qid=1565453149&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Scale: The Universal Laws of Life, Growth, and Death in Organisms, Cities, and Companies
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/scale.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Geoffrey West&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Scale-Universal-Growth-Organisms-Companies/dp/014311090X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1541968528&amp;amp;sr=8-1&amp;amp;keywords=scale&amp;#43;west&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Most Good You Can Do
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/good.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Peter Singer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Most-Good-You-Can-Effective/dp/0300180276/ref=sr_1_1?keywords=the&amp;#43;most&amp;#43;good&amp;#43;you&amp;#43;can&amp;#43;do&amp;amp;qid=1555122359&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Reproducible Research with R and R Studio
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/reproducible_research.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Christopher Gandrud&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Reproducible-Research-Studio-Chapman-Hall/dp/1498715370/ref=sr_1_1?ie=UTF8&amp;amp;qid=1534110087&amp;amp;sr=8-1&amp;amp;keywords=reproducible&amp;#43;research&amp;#43;with&amp;#43;r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Reclaiming Conversation
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/conversation.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Sherry Turkle&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Reclaiming-Conversation-Power-Talk-Digital/dp/0143109790/ref=sr_1_1?keywords=reclaiming&amp;#43;conversation&amp;amp;qid=1565453094&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Stiff: The Curious Lives of Human Cadavers
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/stiff.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mary Roach&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Stiff-Curious-Lives-Human-Cadavers/dp/B0001O356G/ref=sr_1_1?keywords=stiff&amp;amp;qid=1576539098&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Vehicles: Experiments in Synthetic Psychology
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/vehicles.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Valentino Braitenberg&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Vehicles-Experiments-Psychology-Valentino-Braitenberg/dp/0262521121/ref=sr_1_1?keywords=synthetic&amp;#43;psychology&amp;amp;qid=1563232952&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Statistical Rethinking: A Bayesian Course with Examples in R and Stan
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/mcelreath_rethinking.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Richard McElreath&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678418&amp;amp;sr=1-1&amp;amp;keywords=statistical&amp;#43;rethinking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Death of Expertise: The Campaign against Established Knowledge and Why it Matters
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/death_expertise.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thomas M Nichols&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Death-Expertise-Campaign-Established-Knowledge/dp/0190469412/ref=sr_1_1?ie=UTF8&amp;amp;qid=1530631928&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;death&amp;#43;of&amp;#43;expertise&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Book of Why
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/book_why.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Judea Pearl&lt;/p&gt;
&lt;p&gt;Dana Mackenzie&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1527948151&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;book&amp;#43;of&amp;#43;why&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Naked Statistics: Stripping The Dread From The Data
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/nakedstats.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Charles Wheelan&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Naked-Statistics-Stripping-Dread-Data/dp/B00CH7FWWU/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678465&amp;amp;sr=1-1&amp;amp;keywords=naked&amp;#43;statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Ball: Discovering the Object of the Game
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/theball.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;John Fox&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Ball-Discovering-Object-Game/dp/0061881791/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679032&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;ball&amp;#43;discovering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Life You Can Save
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/life_save.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Peter Singer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Life-You-Can-Save-2010-09-14/dp/B01N3MESKP/ref=sr_1_2?keywords=the&amp;#43;life&amp;#43;you&amp;#43;can&amp;#43;save&amp;amp;qid=1555122514&amp;amp;s=gateway&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Nature of Code: Simulating Natural Systems with Processing
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/nature_code.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Daniel Shiffman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Nature-Code-Simulating-Natural-Processing/dp/0985930802/ref=sr_1_1?ie=UTF8&amp;amp;qid=1534109983&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;nature&amp;#43;of&amp;#43;code&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
SuperCooperators: Altruism, Evolution, and Why We Need Each Other to Succeed
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/super_coop.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Martin A. Nowak&lt;/p&gt;
&lt;p&gt;Roger Highfield&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/SuperCooperators-Altruism-Evolution-Other-Succeed/dp/1451626630/ref=sr_1_1?ie=UTF8&amp;amp;qid=1541968376&amp;amp;sr=8-1&amp;amp;keywords=super&amp;#43;cooperators&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Mathematics for the Life Sciences
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/mathlife.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Erin N. Bodine&lt;/p&gt;
&lt;p&gt;Suzanne Lenhart&lt;/p&gt;
&lt;p&gt;Louis J. Gross&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Mathematics-Life-Sciences-Erin-Bodine/dp/0691150729/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678619&amp;amp;sr=1-1&amp;amp;keywords=mathematics&amp;#43;for&amp;#43;the&amp;#43;life&amp;#43;sciences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Nature&#39;s Nether Regions: What the Sex Lives of Bugs, Birds, and Beasts Tell Us About Evolution, Biodiversity, and Ourselves
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/nature_nether.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Menno Schilthuizen&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Natures-Nether-Regions-Schilthuizen-2015-04-28/dp/B01FKSKGY6/ref=sr_1_2?keywords=natures&amp;#43;nether&amp;#43;regions&amp;amp;qid=1569771217&amp;amp;s=gateway&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Algorithms To Live By: The Computer Science Of Human Decisions
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/alglive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Brian Christian&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1250118360/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678670&amp;amp;sr=1-3&amp;amp;keywords=algorithms&amp;#43;to&amp;#43;live&amp;#43;by&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Infinite Powers
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/infinite_powers.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steve Strogatz&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Infinite-Powers-Calculus-Reveals-Universe/dp/1328879984/ref=sr_1_1?keywords=infinite&amp;#43;powers&amp;amp;qid=1563232928&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Coddling of the American Mind
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/coddling.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Greg Lukianoff&lt;/p&gt;
&lt;p&gt;Jonathan Haidt&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Coddling-American-Mind-Intentions-Generation/dp/0735224897&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Invisible Gorilla: How Our Intuitions Deceive Us
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/invisiblegorilla.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Christopher Chabris&lt;/p&gt;
&lt;p&gt;Daniel Simons&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Invisible-Gorilla-How-Intuitions-Deceive/dp/0307459667/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678720&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;invisible&amp;#43;gorilla&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Mindwise: How We Understand What Others Think, Believe, Feel, and Want
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/eplymindwise.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Nicholas Epley&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Mindwise-Understand-Others-Think-Believe/dp/0307595919/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678938&amp;amp;sr=1-1&amp;amp;keywords=mindwise&amp;#43;how&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
This Will Change Everything: Ideas That Will Shape The Future
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/brockmaneverything.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Edited By John Brockman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/This-Will-Change-Everything-Question/dp/0061899674/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678835&amp;amp;sr=1-1&amp;amp;keywords=this&amp;#43;will&amp;#43;change&amp;#43;everything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Sex At Dawn: How We Mate, Why We Stay, and What It Means for Modern Relationships
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/sexdawn.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Christopher Ryan&lt;/p&gt;
&lt;p&gt;Cacilda Jetha&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/SEX-AT-DAWN-STRAY-MEANS/dp/B00KEVTNSK/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678957&amp;amp;sr=1-1&amp;amp;keywords=sex&amp;#43;at&amp;#43;dawn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Can Medicine Be Cured? The Corruption of a Profession
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/cured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Seamus O&amp;rsquo;Mahony&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Can-Medicine-Cured-Corruption-Profession-ebook/dp/B07DKKVBL4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Math with Bad Drawings: Illuminating the Ideas That Shape Our Reality
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/math_draw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ben Orlin&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Math-Bad-Drawings-Illuminating-Reality/dp/0316509035/ref=sr_1_1?ie=UTF8&amp;amp;qid=1541968483&amp;amp;sr=8-1&amp;amp;keywords=mathematics&amp;#43;with&amp;#43;bad&amp;#43;drawings&amp;amp;dpID=51EP1Orpx0L&amp;amp;preST=_SX258_BO1,204,203,200_QL70_&amp;amp;dpSrc=srch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
How We Got to Now: Six Innovations That Made the Modern World
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/now.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steven Johnson&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/How-We-Got-Now-Innovations-ebook/dp/B00INIXU5I/ref=sr_1_1?ie=UTF8&amp;amp;qid=1541968591&amp;amp;sr=8-1&amp;amp;keywords=how&amp;#43;we&amp;#43;got&amp;#43;to&amp;#43;now&amp;#43;johnson&amp;amp;dpID=512mgCAgJQL&amp;amp;preST=_SY445_QL70_&amp;amp;dpSrc=srch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Made to Stick: Why Some Ideas Survive and Others Die
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/stick.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chip Heath&lt;/p&gt;
&lt;p&gt;Dan Heath&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Made-Stick-Ideas-Survive-Others/dp/1400064287/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678997&amp;amp;sr=1-1&amp;amp;keywords=made&amp;#43;to&amp;#43;stick&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Ravenous Brain: How The New Science of Consciousness Explains Our Insatiable Search For Meaning
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/ravenousbrain.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Daniel Bor&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Ravenous-Brain-Consciousness-Explains-Insatiable/dp/046502047X/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679052&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;ravenous&amp;#43;brain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Matrices and Society: Matrix Algebra and Its Applications in the Social Sciences
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/matrices_society.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ian Bradley&lt;/p&gt;
&lt;p&gt;Ronald L. Meek&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Matrices-Society-Applications-Sciences-Princeton/dp/0691610207/ref=sr_1_2?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679070&amp;amp;sr=1-2&amp;amp;keywords=matrices&amp;#43;and&amp;#43;society&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Beginning Of Infinity: Explanations That Transform The World
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/infinity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;David Deutsch&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Beginning-Infinity-Explanations-Transform-World/dp/0143121359/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679088&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;beginning&amp;#43;of&amp;#43;infinity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Visual Display of Quantitative Information
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/visual_display.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Edward R. Tufte&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Visual-Display-Quantitative-Information/dp/B00A2RSEQI/ref=sr_1_2?ie=UTF8&amp;amp;qid=1535220474&amp;amp;sr=8-2&amp;amp;keywords=the&amp;#43;visual&amp;#43;display&amp;#43;of&amp;#43;quantitative&amp;#43;information&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Complex Adaptive Systems: An Introduction to Computational Models of Social Life
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/adaptive.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;John H. Miller&lt;/p&gt;
&lt;p&gt;Scott E. Page&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Complex-Adaptive-Systems-Introduction-Computational/dp/0691127026/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678589&amp;amp;sr=1-1&amp;amp;keywords=complex&amp;#43;adaptive&amp;#43;systems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Homo Deus: A Brief History of Tomorrow
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/homodeus.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Yuval Noah Harari&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Homo-Deus-Brief-History-Tomorrow/dp/0062464310/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679148&amp;amp;sr=1-1&amp;amp;keywords=homo&amp;#43;deus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Moral Landscape: How Science Can Determine Human Values
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/morallandscape.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Sam Harris&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Moral-Landscape-Science-Determine-Values/dp/143917122X/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679169&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;moral&amp;#43;landscape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Sick Societies: Challenging the Myth of Primitive Harmony
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/sick_society.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Robert B. Edgerton&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Sick-Societies-Challenging-Primitive-Harmony/dp/0029089255/ref=sr_1_1?ie=UTF8&amp;amp;qid=1527948228&amp;amp;sr=8-1&amp;amp;keywords=sick&amp;#43;societies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Expanding Circle: Ethics, Evolution, and Moral Progress
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/expanding_circle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Peter Singer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Expanding-Circle-Ethics-Evolution-Progress/dp/0691150699/ref=sr_1_1?keywords=the&amp;#43;expanding&amp;#43;circle&amp;amp;qid=1555122466&amp;amp;s=gateway&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Checklist Manifesto: How to Get Things Right
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/checklist.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Atul Gawande&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Checklist-Manifesto-How-Things-Right/dp/0312430000/ref=sr_1_1?ie=UTF8&amp;amp;qid=1547762703&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;checklist&amp;#43;manifesto&amp;#43;by&amp;#43;atul&amp;#43;gawande&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Discrimination and Disparities
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/disparity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Thomas Sowell&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Discrimination-Disparities-Thomas-Sowell/dp/154164560X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Joy of x: A Guided Tour of Math, from One to Infinity
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/joy_x.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steven Strogatz&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Joy-Guided-Tour-Math-Infinity/dp/0544105850/ref=sr_1_1?ie=UTF8&amp;amp;qid=1547762739&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;joy&amp;#43;of&amp;#43;x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
10 1/2 Things No Commencement Speaker Has Ever Said
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/no_speaker.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Charles Wheelan&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Things-Commencement-Speaker-Ever-Said/dp/B00CL4IEAG/ref=sr_1_1?ie=UTF8&amp;amp;qid=1547762768&amp;amp;sr=8-1&amp;amp;keywords=10&amp;#43;1%2F2&amp;#43;things&amp;#43;no&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Reflections on the Human Condition
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/reflections.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Eric Hoffer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Reflections-Human-Condition-Eric-Hoffer/dp/1933435143/ref=sr_1_2?dchild=1&amp;amp;keywords=eric&amp;#43;hoffer&amp;#43;reflections&amp;#43;of&amp;#43;the&amp;#43;human&amp;amp;qid=1598629438&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Thinking with Data: How to Turn Information into Insights
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/think_data.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Max Shron&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Thinking-Data-Turn-Information-Insights/dp/1449362931/ref=sr_1_1?ie=UTF8&amp;amp;qid=1547762669&amp;amp;sr=8-1&amp;amp;keywords=thinking&amp;#43;with&amp;#43;data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Structural Equations With Latent Variables
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/SEM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Kenneth A. Bollen&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Structural-Equations-Latent-Variables-Kenneth/dp/0471011711/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679315&amp;amp;sr=1-1&amp;amp;keywords=structural&amp;#43;equations&amp;#43;latent&amp;#43;variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Switch: How To Change Things When Change Is Hard
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/switch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chip Heath&lt;/p&gt;
&lt;p&gt;Dan Heath&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Switch-Change-Things-Heath-Hardcover/dp/B010IKJIYA/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679336&amp;amp;sr=1-3&amp;amp;keywords=switch&amp;#43;how&amp;#43;to&amp;#43;change&amp;#43;when&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Mathematical Modeling of Social Relationships
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/math_relationships.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Urszula Strawinska-Zanko&lt;/p&gt;
&lt;p&gt;Larry S. Liebovitch&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Mathematical-Modeling-Social-Relationships-Computational/dp/331976764X/ref=sr_1_1?ie=UTF8&amp;amp;qid=1544905847&amp;amp;sr=8-1&amp;amp;keywords=mathematical&amp;#43;modeling&amp;#43;of&amp;#43;social&amp;#43;relationships&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Seven Pillars of Statistical Wisdom
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/sevenstats.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Stephen M. Stigler&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Seven-Pillars-Statistical-Wisdom/dp/0674088913/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679356&amp;amp;sr=1-1&amp;amp;keywords=the&amp;#43;seven&amp;#43;pillars&amp;#43;of&amp;#43;statistical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Life&#39;s Other Secret: The New Mathematics of the Living World
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/lifes_secret.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ian Stewart&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Lifes-Other-Secret-Mathematics-Living/dp/0471296511/ref=sr_1_1?ie=UTF8&amp;amp;qid=1541968410&amp;amp;sr=8-1&amp;amp;keywords=life%27s&amp;#43;other&amp;#43;secret&amp;#43;ian&amp;amp;dpID=31v8-5eNf-L&amp;amp;preST=_SY291_BO1,204,203,200_QL40_&amp;amp;dpSrc=srch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
What Should We Be Worried About? Real Scenarios That Keep Scientists Up at Night
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/brockmanworried.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Edited by John Brockman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/What-Should-Worried-About-Scientists/dp/006229623X/ref=sr_1_2?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679401&amp;amp;sr=1-2&amp;amp;keywords=what&amp;#43;should&amp;#43;we&amp;#43;be&amp;#43;worried&amp;#43;about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Introduction To Scientific Programming and Simulation Using R
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/rsimprog.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Owen Jones&lt;/p&gt;
&lt;p&gt;Robert Maillardet&lt;/p&gt;
&lt;p&gt;Andrew Robinson&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Introduction-Scientific-Programming-Simulation-Chapman/dp/1466569999/ref=sr_1_3?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678645&amp;amp;sr=1-3&amp;amp;keywords=introduction&amp;#43;to&amp;#43;scientific&amp;#43;programming&amp;#43;and&amp;#43;simulation&amp;#43;using&amp;#43;r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Stumbling on Happiness
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/gilberthappy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Daniel Gilbert&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Stumbling-Happiness-Daniel-Gilbert/dp/1400077427/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679419&amp;amp;sr=1-1&amp;amp;keywords=stumbling&amp;#43;on&amp;#43;happiness&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Brain Bugs: How The Brain&#39;s Flaws Shape Our Lives
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/brainbugs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dean Buonomano&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Brain-Bugs-Brains-Flaws-Shape/dp/0393342220/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679522&amp;amp;sr=1-1&amp;amp;keywords=brain&amp;#43;bugs&amp;#43;how&amp;#43;the&amp;#43;brain%27s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Quantitative Social Science: An Introduction
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/quant_ss.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Kosuke Imai&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Quantitative-Social-Science-Kosuke-Imai/dp/0691175462/ref=sr_1_1?ie=UTF8&amp;amp;qid=1530631848&amp;amp;sr=8-1&amp;amp;keywords=quantitative&amp;#43;social&amp;#43;science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Sync
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/sync.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Steven H. Strogatz&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Sync-Order-Emerges-Universe-Nature-ebook/dp/B0072M0X2Y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
This Idea Is Brilliant
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/brilliant.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Edited by John Brockman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/This-Idea-Brilliant-Overlooked-Underappreciated/dp/0062698214&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
How to Do Nothing: Resisting the Attention Economy
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/do_nothing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Jenny Odell&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/How-Do-Nothing-Resisting-Attention/dp/1612197493/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;amp;qid=1576539107&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
How Not to Be Wrong: The Power of Mathematical Thinking
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/not_wrong.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Jordan Ellenberg&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/How-Not-Be-Wrong-Mathematical/dp/0143127535/ref=sr_1_2?ie=UTF8&amp;amp;qid=1544905443&amp;amp;sr=8-2&amp;amp;keywords=how&amp;#43;not&amp;#43;to&amp;#43;be&amp;#43;wrong&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Introduction to Computation and Programming Using Python
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/computation_programming.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;John V. Guttag&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/gp/product/0262529629/ref=oh_aui_detailpage_o03_s00?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Cooperation Under Anarchy
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/cooperation_anarchy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Kenneth A. Oye&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Cooperation-under-Anarchy-Kenneth-Oye/dp/0691022402/ref=sr_1_1?ie=UTF8&amp;amp;qid=1534110079&amp;amp;sr=8-1&amp;amp;keywords=cooperation&amp;#43;under&amp;#43;anarchy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Predictably Irrational: The Hidden Forces That Shape Our Decisions
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/irrational.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dan Ariely&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Predictably-Irrational-Revised-Expanded-Decisions/dp/0061353248/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678490&amp;amp;sr=1-1&amp;amp;keywords=predictably&amp;#43;irrational&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Passionate State of Mind
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/passionate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Eric Hoffer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Passionate-State-Mind-Other-Aphorisms/dp/1933435097/ref=sr_1_2?dchild=1&amp;amp;keywords=hoffer&amp;#43;passionate&amp;#43;state&amp;#43;of&amp;#43;mind&amp;amp;qid=1598629512&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Big Data: A Revolution That Will Transform How We Live, Work, and Think
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/bigdata.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Viktor Mayer-Schonberger&lt;/p&gt;
&lt;p&gt;Kenneth Cukier&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Big-Data-Revolution-Transform-Think/dp/1848547900/ref=sr_1_10?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524678890&amp;amp;sr=1-10&amp;amp;keywords=big&amp;#43;data&amp;#43;a&amp;#43;revolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
An Introduction to Models in the Social Sciences
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/socialmodels.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Charles A. Lave&lt;/p&gt;
&lt;p&gt;James G. March&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Introduction-Models-Social-Sciences/dp/0819183814&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
In Pursuit of the Unknown: 17 Equations That Changed the World
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/unknown.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ian Stewart&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Pursuit-Equations-That-Changed-World/dp/0465085989/ref=sr_1_1?ie=UTF8&amp;amp;qid=1529323656&amp;amp;sr=8-1&amp;amp;keywords=17&amp;#43;equations&amp;#43;that&amp;#43;changed&amp;#43;the&amp;#43;world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Sapiens: A Brief History of Humankind
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/sapiens.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Yuval Noah Harari&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Sapiens-Humankind-Yuval-Noah-Harari/dp/0062316095/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;amp;qid=1599076931&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Greatest Minds and Ideas of All Time
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/greatest.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Will Durant&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Greatest-Minds-Ideas-All-Time/dp/0743235533/ref=sr_1_2?dchild=1&amp;amp;keywords=will&amp;#43;durant&amp;#43;thinkers&amp;amp;qid=1599076729&amp;amp;sr=8-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
This Blinding Absence of Light
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/blinding.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Tahar Ben Jelloun&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Blinding-Absence-Light-Tahar-Jelloun/dp/014303572X/ref=sr_1_1?crid=B2T2Y3JODDGK&amp;amp;dchild=1&amp;amp;keywords=this&amp;#43;blinding&amp;#43;absence&amp;#43;of&amp;#43;light&amp;amp;qid=1599076711&amp;amp;sprefix=this&amp;#43;blinding&amp;#43;ab%2Caps%2C145&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Working and Thinking on the Waterfront
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/waterfront.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Eric Hoffer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Working-Thinking-Waterfront-Eric-Hoffer/dp/1933435291/ref=sr_1_1?dchild=1&amp;amp;keywords=working&amp;#43;and&amp;#43;thinking&amp;#43;on&amp;#43;the&amp;#43;waterfront&amp;amp;qid=1599076923&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Art of Thinking Clearly
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/thinkclearly.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Rolf Dobelli&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Art-Thinking-Clearly-Rolf-Dobelli-ebook/dp/B00IZP6EVQ/ref=sr_1_3?dchild=1&amp;amp;keywords=the&amp;#43;art&amp;#43;of&amp;#43;thinking&amp;#43;clearly&amp;amp;qid=1599077205&amp;amp;sr=8-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Retire Before Mom and Dad: The Simple Numbers Behind A Lifetime of Financial Freedom
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/retire.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Rob Berger&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Retire-Before-Mom-Dad-Financial/dp/1733914501/ref=sr_1_1_sspa?dchild=1&amp;amp;keywords=retire&amp;#43;before&amp;#43;mom&amp;#43;and&amp;#43;dad&amp;amp;qid=1599077168&amp;amp;sr=8-1-spons&amp;amp;psc=1&amp;amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEySUlMWExPS1FBWVZQJmVuY3J5cHRlZElkPUEwNjgzNjQzMlExSFI0UThYV0FJUCZlbmNyeXB0ZWRBZElkPUExMDIyNzYyMjZJWkJTNUpRQjhPVCZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
7 Tipping Points That Saved the World
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/saved.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chris Stewart&lt;/p&gt;
&lt;p&gt;Ted Stewart&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Tipping-Points-That-Saved-World/dp/160641951X/ref=sr_1_1?dchild=1&amp;amp;keywords=7&amp;#43;tipping&amp;#43;points&amp;#43;that&amp;#43;saved&amp;#43;the&amp;#43;world&amp;amp;qid=1599076908&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Lessons of History
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/history.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Will Durant&lt;/p&gt;
&lt;p&gt;Ariel Durant&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Lessons-History-Will-Durant/dp/143914995X/ref=sr_1_1?dchild=1&amp;amp;keywords=will&amp;#43;durant&amp;#43;thinkers&amp;amp;qid=1599076729&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Humble Pie: A Comedy of Maths Errors
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/humble_pie.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Matt Parker&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Humble-Pi-Comedy-Maths-Errors/dp/0241360234/ref=sr_1_fkmr0_1?keywords=humble&amp;#43;pie&amp;#43;maths&amp;#43;errors&amp;amp;qid=1565453048&amp;amp;s=gateway&amp;amp;sr=8-1-fkmr0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Tyranny of Metrics
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/tyranny_metrics.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Jerry Muller&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Tyranny-Metrics-Jerry-Muller/dp/0691174954/ref=sr_1_1?ie=UTF8&amp;amp;qid=1529323708&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;tyranny&amp;#43;of&amp;#43;metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Model Thinker: What You Need to Know to Make Data Work for You
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/manymodel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Scott E. Page&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Model-Thinker-What-Need-Know/dp/0465094627&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
The Briefest History of Time
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/history_time.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Arieh Ben_Naim&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Briefest-History-Time-Misconstrued-Association/dp/9814749850/ref=sr_1_1?ie=UTF8&amp;amp;qid=1534110135&amp;amp;sr=8-1&amp;amp;keywords=the&amp;#43;briefest&amp;#43;history&amp;#43;of&amp;#43;time&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Too Soon Old, Too Late Smart
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/too_soon_old.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Gordon Livingston&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Too-Soon-Old-Late-Smart/dp/1569243735/ref=sr_1_1?ie=UTF8&amp;amp;qid=1527948162&amp;amp;sr=8-1&amp;amp;keywords=too&amp;#43;soon&amp;#43;old&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
System Dynamics Modeling with R
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/system_dynamics.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Jim Duggan&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/System-Dynamics-Modeling-Lecture-Networks/dp/3319340417/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1526429142&amp;amp;sr=1-1&amp;amp;keywords=system&amp;#43;dynamics&amp;#43;modeling&amp;#43;with&amp;#43;r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Ten Great Ideas About Chance
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/tenchance.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Persi Diaconis&lt;/p&gt;
&lt;p&gt;Brian Skyrms&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Ten-Great-Ideas-about-Chance/dp/0691174164/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679586&amp;amp;sr=1-1&amp;amp;keywords=ten&amp;#43;great&amp;#43;ideas&amp;#43;about&amp;#43;chance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Risk Savvy: How To Make Good Decisions
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/risk_savvy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Gerd Gigerenzer&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Risk-Savvy-Make-Good-Decisions/dp/0143127101/ref=sr_1_1?ie=UTF8&amp;amp;qid=1525549444&amp;amp;sr=8-1&amp;amp;keywords=risk&amp;#43;savvy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
Thinking in Systems: A Primer
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/thinking_systems.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Donella H. Meadows&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Thinking-Systems-Donella-H-Meadows/dp/1603580557/ref=sr_1_2?ie=UTF8&amp;amp;qid=1524678135&amp;amp;sr=8-2&amp;amp;keywords=thinking&amp;#43;in&amp;#43;systems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
This Idea Must Die: Scientific Theories That Are Blocking Progress
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/Cdishop/website/raw/master/misc/books/brockmandie.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Edited by John Brockman&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/This-Idea-Must-Die-Scientific/dp/0062374346/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1524679604&amp;amp;sr=1-1&amp;amp;keywords=this&amp;#43;idea&amp;#43;must&amp;#43;die&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for link&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>https://christopherdishop.netlify.app/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://christopherdishop.netlify.app/cv/</guid>
      <description>&lt;p&gt;Download this file 
&lt;a href=&#34;https://raw.githubusercontent.com/Cdishop/homepage/master/misc/CV.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;object data=&#34;https://raw.githubusercontent.com/Cdishop/website/master/misc/CV.pdf&#34; width=&#34;800px&#34; height=&#34;700px&#34;&gt;
    &lt;embed src=&#34;https://raw.githubusercontent.com/Cdishop/website/master/misc/CV.pdf&#34;&gt;
        &lt;p&gt;This browser does not support PDFs. Please download the PDF to view it: &lt;a href=&#34;https://raw.githubusercontent.com/Cdishop/website/master/misc/CV.pdf&#34;&gt;Download PDF&lt;/a&gt;.&lt;/p&gt;
    &lt;/embed&gt;
&lt;/object&gt;</description>
    </item>
    
  </channel>
</rss>
