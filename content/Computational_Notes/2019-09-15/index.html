---
title: "Meta Analysis Comps Notes"
Date: "2019-09-15"
summary: "-----"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>I’m taking comps pretty soon so this is my summary document regarding meta-analysis.</p>
<p>MAs give us an average estimate across settings, tests, and people after correcting for noise. In a bare-bones MA, we correct only for sampling error. In a full MA, we correct for sampling error, unreliability, and range restriction. I’ll demonstrate a full MA here where we assume direct (rather than indirect) range restriction.</p>
<div id="steps" class="section level1">
<h1>Steps</h1>
<ol style="list-style-type: decimal">
<li><p>Literature Review</p>
<ul>
<li>Create inclusion criteria for how you are going to select studies</li>
<li>Find relevant articles</li>
</ul></li>
<li><p>Code Articles</p>
<ul>
<li>Measures</li>
<li>Reliability</li>
<li>SD</li>
<li>Means</li>
<li>Effect sizes</li>
<li>Moderators</li>
</ul></li>
<li><p>Calculate Meta-Analytic Estimate</p>
<ul>
<li>Calculate meta-analytic effect size</li>
<li>Calculate its variance</li>
</ul></li>
</ol>
<p>In this post, I’m focusing only on step 3, the calculations, even though steps 1 and 2 are arguably the more important pieces.</p>
<div id="calculating-ma-estimate-and-variance" class="section level3">
<h3>Calculating MA Estimate and Variance</h3>
<p>Within this step, there are many substeps:</p>
<ul>
<li><p>Calculate the MA effect estimate (typically from correlations or cohen’s d’s).</p>
<p>Within each study gathered from our literature review…</p>
<ol style="list-style-type: decimal">
<li><p>Correct the observed correlation for range restriction, which produces a rr-corrected correlation</p></li>
<li><p>Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity</p></li>
<li><p>Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation</p></li>
</ol></li>
<li><p>Calculate the variance in our MA effect estimate</p>
<p>Within each study gathered from our literature review…</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Compute a correction factor for unreliability on X</p></li>
<li><p>Compute a correction factor for unreliability on Y</p></li>
<li><p>Compute a correction factor for range restriction</p></li>
<li><p>Combine all of those together</p></li>
<li><p>Compute the error variance for a given observed correlation and correct it using the combined correction factor. This step produces the sampling error correction</p></li>
<li><p>Calculate the average sampling error from the sampling error corrections</p></li>
<li><p>Calculate the observed error variance</p></li>
<li><p>The MA variance estimate is equal to the observed error variance - the average sampling error</p></li>
</ol></li>
</ul>
<p>Before we begin, here is a peak at the (mock) data set. I reviewed four studies and compiled their observed effect sizes – in this case we’re going to use correlations. Let’s say that our IV is dancing ability and our DV is life satisfaction, both are continuous variables. We are interested in the meta-analytic correlation between dancing ability and life satisfaction.</p>
<pre><code>##   study restricted_predictor_sd unrestricted_predictor_sd predictor_reliability
## 1     1                      14                        20                  0.94
## 2     2                      13                        20                  0.73
## 3     3                      16                        20                  0.82
## 4     4                      18                        20                  0.75
##   criterion_reliability sample_size observed_correlation
## 1                  0.75          50                 0.32
## 2                  0.80         100                 0.10
## 3                  0.83         125                 0.25
## 4                  0.94         240                 0.40</code></pre>
<ul>
<li>Study = an ID number for each study in my meta-analysis</li>
<li>Restricted SD = the standard deviation of scores on dancing ability within the study</li>
<li>Unrestricted SD = the standard deviation of scores on on dancing ability across a larger population – from a manual, prior studies, known SDs, field reports, etc.</li>
<li>Predictor reliability = the reliability of the measure used to assess dancing ability within the study</li>
<li>Criterion reliability = the reliability of the measure used to assess life satisfaction within the study</li>
<li>Sample size = how many people were observed within the study</li>
<li>Observed correlation = the correlation between dancing ability and life satisfaction within the study</li>
</ul>
</div>
<div id="a-calculate-the-ma-correlation" class="section level3">
<h3>a) Calculate the MA correlation</h3>
<p>For each study gathered from our literature review…</p>
<div id="section" class="section level4">
<h4>1)</h4>
<p>Correct the observed correlation for range restriction, which produces a rr-corrected correlation</p>
<p><span class="math display">\[\begin{equation}
r_{RR} = \dfrac{  \left(\dfrac{US_{x}}{RS_{x}}\right)r_{xy} } {\sqrt{1 + r^2_{xy}\left(\dfrac{US^2_{x}}{RS^2_{x}} - 1\right)} }
\end{equation}\]</span></p>
<p>where <span class="math inline">\(r_{RR}\)</span> is the correlation that is corrected for range restriction, <span class="math inline">\(US_x\)</span> is the unrestricted SD on dancing ability, <span class="math inline">\(RS_x\)</span> is the restricted SD on dancing ability, and <span class="math inline">\(r_{xy}\)</span> is the correlation between dancing ability and life satisfaction. We are going to compute <span class="math inline">\(r_{RR}\)</span> for every study.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(r_RR = 
           ((unrestricted_predictor_sd / restricted_predictor_sd)*observed_correlation) / sqrt(
             
             1 + ((observed_correlation^2) * ((unrestricted_predictor_sd / restricted_predictor_sd) -1))    
             
           )
         
  )</code></pre>
</div>
<div id="section-1" class="section level4">
<h4>2)</h4>
<p>Use the rr-corrected correlation along with the criterion reliability to correct for unreliability, which produces operational validity</p>
<p><span class="math display">\[\begin{equation}
r_{ov} = \dfrac{r_{RR}}{\sqrt{r_{yy}}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(r_{ov}\)</span> is the operational validity of dancing ability and life satisfaction, <span class="math inline">\(r_{RR}\)</span> is the correlation we calculated in step 1 (the range-restriction-corrected correlation) between dancing ability and life satisfaction, and <span class="math inline">\(r_{yy}\)</span> is the reliability of the criterion, life satisfaction.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(r_ov = 
           r_RR / sqrt(criterion_reliability))</code></pre>
</div>
<div id="section-2" class="section level4">
<h4>3)</h4>
<p>Then, use the operational validities along with sample sizes to correct for sampling error and produce a sample-size-weighted meta-analytic correlation</p>
<p><span class="math display">\[\begin{equation}
\rho = \dfrac{\sum{w_sr_{ov_{i}}}}{\sum{w_s}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the meta-analytic estimate, <span class="math inline">\(r_{ov_{i}}\)</span> is the operational validity between dancing ability and life satisfaction for each study, and <span class="math inline">\(w_s\)</span> is the sample size for each study.</p>
<pre class="r"><code>ovs_by_sample_size &lt;- df$sample_size * df$r_ov
ma_correlation &lt;- sum(ovs_by_sample_size) / sum(df$sample_size)

df &lt;- df %&gt;%
  mutate(ma_correlation = ma_correlation)</code></pre>
</div>
</div>
<div id="b-calculate-the-variance-in-our-ma-effect-estimate" class="section level3">
<h3>b) Calculate the variance in our MA effect estimate</h3>
<p>Compile all of the corrections – steps 4 through 7</p>
<div id="section-3" class="section level4">
<h4>4)</h4>
<p>Compute the correction factor for unreliability on X, dancing ability (take the square root of the reliability)</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(cf_x = sqrt(predictor_reliability))</code></pre>
</div>
<div id="section-4" class="section level4">
<h4>5)</h4>
<p>Compute the correction factor for unreliability on Y, life satisfaction (take the square root of the reliability)</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(cf_y = sqrt(criterion_reliability))</code></pre>
</div>
<div id="section-5" class="section level4">
<h4>6)</h4>
<p>Compute the correction factor for range restriction</p>
<p><span class="math display">\[\begin{equation}
a_{rr} = \dfrac{1}{ \left(\left(\dfrac{US_x}{RS_x}\right)^2 - 1\right)r_{xy}^2 + 1}
\end{equation}\]</span></p>
<p>where all terms are defined above.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(cf_rr = 1 / 
          (  ((unrestricted_predictor_sd / restricted_predictor_sd)^2 - 1)*(observed_correlation^2) + 1 )
         )</code></pre>
</div>
<div id="section-6" class="section level4">
<h4>7)</h4>
<p>Combine all of those correction factors together into one common correction factor, <span class="math inline">\(A\)</span>.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(A = cf_x*cf_y*cf_rr)</code></pre>
</div>
<div id="section-7" class="section level4">
<h4>8)</h4>
<p>Compute the error variance for a given observed correlation and correct it using the combined correction factor.</p>
<p>This part takes three steps.</p>
<pre><code>I: Compute the sample size weighted observed correlation

    - Essentially the same thing as step 3 but using observed correlations rather than operational validities
    </code></pre>
<p><span class="math display">\[\begin{equation}
r_{wa} = \dfrac{\sum{w_sr_{xy_{i}}}}{\sum{w_s}}
\end{equation}\]</span></p>
<pre class="r"><code>ss_times_correlations &lt;- df$sample_size*df$observed_correlation
wa_correlation &lt;- sum(ss_times_correlations) / sum(df$sample_size)</code></pre>
<pre><code>II: Compute the error variance on the observed correlation for each study</code></pre>
<p><span class="math display">\[\begin{equation}
\sigma^2_e = \dfrac{\left(1-r_{wa}^2\right)^2}{N-1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\sigma^2_e\)</span> is the error variance (for each study), <span class="math inline">\(r_{wa}\)</span> is the weighted average observed correlation between dancing ability and life satisfaction that we computed above, and <span class="math inline">\(N\)</span> is the sample size.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(sigma2e = 
           ((1 - wa_correlation^2)^2) / (sample_size - 1)
  )</code></pre>
<pre><code>III: Compute the sampling error correction for each study</code></pre>
<p><span class="math display">\[\begin{equation}
Var_{ec} = \dfrac{\sigma^2_e}{A^2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(Var_{ec}\)</span> is the sampling error correction, <span class="math inline">\(\sigma^2_e\)</span> is what we just calculated above, the error variance on the observed correlation for each study, and <span class="math inline">\(A\)</span> is the combined correction factor for each study.</p>
<pre class="r"><code>df &lt;- df %&gt;%
  mutate(var_ec = sigma2e^2 / A^2)</code></pre>
</div>
<div id="section-8" class="section level4">
<h4>9)</h4>
<p>Calculate the average sampling error</p>
<p><span class="math display">\[\begin{equation}
Ave_{var_{ec}} = \dfrac{\sum{w_sVar_{ec}}}{\sum{w_s}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(Ave_{var_{ec}}\)</span> is the average sampling error, <span class="math inline">\(w_s\)</span> is the sample size for each study, and <span class="math inline">\(Var_{ec}\)</span> is the sampling error correction for each individual study.</p>
<pre class="r"><code>ss_times_varec &lt;- df$sample_size*df$var_ec
ave_var_ec &lt;- sum(ss_times_varec) / sum(df$sample_size)</code></pre>
</div>
<div id="section-9" class="section level4">
<h4>10)</h4>
<p>Calculate the observed error variance</p>
<p><span class="math display">\[\begin{equation}
var_r = \dfrac{\sum{w_s\left(r_{xy} - r_{ov}\right)^2}}{\sum{w_s}}
\end{equation}\]</span></p>
<p>where all terms are defined above.</p>
<pre class="r"><code>ss_times_r_minus_ov &lt;- df$sample_size*((df$observed_correlation - df$r_ov)^2)
var_r &lt;- sum(ss_times_r_minus_ov) / sum(df$sample_size)</code></pre>
</div>
<div id="section-10" class="section level4">
<h4>11)</h4>
<p>The MA variance estimate is equal to the observed error variance - the average sampling error</p>
<p><span class="math display">\[\begin{equation}
Var_p = var_r - Ave_{var_{ec}}
\end{equation}\]</span></p>
<pre class="r"><code>var_p &lt;- var_r - ave_var_ec</code></pre>
</div>
</div>
<div id="recap" class="section level3">
<h3>Recap</h3>
<p>What a nightmare. Here’s a recap:</p>
<ul>
<li><p>Correct the observed correlations for unreliability and range restriction, use them to compute a sample-size-weighted MA correlation coefficient</p></li>
<li><p>Make a bunch of corrections and compute the average sampling error, and subtract that from the observed variance of the correlation coefficient to get a sense for the MA correlation coefficient variance</p></li>
</ul>
<p>Now we can calculate credibility and confidence intervals.</p>
</div>
<div id="credibility-interval" class="section level3">
<h3>Credibility Interval</h3>
<p>Gives us a sense for whether or not moderators are at play.</p>
<p><span class="math display">\[\begin{equation}
\textrm{95 credibility interval} = \rho +- 1.96*\sqrt{Var_p}
\end{equation}\]</span></p>
<pre class="r"><code>upper_cred_i = ma_correlation + (1.96 * sqrt(var_p))
lower_cred_i = ma_correlation - (1.96 * sqrt(var_p))
upper_cred_i</code></pre>
<pre><code>## [1] 0.5532591</code></pre>
<pre class="r"><code>lower_cred_i</code></pre>
<pre><code>## [1] 0.2024129</code></pre>
<p>Credibility Ratio: if <span class="math inline">\(\dfrac{ave_{var_{ec}}}{var_{r}}\)</span> is lower than 0.75, then moderators may be at play.</p>
<pre class="r"><code>ave_var_ec / var_r</code></pre>
<pre><code>## [1] 0.01211969</code></pre>
</div>
<div id="confidence-interval" class="section level3">
<h3>Confidence Interval</h3>
<p><span class="math display">\[\begin{equation}
\textrm{95 confidence interval} = r_{ov} +- 1.96*SE_{r_{ov}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(SE_{r_{ov}}\)</span> is the standard error of the operational validities and is calculated as…</p>
<p><span class="math display">\[\begin{equation}
SE_{r_{ov}} = \dfrac{SD_{r_{ov}}}{\sqrt{k}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the number of studies.</p>
<pre class="r"><code>se_r_ov = sd(df$r_ov) / sqrt(length(df$study))

upper_ci = ma_correlation + (1.96 * se_r_ov)
lower_ci = ma_correlation - (1.96 * se_r_ov)
upper_ci</code></pre>
<pre><code>## [1] 0.5263396</code></pre>
<pre class="r"><code>lower_ci</code></pre>
<pre><code>## [1] 0.2293324</code></pre>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
</div>
</div>
