---
title: "Regression Creates Weighted Linear Composites"
date: "2019-12-01"
summary: "-----"
---



<p>One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.</p>
<p>Here’s a simulation to punch that point home.</p>
<p>500 people.</p>
<pre class="r"><code>N &lt;- 500</code></pre>
<p>The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.</p>
<pre class="r"><code>sigma &lt;- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)</code></pre>
<p>The mean for each variable is 0.</p>
<pre class="r"><code>mu &lt;- c(0,0,0)</code></pre>
<p>Use the correlation matrix and mean specifications to generate data.</p>
<pre class="r"><code>library(MASS)

df &lt;- mvrnorm(N, mu, sigma)</code></pre>
<p>Turn it into a data frame and label it.</p>
<pre class="r"><code>df &lt;- data.frame(df)
names(df) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;y&#39;)
df$id &lt;- c(1:N)</code></pre>
<p>Run regression and print the output.</p>
<pre class="r"><code>summary(lm(y ~ x1 + x2,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.05335 -0.55914 -0.00111  0.56756  2.40146 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.005233   0.037239  -0.141    0.888    
## x1           0.325124   0.037685   8.627  &lt; 2e-16 ***
## x2           0.294367   0.037293   7.893 1.89e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8322 on 497 degrees of freedom
## Multiple R-squared:  0.235,  Adjusted R-squared:  0.2319 
## F-statistic: 76.33 on 2 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.</p>
<p>Create a composite using the regression weights.</p>
<pre class="r"><code>library(tidyverse)
df &lt;- df %&gt;%
  mutate(composite_x = 0.33*x1 + 0.4*x2)</code></pre>
<p>Those weights provide the maximum correlation between our composite and the outcome.</p>
<pre class="r"><code>cor(df$y, df$composite_x)</code></pre>
<pre><code>## [1] 0.4806125</code></pre>
<p>In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.</p>
<pre class="r"><code>summary(lm(y ~ composite_x,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ composite_x, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.14316 -0.59270  0.03342  0.55755  2.40687 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.003126   0.037276  -0.084    0.933    
## composite_x  0.837947   0.068513  12.230   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8335 on 498 degrees of freedom
## Multiple R-squared:  0.231,  Adjusted R-squared:  0.2294 
## F-statistic: 149.6 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
