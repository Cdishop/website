---
title: "Regression Creates Weighted Linear Composites"
date: "2019-12-01"
summary: "-----"
---

One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome. 

Here's a simulation to punch that point home.

500 people.

```{r}
N <- 500
```

The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.

```{r}

sigma <- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)

```

The mean for each variable is 0.

```{r}
mu <- c(0,0,0)
```

Use the correlation matrix and mean specifications to generate data.

```{r, message=F, warning = F}
library(MASS)

df <- mvrnorm(N, mu, sigma)
```

Turn it into a data frame and label it.

```{r}
df <- data.frame(df)
names(df) <- c('x1', 'x2', 'y')
df$id <- c(1:N)
```

Run regression and print the output.

```{r}
summary(lm(y ~ x1 + x2,
           data = df))
```

Here's the kicker: you can think of those weights as optimal functions telling us how to create the composite. 

Create a composite using the regression weights.

```{r, warning = F, message = F}
library(tidyverse)
df <- df %>%
  mutate(composite_x = 0.33*x1 + 0.4*x2)
```

Those weights provide the maximum correlation between our composite and the outcome.

```{r}
cor(df$y, df$composite_x)
```

In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be. 

```{r}
summary(lm(y ~ composite_x,
           data = df))
```


Bo$^2$m =)