---
title: "Regression Creates Weighted Linear Composites"
date: "2019-12-01"
summary: "-----"
---



<p>One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.</p>
<p>Here’s a simulation to punch that point home.</p>
<p>500 people.</p>
<pre class="r"><code>N &lt;- 500</code></pre>
<p>The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.</p>
<pre class="r"><code>sigma &lt;- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)</code></pre>
<p>The mean for each variable is 0.</p>
<pre class="r"><code>mu &lt;- c(0,0,0)</code></pre>
<p>Use the correlation matrix and mean specifications to generate data.</p>
<pre class="r"><code>library(MASS)

df &lt;- mvrnorm(N, mu, sigma)</code></pre>
<p>Turn it into a data frame and label it.</p>
<pre class="r"><code>df &lt;- data.frame(df)
names(df) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;y&#39;)
df$id &lt;- c(1:N)</code></pre>
<p>Run regression and print the output.</p>
<pre class="r"><code>summary(lm(y ~ x1 + x2,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4789 -0.5403  0.0095  0.5213  3.2168 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.02138    0.03713   0.576    0.565    
## x1           0.35510    0.03705   9.585   &lt;2e-16 ***
## x2           0.38420    0.03566  10.775   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8282 on 497 degrees of freedom
## Multiple R-squared:  0.3226, Adjusted R-squared:  0.3199 
## F-statistic: 118.4 on 2 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.</p>
<p>Create a composite using the regression weights.</p>
<pre class="r"><code>library(tidyverse)
df &lt;- df %&gt;%
  mutate(composite_x = 0.33*x1 + 0.4*x2)</code></pre>
<p>Those weights provide the maximum correlation between our composite and the outcome.</p>
<pre class="r"><code>cor(df$y, df$composite_x)</code></pre>
<pre><code>## [1] 0.5673049</code></pre>
<p>In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.</p>
<pre class="r"><code>summary(lm(y ~ composite_x,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ composite_x, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4408 -0.5265  0.0191  0.5268  3.2003 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.01950    0.03703   0.527    0.599    
## composite_x  1.00657    0.06548  15.373   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8278 on 498 degrees of freedom
## Multiple R-squared:  0.3218, Adjusted R-squared:  0.3205 
## F-statistic: 236.3 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
