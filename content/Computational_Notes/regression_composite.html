---
title: "Regression Creates Weighted Linear Composites"
date: "2019-12-01"
output: pdf_document
---



<p>One way to think about regression is as a tool that takes a set of predictors and creates a weighted, linear composite that maximally correlates with the response variable. It finds a way to combine multiple predictors into a single thing, using regression weights, and the weights are chosen such that, once the single composite is formed, it maximally correlates with the outcome.</p>
<p>Here’s a simulation to punch that point home.</p>
<p>500 people.</p>
<pre class="r"><code>N &lt;- 500</code></pre>
<p>The correlation matrix for three variables, x1, x2, and the outcome, y. The correlation between x1 and x2 is 0.1, the correlation between x1 and y is 0.4, and the correlation between x2 and y is 0.4.</p>
<pre class="r"><code>sigma &lt;- matrix(c(1.0, 0.1, 0.4,
                  0.1, 1.0, 0.4,
                  0.4, 0.4, 1.0), 3, 3, byrow = T)</code></pre>
<p>The mean for each variable is 0.</p>
<pre class="r"><code>mu &lt;- c(0,0,0)</code></pre>
<p>Use the correlation matrix and mean specifications to generate data.</p>
<pre class="r"><code>library(MASS)

df &lt;- mvrnorm(N, mu, sigma)</code></pre>
<p>Turn it into a data frame and label it.</p>
<pre class="r"><code>df &lt;- data.frame(df)
names(df) &lt;- c(&#39;x1&#39;, &#39;x2&#39;, &#39;y&#39;)
df$id &lt;- c(1:N)</code></pre>
<p>Run regression and print the output.</p>
<pre class="r"><code>summary(lm(y ~ x1 + x2,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.53807 -0.56098 -0.00374  0.53074  2.55853 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.03609    0.03792   0.952    0.342    
## x1           0.29322    0.03867   7.583 1.67e-13 ***
## x2           0.38240    0.03898   9.809  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.847 on 497 degrees of freedom
## Multiple R-squared:  0.2509, Adjusted R-squared:  0.2478 
## F-statistic: 83.21 on 2 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here’s the kicker: you can think of those weights as optimal functions telling us how to create the composite.</p>
<p>Create a composite using the regression weights.</p>
<pre class="r"><code>library(tidyverse)
df &lt;- df %&gt;%
  mutate(composite_x = 0.33*x1 + 0.4*x2)</code></pre>
<p>Those weights provide the maximum correlation between our composite and the outcome.</p>
<pre class="r"><code>cor(df$y, df$composite_x)</code></pre>
<pre><code>## [1] 0.5005821</code></pre>
<p>In other words, the above correlation could not be higher with any other set of weights. Regression found the weights that makes the correlation above as large as it can be.</p>
<pre class="r"><code>summary(lm(y ~ composite_x,
           data = df))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ composite_x, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.53656 -0.56005 -0.00662  0.52492  2.55748 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.03670    0.03786   0.969    0.333    
## composite_x  0.92798    0.07191  12.904   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8463 on 498 degrees of freedom
## Multiple R-squared:  0.2506, Adjusted R-squared:  0.2491 
## F-statistic: 166.5 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Bo<span class="math inline">\(^2\)</span>m =)</p>
